{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmita-mukherjee/Closing-price-prediction-of-Indian-Stock-/blob/main/Causal_lm_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADtoTMAh9OZD"
      },
      "source": [
        "## Get the Data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Fig0WMp9QoK"
      },
      "outputs": [],
      "source": [
        "!pip install -q opendatasets\n",
        "!pip install -q torch==2.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xCXRSp2T9WVt"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEd2f9ik9aMX",
        "outputId": "4139c163-42ca-49ba-e69f-d15e259ffd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./tinystories-custom-bpe\" (use force=True to force download)\n",
            "Skipping, found downloaded files in \"./tiny-shakespeare\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "# download custom tokenizer and vocab\n",
        "od.download(\"https://www.kaggle.com/datasets/asmitamukh/tinystories-custom-bpe\")\n",
        "\n",
        "#download the train and validation dataset\n",
        "od.download(\"https://www.kaggle.com/datasets/kaushaltiwari/tiny-shakespeare\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oql6SUQ_xqH-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7BkLT8K39HCj"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format=\"%(asctime)s - %(message)s\",level=logging.INFO,force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q7stPF1W9HCn"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchinfo wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RJrN0Osz9HCp"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rYxEiYi_9HCp"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y6FOonT9HCq",
        "outputId": "dd6efe64-93ef-4e54-ec1b-664c707a846b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4QkwK4W59HCq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ARI6-WFt9HCq"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNU8yJXx9HCr",
        "outputId": "d443aa21-bf3a-4882-e56b-44983ccec0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-14 09:58:10,729 - NumExpr defaulting to 2 threads.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33masmitaxyz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "wandb.login(key=\"b9fca33ebeeea7e66c03c7145a1072cc1bb07412\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-gg9y-6S9HCr"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nGwab2Yp9HCs"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dpf9kb6C9HCs"
      },
      "outputs": [],
      "source": [
        "torch.set_default_dtype(torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_tCrawP9HCt"
      },
      "source": [
        "## helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pSyNZTlg9HCu"
      },
      "outputs": [],
      "source": [
        "# Read the vocab and merge rules\n",
        "\n",
        "with open(\"/content/tinystories-custom-bpe/merge_rules_tiny_shakespeare.pkl\",\"rb\") as f:\n",
        "    merge_rules = pickle.load(f)\n",
        "\n",
        "with open(\"/content/tinystories-custom-bpe/vocab_tiny_shakespeare.pkl\",\"rb\") as f:\n",
        "    vocab = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGDMNZCi3EzR",
        "outputId": "71cd5615-c187-46af-eca3-c29ec1c9768b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gentlewoman',\n",
              " 'inte',\n",
              " 'flattering',\n",
              " 'moiety',\n",
              " 'seat',\n",
              " 'prick',\n",
              " 'marve',\n",
              " 'd',\n",
              " 'saints',\n",
              " 'below',\n",
              " 'richmond',\n",
              " 'trouble',\n",
              " 'cousins',\n",
              " 'ar',\n",
              " 'seeing',\n",
              " 'perforce',\n",
              " 'disdain',\n",
              " \"'twere\",\n",
              " 'ad',\n",
              " 'took',\n",
              " 'incensed',\n",
              " 'six',\n",
              " 'sacra',\n",
              " '--',\n",
              " 'bohemia',\n",
              " 'hers',\n",
              " 'adie',\n",
              " 'heavi',\n",
              " 'asha',\n",
              " 'flock',\n",
              " 'folk',\n",
              " 'substit',\n",
              " 'come',\n",
              " 'lancaster',\n",
              " 'hun',\n",
              " 'graves',\n",
              " 'al',\n",
              " 'cun',\n",
              " 'enjoy',\n",
              " 'sho',\n",
              " 'ze',\n",
              " 'flourish',\n",
              " 'orrow',\n",
              " 'physic',\n",
              " 'entertain',\n",
              " 'joints',\n",
              " 'charges',\n",
              " 'sky',\n",
              " 'soldi',\n",
              " 'strong',\n",
              " 'affair',\n",
              " 'flood',\n",
              " 'secure',\n",
              " 'maje',\n",
              " 'offices',\n",
              " 'bold',\n",
              " 'cous',\n",
              " 'myself',\n",
              " 'grain',\n",
              " 'cen',\n",
              " 'sick',\n",
              " 'blind',\n",
              " 'burgundy',\n",
              " 'along',\n",
              " 'nup',\n",
              " 'grim',\n",
              " 'puts',\n",
              " 'forthwit',\n",
              " 'marg',\n",
              " 'destruc',\n",
              " 'sme',\n",
              " 'mightst',\n",
              " 'adri',\n",
              " 'letters',\n",
              " 'got',\n",
              " 'fury',\n",
              " 'fal',\n",
              " 'outward',\n",
              " 'thinks',\n",
              " 'dish',\n",
              " 'richmon',\n",
              " 'fur',\n",
              " 'mercy',\n",
              " 'ond',\n",
              " 'lt',\n",
              " 'plague',\n",
              " 'dur',\n",
              " 'stood',\n",
              " 'behavi',\n",
              " 'bad',\n",
              " 'cab',\n",
              " 'bestow',\n",
              " 'conquer',\n",
              " 'iiii',\n",
              " 'kit',\n",
              " 'govern',\n",
              " 'wave',\n",
              " 'nat',\n",
              " 'va',\n",
              " 'step',\n",
              " 'france',\n",
              " 'towards',\n",
              " 'nature',\n",
              " 'shadow',\n",
              " 'westmoreland',\n",
              " 'edge',\n",
              " 'aint',\n",
              " 'ended',\n",
              " 'scept',\n",
              " 'advers',\n",
              " 'clo',\n",
              " 'call',\n",
              " 'saying',\n",
              " 'rose',\n",
              " \"e'er\",\n",
              " 'garments',\n",
              " 'equ',\n",
              " 'tongues',\n",
              " 'bosoms',\n",
              " 'feit',\n",
              " 'country',\n",
              " 'lamb',\n",
              " 'thro',\n",
              " 'shown',\n",
              " 'knife',\n",
              " 'ies',\n",
              " 'treacher',\n",
              " 'fee',\n",
              " 'ld',\n",
              " 'certainly',\n",
              " 'though',\n",
              " 'thumb',\n",
              " 'part',\n",
              " 'giving',\n",
              " 'besee',\n",
              " 'tribunes',\n",
              " 'stops',\n",
              " 'sway',\n",
              " 'affect',\n",
              " 'event',\n",
              " 'hearing',\n",
              " 'bitterly',\n",
              " 'faced',\n",
              " 'cal',\n",
              " 'torment',\n",
              " 'ut',\n",
              " 'trou',\n",
              " 'score',\n",
              " 'sma',\n",
              " 'murders',\n",
              " 'tells',\n",
              " 'eat',\n",
              " 'mb',\n",
              " 'wre',\n",
              " 'increase',\n",
              " 'shroud',\n",
              " 'om',\n",
              " 'churcchard',\n",
              " 'moi',\n",
              " 'ider',\n",
              " 'pompey',\n",
              " 'measure',\n",
              " 'attempt',\n",
              " 'westmore',\n",
              " 'gladly',\n",
              " 'ont',\n",
              " 'insulti',\n",
              " 'kisses',\n",
              " 'lift',\n",
              " 'l',\n",
              " 'commons',\n",
              " 'ped',\n",
              " 'cle',\n",
              " 'inf',\n",
              " 'boo',\n",
              " 'ele',\n",
              " 'revenged',\n",
              " 'knight',\n",
              " 'despite',\n",
              " 'ct',\n",
              " 'ravenspur',\n",
              " 'despised',\n",
              " 'watch',\n",
              " 'stays',\n",
              " 'arri',\n",
              " 'whereto',\n",
              " 'trow',\n",
              " 'offence',\n",
              " 'poli',\n",
              " 'cank',\n",
              " 'suit',\n",
              " 'wa',\n",
              " 'soldier',\n",
              " 'fav',\n",
              " 's',\n",
              " 'cheek',\n",
              " 'lightning',\n",
              " 'londoon',\n",
              " 'plead',\n",
              " 'subtle',\n",
              " 'exc',\n",
              " 'aught',\n",
              " 'kate',\n",
              " 'fu',\n",
              " 'grievous',\n",
              " 'inno',\n",
              " 'oratoor',\n",
              " 'tew',\n",
              " 'messen',\n",
              " 'couple',\n",
              " 'had',\n",
              " 'stead',\n",
              " 'peers',\n",
              " 'sed',\n",
              " 'pala',\n",
              " 'deadly',\n",
              " 'reap',\n",
              " 'exile',\n",
              " 'conf',\n",
              " 'boar',\n",
              " 'id',\n",
              " 'ath',\n",
              " 'greater',\n",
              " 'hands',\n",
              " 'carries',\n",
              " 'c',\n",
              " 'gold',\n",
              " 'shephhed',\n",
              " 'torn',\n",
              " 'careful',\n",
              " 'apt',\n",
              " 'peter',\n",
              " 'overdone',\n",
              " 'west',\n",
              " 'earl',\n",
              " 'raised',\n",
              " 'lived',\n",
              " 'constant',\n",
              " 'forward',\n",
              " 'langu',\n",
              " 'up',\n",
              " 'sc',\n",
              " 'thoughts',\n",
              " 'lion',\n",
              " 'even',\n",
              " 'waking',\n",
              " 'achieve',\n",
              " 'lodged',\n",
              " 'advice',\n",
              " 'trusty',\n",
              " 'duchess',\n",
              " 'musicians',\n",
              " 'necessity',\n",
              " 'orac',\n",
              " 'sound',\n",
              " 'oppression',\n",
              " 'dismal',\n",
              " 'rious',\n",
              " 'living',\n",
              " 'suddenly',\n",
              " 'dor',\n",
              " 'fellow',\n",
              " 'is',\n",
              " 'need',\n",
              " 'pub',\n",
              " 'entreaties',\n",
              " 'banish',\n",
              " 'armour',\n",
              " 'season',\n",
              " 'grows',\n",
              " 'tard',\n",
              " 'guess',\n",
              " 'et',\n",
              " 'executioner',\n",
              " 'soon',\n",
              " 'perpet',\n",
              " 'consequence',\n",
              " 'froward',\n",
              " 'owe',\n",
              " 'supper',\n",
              " 'choose',\n",
              " 'choice',\n",
              " \"'ay\",\n",
              " 'wr',\n",
              " 'for',\n",
              " 'cell',\n",
              " 'raise',\n",
              " 'straight',\n",
              " 'prevented',\n",
              " 'hanged',\n",
              " 'plan',\n",
              " 'chang',\n",
              " 'mar',\n",
              " 'ought',\n",
              " 'furn',\n",
              " 'brain',\n",
              " 'banished',\n",
              " 'should',\n",
              " 'habit',\n",
              " 'pomfret',\n",
              " 'spread',\n",
              " 'say',\n",
              " 'ambitious',\n",
              " 'accusation',\n",
              " 'dreams',\n",
              " 'soundly',\n",
              " 'flat',\n",
              " 'parlia',\n",
              " 'yea',\n",
              " 'du',\n",
              " 'harsh',\n",
              " 'possi',\n",
              " 'tor',\n",
              " 'falsehood',\n",
              " 'sue',\n",
              " 'oppo',\n",
              " \"'fore\",\n",
              " '3',\n",
              " 'um',\n",
              " 'ballad',\n",
              " 'sink',\n",
              " 'himself',\n",
              " 'blue',\n",
              " 'point',\n",
              " 'compan',\n",
              " \"is't\",\n",
              " 'written',\n",
              " 'praise',\n",
              " 'myster',\n",
              " 'courteoou',\n",
              " 'says',\n",
              " 'rail',\n",
              " 'then',\n",
              " 'unknown',\n",
              " 'sal',\n",
              " 'pi',\n",
              " 'discover',\n",
              " 'drops',\n",
              " 'count',\n",
              " 'roman',\n",
              " 'este',\n",
              " 'ager',\n",
              " 'would',\n",
              " 'where',\n",
              " 'hum',\n",
              " 'next',\n",
              " 're',\n",
              " 'denied',\n",
              " 'occupation',\n",
              " 'prison',\n",
              " 'off',\n",
              " 'afore',\n",
              " 'stretch',\n",
              " 'lewd',\n",
              " 'league',\n",
              " 'el',\n",
              " 'apollo',\n",
              " 'lamen',\n",
              " 'know',\n",
              " 'fires',\n",
              " 'marin',\n",
              " 'crac',\n",
              " 'fam',\n",
              " 'apace',\n",
              " 'goodly',\n",
              " 'hi',\n",
              " 'glasses',\n",
              " 'plantagenet',\n",
              " 'wholesome',\n",
              " 'longer',\n",
              " 'fitz',\n",
              " 'quest',\n",
              " 'posse',\n",
              " 'hung',\n",
              " 'terror',\n",
              " 'order',\n",
              " 'banishment',\n",
              " 'afraid',\n",
              " 'cheque',\n",
              " 'numbers',\n",
              " 'mean',\n",
              " 'keep',\n",
              " 'safety',\n",
              " 'bless',\n",
              " 'maids',\n",
              " 'price',\n",
              " 'image',\n",
              " 'yonder',\n",
              " 'vincentio',\n",
              " 'commends',\n",
              " 'weeping',\n",
              " 'isabel',\n",
              " 'youngest',\n",
              " 'talking',\n",
              " 'jewel',\n",
              " 'fill',\n",
              " 'lucentio',\n",
              " 'derby',\n",
              " 'neighbour',\n",
              " 'alike',\n",
              " 'proce',\n",
              " 'ju',\n",
              " 'pace',\n",
              " 'tu',\n",
              " 'coast',\n",
              " 'lent',\n",
              " 'amiss',\n",
              " 'needs',\n",
              " \"'gainst\",\n",
              " 'censure',\n",
              " 'cham',\n",
              " 'thur',\n",
              " 'given',\n",
              " 'streng',\n",
              " 'shin',\n",
              " 'ples',\n",
              " 'lance',\n",
              " 'endured',\n",
              " 'iel',\n",
              " 'reconcile',\n",
              " 'sanctu',\n",
              " 'sole',\n",
              " 'insul',\n",
              " 'being',\n",
              " 'moon',\n",
              " 'draws',\n",
              " 'divine',\n",
              " 'ina',\n",
              " 'doubt',\n",
              " 'extremes',\n",
              " 'br',\n",
              " 'approach',\n",
              " 'respected',\n",
              " 'tend',\n",
              " 'greate',\n",
              " 'mistre',\n",
              " 'execution',\n",
              " 'alms',\n",
              " 'vinc',\n",
              " 'pract',\n",
              " 'perdit',\n",
              " 'hip',\n",
              " 'will',\n",
              " 'blessed',\n",
              " 'enm',\n",
              " 'ous',\n",
              " 'history',\n",
              " 'lean',\n",
              " 'accuse',\n",
              " 'coun',\n",
              " 'mir',\n",
              " 'asure',\n",
              " 'young',\n",
              " 'move',\n",
              " 'neck',\n",
              " 'wednesday',\n",
              " 'favours',\n",
              " 'hind',\n",
              " 'accord',\n",
              " 'prophecy',\n",
              " 'wind',\n",
              " 'utes',\n",
              " 'il',\n",
              " 'concerns',\n",
              " 'se',\n",
              " 'dorset',\n",
              " 'aid',\n",
              " 'babe',\n",
              " 'integr',\n",
              " 'spent',\n",
              " 'looks',\n",
              " 'besides',\n",
              " 'tenderness',\n",
              " 'perd',\n",
              " 'scope',\n",
              " 'honesty',\n",
              " 'imagination',\n",
              " 'suff',\n",
              " 'adrian',\n",
              " 'liberty',\n",
              " 'now',\n",
              " 'prince',\n",
              " 'chide',\n",
              " 'terms',\n",
              " 'forty',\n",
              " 'assur',\n",
              " 'model',\n",
              " 'struck',\n",
              " 'younger',\n",
              " 'compare',\n",
              " 'ones',\n",
              " 'petruchio',\n",
              " 'ster',\n",
              " 'sees',\n",
              " 'wager',\n",
              " 'heard',\n",
              " 'hadst',\n",
              " 'rebe',\n",
              " 'cait',\n",
              " 'partic',\n",
              " 'prove',\n",
              " 'use',\n",
              " 'rever',\n",
              " 'she',\n",
              " 'birds',\n",
              " 'incre',\n",
              " 'miserable',\n",
              " 'affli',\n",
              " 'steep',\n",
              " 'sad',\n",
              " 'destiny',\n",
              " 'share',\n",
              " 'pie',\n",
              " 'others',\n",
              " \"do't\",\n",
              " 'tongue',\n",
              " 'hast',\n",
              " 'haz',\n",
              " 'ch',\n",
              " 'ring',\n",
              " 'shake',\n",
              " 'fore',\n",
              " 'troop',\n",
              " 'become',\n",
              " 'grief',\n",
              " 'effect',\n",
              " 'dost',\n",
              " 'ravenspurgh',\n",
              " 'robe',\n",
              " 'burthen',\n",
              " 'castle',\n",
              " 'ere',\n",
              " 'justice',\n",
              " 'rock',\n",
              " 'rep',\n",
              " 'concer',\n",
              " 'haply',\n",
              " 'waits',\n",
              " 'rede',\n",
              " 'glou',\n",
              " 'silent',\n",
              " 'patience',\n",
              " 'post',\n",
              " \"'hic\",\n",
              " 'cominius',\n",
              " 'bore',\n",
              " 'peace',\n",
              " 'caliban',\n",
              " 'viol',\n",
              " 'midst',\n",
              " 'hie',\n",
              " 'shrift',\n",
              " 'condem',\n",
              " 'speaking',\n",
              " 'provide',\n",
              " 'cou',\n",
              " 'jes',\n",
              " 'march',\n",
              " 'heave',\n",
              " 'virgin',\n",
              " 'affords',\n",
              " 'best',\n",
              " 'themselves',\n",
              " 'condit',\n",
              " 'statutes',\n",
              " 'exer',\n",
              " 'devils',\n",
              " 'pardon',\n",
              " 'scar',\n",
              " 'ce',\n",
              " 'downfall',\n",
              " 'vio',\n",
              " 'ian',\n",
              " 'daily',\n",
              " 'fort',\n",
              " 'bl',\n",
              " 'balth',\n",
              " 'steps',\n",
              " 'officer',\n",
              " 'safet',\n",
              " 'verily',\n",
              " 'showing',\n",
              " 'unseen',\n",
              " 'prou',\n",
              " 'corrupt',\n",
              " 'pilgrimage',\n",
              " 'confessioon',\n",
              " 'hundred',\n",
              " 'visit',\n",
              " 'worse',\n",
              " 'minola',\n",
              " 'atten',\n",
              " 'canker',\n",
              " 'braved',\n",
              " 'parents',\n",
              " 'sel',\n",
              " 'angels',\n",
              " 'member',\n",
              " 'lowly',\n",
              " \"'scape\",\n",
              " 'ba',\n",
              " 'ins',\n",
              " 'upo',\n",
              " 'whoreson',\n",
              " 'sold',\n",
              " 'thursday',\n",
              " 'pem',\n",
              " 'sple',\n",
              " 'mut',\n",
              " 'gar',\n",
              " 'fix',\n",
              " 'trifles',\n",
              " 'twenty',\n",
              " 'teach',\n",
              " 'fas',\n",
              " 'bones',\n",
              " 'better',\n",
              " 'betimes',\n",
              " 'humbly',\n",
              " 'tranio',\n",
              " 'philosophy',\n",
              " 'st',\n",
              " 'stoma',\n",
              " 'a',\n",
              " 'ham',\n",
              " 'betwe',\n",
              " 'ob',\n",
              " 'le',\n",
              " 'ush',\n",
              " 'as',\n",
              " 'blushing',\n",
              " 'georgge',\n",
              " 'dise',\n",
              " 'lesser',\n",
              " 'fier',\n",
              " 'rutland',\n",
              " 'ferdin',\n",
              " 'ste',\n",
              " 'friar',\n",
              " 'ard',\n",
              " 'ved',\n",
              " 'm',\n",
              " 'dev',\n",
              " 'town',\n",
              " 'depart',\n",
              " 'borne',\n",
              " 'mayor',\n",
              " 'eter',\n",
              " 'gentle',\n",
              " 'page',\n",
              " 'nay',\n",
              " 'thousands',\n",
              " 'ceremonious',\n",
              " 'harmony',\n",
              " 'ready',\n",
              " 'pea',\n",
              " 'advise',\n",
              " 'ru',\n",
              " 'cha',\n",
              " 'design',\n",
              " 'inquire',\n",
              " 'nails',\n",
              " 'army',\n",
              " 'e',\n",
              " 'pierce',\n",
              " 'lip',\n",
              " 'delight',\n",
              " 'varlet',\n",
              " 'intends',\n",
              " 'business',\n",
              " 'gates',\n",
              " 'senator',\n",
              " 'lack',\n",
              " 'ou',\n",
              " 'inc',\n",
              " 'coronatioon',\n",
              " 'wet',\n",
              " 'temples',\n",
              " 'wanton',\n",
              " 'ly',\n",
              " 'water',\n",
              " 'ude',\n",
              " 'pain',\n",
              " 'grand',\n",
              " 'ree',\n",
              " 'consorted',\n",
              " 'loss',\n",
              " 'warlike',\n",
              " 'slaughter',\n",
              " 'ceive',\n",
              " 'requirre',\n",
              " 'glorious',\n",
              " 'silly',\n",
              " 'steeds',\n",
              " 'herself',\n",
              " 'samp',\n",
              " 'island',\n",
              " 'errand',\n",
              " 'tired',\n",
              " 'courteo',\n",
              " 'contin',\n",
              " 'groan',\n",
              " 'hunt',\n",
              " 'fancy',\n",
              " 'goddess',\n",
              " 'sanctuary',\n",
              " 'recomp',\n",
              " 'keeps',\n",
              " 'remove',\n",
              " 'received',\n",
              " 'leng',\n",
              " 'lic',\n",
              " 'lap',\n",
              " 'wings',\n",
              " 'das',\n",
              " 'lodg',\n",
              " 'mercut',\n",
              " 'roaring',\n",
              " 'seize',\n",
              " 'used',\n",
              " 'conceit',\n",
              " 'my',\n",
              " 'stephen',\n",
              " 'escape',\n",
              " 'bushy',\n",
              " 'falsely',\n",
              " 'groans',\n",
              " 'fi',\n",
              " 'passa',\n",
              " 'thom',\n",
              " 'sensible',\n",
              " 'innoc',\n",
              " 'princes',\n",
              " 'triumph',\n",
              " 'supp',\n",
              " 'banis',\n",
              " 'proudest',\n",
              " 'friends',\n",
              " 'purchase',\n",
              " 'learn',\n",
              " 'vers',\n",
              " 'amain',\n",
              " 'occup',\n",
              " 'things',\n",
              " 'mild',\n",
              " 'beaten',\n",
              " 'q',\n",
              " 'wilts',\n",
              " 'revolt',\n",
              " 'right',\n",
              " 'behold',\n",
              " 'wednes',\n",
              " 'deceit',\n",
              " 'disgrace',\n",
              " 'mort',\n",
              " 'crue',\n",
              " 'tyrann',\n",
              " 'mani',\n",
              " 'tyrrel',\n",
              " 'tooth',\n",
              " 'qu',\n",
              " 'avoided',\n",
              " 'wear',\n",
              " 'ious',\n",
              " 'ten',\n",
              " 'gloucester',\n",
              " 'earth',\n",
              " 'yours',\n",
              " 'pt',\n",
              " 'beseech',\n",
              " 'gr',\n",
              " 'vile',\n",
              " 'hen',\n",
              " 'instruc',\n",
              " 'day',\n",
              " 'bade',\n",
              " 'ws',\n",
              " 'rise',\n",
              " 'stu',\n",
              " 'corrup',\n",
              " 'intelligence',\n",
              " 'worser',\n",
              " 'lieuten',\n",
              " 'fig',\n",
              " 'parting',\n",
              " \"for't\",\n",
              " 'destruction',\n",
              " 'mi',\n",
              " 'wont',\n",
              " 'dr',\n",
              " 'warm',\n",
              " 'lend',\n",
              " 'ure',\n",
              " 'priv',\n",
              " 'bui',\n",
              " 'ault',\n",
              " 'whole',\n",
              " 'brid',\n",
              " \"o'\",\n",
              " 'wink',\n",
              " 'dine',\n",
              " 'redemp',\n",
              " 'lieutenant',\n",
              " 'ong',\n",
              " 'ke',\n",
              " 'to-morrow',\n",
              " 'excellent',\n",
              " 'wedding-day',\n",
              " 'bride',\n",
              " 'pitiful',\n",
              " 'unhappy',\n",
              " 'cast',\n",
              " 'mirand',\n",
              " 'example',\n",
              " 'danger',\n",
              " 'dozen',\n",
              " 'irre',\n",
              " 'native',\n",
              " 'far',\n",
              " 'choler',\n",
              " 'coat',\n",
              " 'seek',\n",
              " 'col',\n",
              " 'steel',\n",
              " 'pet',\n",
              " 'z',\n",
              " 'perpetual',\n",
              " 'san',\n",
              " 'ange',\n",
              " 'shines',\n",
              " 'faults',\n",
              " 'wondr',\n",
              " 'balthasar',\n",
              " 'shr',\n",
              " 'intell',\n",
              " 'calm',\n",
              " 'confid',\n",
              " 'dy',\n",
              " 'can',\n",
              " 'forced',\n",
              " 'blood',\n",
              " 'whose',\n",
              " 'liest',\n",
              " 'sleep',\n",
              " 'voices',\n",
              " 'ross',\n",
              " 'stre',\n",
              " 'herald',\n",
              " 'encounter',\n",
              " 'takes',\n",
              " 'sensse',\n",
              " 'sep',\n",
              " 'dear',\n",
              " 'fra',\n",
              " 'gods',\n",
              " 'escal',\n",
              " 'ton',\n",
              " 'forgive',\n",
              " 'surrey',\n",
              " 'bre',\n",
              " 'reason',\n",
              " 'fection',\n",
              " 'remember',\n",
              " 'malice',\n",
              " 'wreck',\n",
              " 'divided',\n",
              " 'counterfeit',\n",
              " 'repent',\n",
              " 'whereforre',\n",
              " 'mem',\n",
              " 'daughters',\n",
              " 'vessel',\n",
              " 'val',\n",
              " 'taught',\n",
              " 'seas',\n",
              " 'curre',\n",
              " 'married',\n",
              " 'extrem',\n",
              " 'too',\n",
              " 'train',\n",
              " 'regard',\n",
              " 'bucking',\n",
              " 'clothes',\n",
              " 'fru',\n",
              " 'statue',\n",
              " 'passing',\n",
              " 'mock',\n",
              " 'vict',\n",
              " 'murdered',\n",
              " 'exeter',\n",
              " 'pleased',\n",
              " 'salis',\n",
              " 'wanting',\n",
              " 'whipt',\n",
              " 'mariana',\n",
              " 'careless',\n",
              " 'thing',\n",
              " 'lambs',\n",
              " 'taunts',\n",
              " 'waves',\n",
              " 'behalf',\n",
              " 'falling',\n",
              " 'dearly',\n",
              " 'montagues',\n",
              " 'heart',\n",
              " 'boots',\n",
              " 'luc',\n",
              " 'def',\n",
              " 'weapon',\n",
              " 'pad',\n",
              " 'joys',\n",
              " 'mainta',\n",
              " 'dust',\n",
              " 'oul',\n",
              " 'modest',\n",
              " 'soul',\n",
              " 'midwife',\n",
              " 'signify',\n",
              " 'beauteous',\n",
              " 'mine',\n",
              " 'tender',\n",
              " 'estim',\n",
              " 'height',\n",
              " 'foe',\n",
              " 'punish',\n",
              " 'cried',\n",
              " 'flint',\n",
              " 'pilot',\n",
              " 'tush',\n",
              " 'sixteen',\n",
              " 'lia',\n",
              " 'bed',\n",
              " 'under',\n",
              " 'gregory',\n",
              " 'gaze',\n",
              " 'merciful',\n",
              " 'error',\n",
              " 'pursuiv',\n",
              " 'setting',\n",
              " 'knaves',\n",
              " 'arm',\n",
              " 'keeper',\n",
              " 'mone',\n",
              " 'sheph',\n",
              " 'resolution',\n",
              " 'lips',\n",
              " 'recompen',\n",
              " 'ss',\n",
              " 'its',\n",
              " 'jupit',\n",
              " 'ever',\n",
              " 'damnable',\n",
              " 'sickness',\n",
              " 'serp',\n",
              " 'manifest',\n",
              " 'swell',\n",
              " 'sights',\n",
              " 'mali',\n",
              " 'oun',\n",
              " 'him',\n",
              " 'plays',\n",
              " 'reigns',\n",
              " 'aspect',\n",
              " 'brakenbury',\n",
              " \"'sha\",\n",
              " 'contents',\n",
              " 'character',\n",
              " 'double',\n",
              " 'bosom',\n",
              " 'aed',\n",
              " 'mit',\n",
              " 'sat',\n",
              " 'load',\n",
              " 'mere',\n",
              " 'op',\n",
              " 'wi',\n",
              " 'volumni',\n",
              " 'rel',\n",
              " 'mowbray',\n",
              " 'tail',\n",
              " 'citizen',\n",
              " 'brief',\n",
              " 'recover',\n",
              " 'happy',\n",
              " 'igh',\n",
              " 'royalties',\n",
              " 'anoint',\n",
              " 'eas',\n",
              " 'mamill',\n",
              " 'becomes',\n",
              " 'prepar',\n",
              " 'alk',\n",
              " 'hostess',\n",
              " 'ceremon',\n",
              " 'hail',\n",
              " 'claim',\n",
              " 'lucio',\n",
              " 'cor',\n",
              " 'ror',\n",
              " 'fail',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yZyEUECmPxXJ",
        "outputId": "f874b39a-a59e-402e-aef3-e660b1b5ddf0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.2+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "smPF-0ly9HCv"
      },
      "outputs": [],
      "source": [
        "vocab.add(\"</s>\")\n",
        "vocab.add(\"<unk>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oJQADLxp9HCv"
      },
      "outputs": [],
      "source": [
        "def tokenize(text,merge_rules=merge_rules):\n",
        "    '''Return tokens from the text'''\n",
        "    text = text.lower() # our vocabulary is uncased.\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    ##preparing dict where word is the key and the tokens is its value\n",
        "    word_dict = dict()\n",
        "    for word in words:\n",
        "        word_dict[word] = []\n",
        "        for char in word:\n",
        "            word_dict[word].append(char)\n",
        "\n",
        "    for merge_rule in merge_rules.keys():\n",
        "        #go through each merge rule and tokenize each word from left to right\n",
        "        for word in word_dict.keys():\n",
        "            tokens = word_dict[word]\n",
        "            idx=0\n",
        "            while(idx<len(tokens)-1):\n",
        "                pair = (tokens[idx],tokens[idx+1])\n",
        "                if pair==merge_rule:\n",
        "                    #merge the token of the word as per the merge rule\n",
        "                    #if merged we dont increase the index of the tokens because the next pair should include the new pair and the next char\n",
        "                    tokens = tokens[:idx]+[tokens[idx]+tokens[idx+1]]+tokens[idx+2:]\n",
        "                    word_dict[word] = tokens\n",
        "                else:\n",
        "                    #if not merged then we slide the window over\n",
        "                    idx = idx+1\n",
        "\n",
        "    tokens = [token for item in word_dict.values() for token in item ]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uhkR2fPm0lIO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def encode(list_of_tokens):\n",
        "  vocab_l = list(vocab)\n",
        "  token_tensor = []\n",
        "  for token in list_of_tokens:\n",
        "    try:\n",
        "      token_tensor.append(vocab_l.index(token))\n",
        "    except ValueError:\n",
        "      logging.info(f\"token {token} not in vocab\")\n",
        "\n",
        "  return token_tensor\n",
        "\n",
        "\n",
        "def decode(list_of_idx):\n",
        "  tokens = []\n",
        "  vocab_l = list(vocab)\n",
        "  for idx in list_of_idx:\n",
        "    tokens.append(vocab_l[idx])\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSZiV3wn9HCv"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4GPoPMdY9HCw"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "with open(\"/content/tiny-shakespeare/tiny-shakespeare.txt\",\"r\") as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3R7Jyqro4Fi",
        "outputId": "f86267de-7973-4538-fa14-4c58acaeea95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first', 'citizen', ':', 'before', 'we']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "tokenized_data = tokenize(text)\n",
        "tokenized_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OSmvoQW1wOr",
        "outputId": "158534bd-ae10-49c8-9e49-e2341c2318ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-14 09:59:06,068 - token & not in vocab\n",
            "2024-01-14 09:59:06,935 - token $ not in vocab\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1094, 980, 3778, 1045, 3832]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "encoded_text = encode(tokenized_data)\n",
        "encoded_text[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETjc_81y4mhP",
        "outputId": "c93f9c61-07a8-42cc-eedb-4a3124163a88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first', 'citizen', ':', 'before', 'we']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "decode(encoded_text[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "k9IFKqXIoCng"
      },
      "outputs": [],
      "source": [
        "context_length = 8\n",
        "\n",
        "#chunking windows of 8 from the text\n",
        "chunks = []\n",
        "\n",
        "\n",
        "list_of_inputs = []\n",
        "list_of_labels = []\n",
        "\n",
        "for idx in range(0,len(encoded_text),context_length+1):\n",
        "  list_of_inputs.append(encoded_text[idx:idx+context_length])\n",
        "\n",
        "  if idx+1>len(encoded_text):\n",
        "    list_of_labels.append(vocab.index(\"</s>\"))\n",
        "  else:\n",
        "    list_of_labels.append(encoded_text[idx+1:idx+context_length+1])\n",
        "\n",
        "    if idx+context_length>len(encoded_text):\n",
        "      list_of_labels[-1].extend(vocab.index(\"</s>\"))\n",
        "\n",
        "vocab.add(\"</s>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THOBHxXCxFVt",
        "outputId": "4b0c9068-a51e-4e79-fa4c-3219f558a196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1094, 980, 3778, 1045, 3832, 3603, 2037, 3404],\n",
              " [1768, 4685, 2693, 1612, 2895, 1034, 1132, 1617],\n",
              " [3791, 1344, 4135, 362, 2240, 1589, 358, 4111],\n",
              " [262, 2532, 2016, 1025, 1305, 1681, 3958, 3239],\n",
              " [959, 2826, 2023, 3918, 4079, 3920, 2379, 1140]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "list_of_inputs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1R7uww-xImR",
        "outputId": "fb847999-c82f-4ffe-a96e-a7ffa5b20e1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[980, 3778, 1045, 3832, 3603, 2037, 3404, 2207],\n",
              " [4685, 2693, 1612, 2895, 1034, 1132, 1617, 1328],\n",
              " [1344, 4135, 362, 2240, 1589, 358, 4111, 4357],\n",
              " [2532, 2016, 1025, 1305, 1681, 3958, 3239, 3642],\n",
              " [2826, 2023, 3918, 4079, 3920, 2379, 1140, 385]]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "list_of_labels[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BE61F2iExRwS"
      },
      "outputs": [],
      "source": [
        "#shuffle the list of inputs and labels and split into train and test set\n",
        "temp = list(zip(list_of_inputs,list_of_labels))\n",
        "random.shuffle(temp)\n",
        "list_of_inputs,list_of_labels = zip(*temp)\n",
        "list_of_inputs,list_of_labels = list(list_of_inputs),list(list_of_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UEq0EWHUyL5h"
      },
      "outputs": [],
      "source": [
        "len_of_train = int((0.9)*len(list_of_inputs))\n",
        "train_inputs,train_labels = list_of_inputs[:len_of_train],list_of_labels[:len_of_train]\n",
        "test_inputs,test_labels = list_of_inputs[len_of_train:],list_of_labels[len_of_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L7GafMjzc_0",
        "outputId": "6855b8d2-46de-4bbd-ee62-d7d5ba35a648"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2914, 2914)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(train_inputs),len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MayzbFYF5Ox9",
        "outputId": "55545149-1bf4-4756-d1b6-42e10d791ea1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(324, 324)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "len(test_inputs),len(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3R7jFyp7CAx"
      },
      "source": [
        "> Divide into Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AkVYURNQ7E7Q"
      },
      "outputs": [],
      "source": [
        "batch_inputs_train = []\n",
        "batch_labels_train = []\n",
        "\n",
        "for idx in range(0,len(train_inputs),BATCH_SIZE):\n",
        "  batch_inputs_train.append(train_inputs[idx:idx+BATCH_SIZE])\n",
        "  batch_labels_train.append(train_labels[idx:idx+BATCH_SIZE])\n",
        "\n",
        "batch_inputs_train = torch.tensor(batch_inputs_train,dtype=torch.long)\n",
        "batch_labels_train = torch.tensor(batch_labels_train,dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okRe30FXTC5v",
        "outputId": "6550d692-3a20-4732-ad2f-0256382a58f1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0niJhwESoKM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs_train.shape,batch_labels_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik0HdWuY0svw",
        "outputId": "22e9988e-c09c-4534-805b-583d2c4dcace"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1457, 2, 8]), torch.Size([1457, 2, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs_test = []\n",
        "batch_labels_test = []\n",
        "\n",
        "for idx in range(0,len(test_inputs),BATCH_SIZE):\n",
        "  batch_inputs_test.append(test_inputs[idx:idx+BATCH_SIZE])\n",
        "  batch_labels_test.append(test_labels[idx:idx+BATCH_SIZE])\n",
        "\n",
        "batch_inputs_test = torch.tensor(batch_inputs_test,dtype=torch.long)\n",
        "batch_labels_test = torch.tensor(batch_labels_test,dtype=torch.long)\n",
        "\n",
        "batch_inputs_test.shape,batch_labels_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS_HjTsO09Kt",
        "outputId": "32288b00-826d-4f67-9584-84bce63ee387"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([162, 2, 8]), torch.Size([162, 2, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs_train= batch_inputs_train.to(device)\n",
        "batch_labels_train= batch_labels_train.to(device)\n",
        "batch_inputs_test= batch_inputs_test.to(device)\n",
        "batch_labels_test= batch_labels_test.to(device)"
      ],
      "metadata": {
        "id": "sVUbLWkF2uq4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1BGteB329HC1"
      },
      "outputs": [],
      "source": [
        "class BareLLM(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
        "        #self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.Linear1 = nn.Linear(embed_dim,hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.Linear2 = nn.Linear(hidden_dim,vocab_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self,input_tensor):\n",
        "        logging.debug(f\"Shape of input tensor {input_tensor.shape}\")\n",
        "        embedded_tensor = self.embedding(input_tensor)\n",
        "        logging.debug(f\"Shape of embedding matrix {embedded_tensor.shape}\")\n",
        "        #norm_tensor = self.layer_norm(embedded_tensor)\n",
        "        #logging.debug(f\"Shape of matrix after norm {norm_tensor.shape}\")\n",
        "        linear_output_1 = self.Linear1(embedded_tensor)\n",
        "        logging.debug(f\"Shape of tensor after linear layer 1 {linear_output_1.shape}\")\n",
        "        relu_output_1 = self.relu1(linear_output_1)\n",
        "        linear_output_2 = self.Linear2(relu_output_1)\n",
        "        logging.debug(f\"Shape of tensor after linear layer 2 {linear_output_2.shape}\")\n",
        "        relu_output_2 = self.relu2(linear_output_2)\n",
        "        return relu_output_2\n",
        "\n",
        "    def generate(self,starting_idx,max_new_tokens):\n",
        "      '''\n",
        "        Autoregressively generates a string upto max_new_tokens starting from starting idx\n",
        "      '''\n",
        "      gen_tokens = 0\n",
        "\n",
        "      idx = starting_idx\n",
        "\n",
        "      while gen_tokens<max_new_tokens:\n",
        "        next_token_logits = self(idx)\n",
        "        logging.debug(f\"Shape of next token logits {next_token_logits.shape}\")\n",
        "        #only take the logit of the last token of the input\n",
        "        next_token_logits = next_token_logits[:,-1,:]\n",
        "        logging.debug(f\"Shape of next token logits of the last token {next_token_logits.shape}\")\n",
        "        prob_of_next_token = torch.nn.functional.softmax(next_token_logits,dim=1)\n",
        "        logging.debug(f\"Prob of next token {prob_of_next_token.shape}\")#1*vocab_size\n",
        "        next_token_idx = torch.topk(prob_of_next_token,dim=1,k=3)\n",
        "        next_token_idx = torch.multinomial(next_token_idx.values,1)\n",
        "        logging.debug(f\"Next token idx {next_token_idx}\")\n",
        "        idx = torch.cat((idx,next_token_idx),dim=1)\n",
        "        gen_tokens = gen_tokens+1\n",
        "\n",
        "      return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "xbV4yAUQ9HC1"
      },
      "outputs": [],
      "source": [
        "model = BareLLM(vocab_size=len(vocab),embed_dim=768,hidden_dim=128)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_tokens = model.generate(torch.tensor([[1458]],dtype=torch.long,device=device),200)"
      ],
      "metadata": {
        "id": "Ez4cJV1IBkSP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_bef_training = ' '.join(decode(list(gen_tokens[0])))"
      ],
      "metadata": {
        "id": "1_ld2zsHLl3h"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utW-wax19HC2",
        "outputId": "a7c5ad19-84b6-42ef-ca5b-0ddd9f2bc6fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "BareLLM                                  --\n",
              "├─Embedding: 1-1                         3,607,296\n",
              "├─Linear: 1-2                            98,432\n",
              "├─ReLU: 1-3                              --\n",
              "├─Linear: 1-4                            605,913\n",
              "├─ReLU: 1-5                              --\n",
              "=================================================================\n",
              "Total params: 4,311,641\n",
              "Trainable params: 4,311,641\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aiKWUvDS9HC2"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpTYuaT19HC2",
        "outputId": "4624d9f4-d8b6-4511-902f-c177263e79e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7bd79aae2d50>\n"
          ]
        }
      ],
      "source": [
        "print(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "FqNXj6kq9HC2"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE7xSyOD9HC2"
      },
      "source": [
        "> Checking forward pass for one iteration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s_output = model(torch.tensor([0],dtype=torch.long,device=device))"
      ],
      "metadata": {
        "id": "95dcY1TE9_AA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "I3OOq3Jt9HC3"
      },
      "outputs": [],
      "source": [
        "s_output = model(batch_inputs_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyrWUQNx9HC3",
        "outputId": "a9ef9753-1da1-4b4d-d91d-e7cc96c6e54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 4697])\n"
          ]
        }
      ],
      "source": [
        "print(s_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juNhORzQ9HC3"
      },
      "source": [
        "### Write Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "VokPeMmJ9HC3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh_-1yKm9HC3",
        "outputId": "de16cda8-1292-417f-8a1b-c44c2f38857e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4697"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "GanCqrxJ9HC4"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(),lr=1e-2,momentum=0.9)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(input_batch,label_batch):\n",
        "    running_loss = 0 #captures the loss at each iteration of the dataset\n",
        "    last_loss = 0 #captures the loss at the training of the epoch\n",
        "\n",
        "    for batch_nos,(inputs,labels) in enumerate(zip(input_batch,label_batch)):\n",
        "        logging.debug(f\"Batch nos {batch_nos}\")\n",
        "        #at the start of every batch zero grad optimizer to clean the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #output for the batch\n",
        "        logits = model(inputs)\n",
        "\n",
        "        #compute loss\n",
        "        #reshaping the logits to batch_size*vocab_size*context_length for pytorch nn.cross_entropy\n",
        "        batch_size,context_len,vocab_size = logits.shape\n",
        "        logits = logits.reshape(batch_size,vocab_size,context_len)\n",
        "        loss = loss_fn(logits,labels)\n",
        "        loss.backward()\n",
        "\n",
        "        #apply the updates to all parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_nos%100 ==99:\n",
        "            last_loss = running_loss/500\n",
        "            logging.info(f\"For batch {batch_nos+1} loss at 500 samples is {last_loss}\")\n",
        "            running_loss = 0\n",
        "\n",
        "    return last_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "An2DQeGd9HC4",
        "outputId": "4239ab7d-0afb-4892-a10a-8282ee894717"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240114_095910-0ajtzjzq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/asmitaxyz/bare_min_llm_14th_jan/runs/0ajtzjzq' target=\"_blank\">correct_causal_lm_14th_jan</a></strong> to <a href='https://wandb.ai/asmitaxyz/bare_min_llm_14th_jan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/asmitaxyz/bare_min_llm_14th_jan' target=\"_blank\">https://wandb.ai/asmitaxyz/bare_min_llm_14th_jan</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/asmitaxyz/bare_min_llm_14th_jan/runs/0ajtzjzq' target=\"_blank\">https://wandb.ai/asmitaxyz/bare_min_llm_14th_jan/runs/0ajtzjzq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "wandb.init(project=\"bare_min_llm_14th_jan\",name=\"correct_causal_lm_14th_jan\")\n",
        "wandb.watch(model, log_freq=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs_train.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpHkj3wxOQDj",
        "outputId": "19e7cd9c-d946-42ea-cb88-f4eb505ec22f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "21Q5W7Bd9HC4",
        "outputId": "62f8341c-0fc0-4071-974c-8fe15545b88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-14 09:59:12,611 - Starting training at 1\n",
            "2024-01-14 09:59:13,664 - For batch 100 loss at 500 samples is 1.690265625\n",
            "2024-01-14 09:59:14,544 - For batch 200 loss at 500 samples is 1.680625\n",
            "2024-01-14 09:59:15,515 - For batch 300 loss at 500 samples is 1.663171875\n",
            "2024-01-14 09:59:16,404 - For batch 400 loss at 500 samples is 1.6590234375\n",
            "2024-01-14 09:59:17,297 - For batch 500 loss at 500 samples is 1.6510234375\n",
            "2024-01-14 09:59:18,423 - For batch 600 loss at 500 samples is 1.6391875\n",
            "2024-01-14 09:59:19,794 - For batch 700 loss at 500 samples is 1.628140625\n",
            "2024-01-14 09:59:21,069 - For batch 800 loss at 500 samples is 1.622671875\n",
            "2024-01-14 09:59:21,957 - For batch 900 loss at 500 samples is 1.619625\n",
            "2024-01-14 09:59:22,812 - For batch 1000 loss at 500 samples is 1.61190625\n",
            "2024-01-14 09:59:23,688 - For batch 1100 loss at 500 samples is 1.606671875\n",
            "2024-01-14 09:59:24,575 - For batch 1200 loss at 500 samples is 1.5984765625\n",
            "2024-01-14 09:59:25,448 - For batch 1300 loss at 500 samples is 1.605859375\n",
            "2024-01-14 09:59:26,346 - For batch 1400 loss at 500 samples is 1.60609375\n",
            "<ipython-input-54-adc77d5dd48b>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  v_input = torch.tensor(v_input,dtype=torch.long,device=device)\n",
            "<ipython-input-54-adc77d5dd48b>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  v_label = torch.tensor(v_label,dtype=torch.long,device=device)\n",
            "2024-01-14 09:59:26,860 - LOSS train 1.60609375 valid 1301.0\n",
            "2024-01-14 09:59:26,862 - Starting training at 2\n",
            "2024-01-14 09:59:27,739 - For batch 100 loss at 500 samples is 1.5365390625\n",
            "2024-01-14 09:59:28,599 - For batch 200 loss at 500 samples is 1.5383046875\n",
            "2024-01-14 09:59:29,476 - For batch 300 loss at 500 samples is 1.52759375\n",
            "2024-01-14 09:59:30,363 - For batch 400 loss at 500 samples is 1.5203671875\n",
            "2024-01-14 09:59:31,445 - For batch 500 loss at 500 samples is 1.5257890625\n",
            "2024-01-14 09:59:32,802 - For batch 600 loss at 500 samples is 1.5172578125\n",
            "2024-01-14 09:59:34,113 - For batch 700 loss at 500 samples is 1.4890078125\n",
            "2024-01-14 09:59:34,954 - For batch 800 loss at 500 samples is 1.5008984375\n",
            "2024-01-14 09:59:35,793 - For batch 900 loss at 500 samples is 1.487921875\n",
            "2024-01-14 09:59:36,648 - For batch 1000 loss at 500 samples is 1.4901640625\n",
            "2024-01-14 09:59:37,503 - For batch 1100 loss at 500 samples is 1.490625\n",
            "2024-01-14 09:59:38,352 - For batch 1200 loss at 500 samples is 1.4805625\n",
            "2024-01-14 09:59:39,196 - For batch 1300 loss at 500 samples is 1.4810703125\n",
            "2024-01-14 09:59:40,052 - For batch 1400 loss at 500 samples is 1.489203125\n",
            "2024-01-14 09:59:40,616 - LOSS train 1.489203125 valid 657.0\n",
            "2024-01-14 09:59:40,618 - Starting training at 3\n",
            "2024-01-14 09:59:41,489 - For batch 100 loss at 500 samples is 1.4134765625\n",
            "2024-01-14 09:59:42,345 - For batch 200 loss at 500 samples is 1.4218046875\n",
            "2024-01-14 09:59:43,199 - For batch 300 loss at 500 samples is 1.415421875\n",
            "2024-01-14 09:59:44,035 - For batch 400 loss at 500 samples is 1.40640625\n",
            "2024-01-14 09:59:45,356 - For batch 500 loss at 500 samples is 1.41896875\n",
            "2024-01-14 09:59:47,103 - For batch 600 loss at 500 samples is 1.4084453125\n",
            "2024-01-14 09:59:48,361 - For batch 700 loss at 500 samples is 1.3732109375\n",
            "2024-01-14 09:59:49,439 - For batch 800 loss at 500 samples is 1.399578125\n",
            "2024-01-14 09:59:50,302 - For batch 900 loss at 500 samples is 1.3816875\n",
            "2024-01-14 09:59:51,156 - For batch 1000 loss at 500 samples is 1.3839453125\n",
            "2024-01-14 09:59:52,018 - For batch 1100 loss at 500 samples is 1.3879296875\n",
            "2024-01-14 09:59:52,896 - For batch 1200 loss at 500 samples is 1.37234375\n",
            "2024-01-14 09:59:53,728 - For batch 1300 loss at 500 samples is 1.3705078125\n",
            "2024-01-14 09:59:54,565 - For batch 1400 loss at 500 samples is 1.3822421875\n",
            "2024-01-14 09:59:55,160 - LOSS train 1.3822421875 valid 455.25\n",
            "2024-01-14 09:59:55,162 - Starting training at 4\n",
            "2024-01-14 09:59:56,033 - For batch 100 loss at 500 samples is 1.3041328125\n",
            "2024-01-14 09:59:56,911 - For batch 200 loss at 500 samples is 1.306375\n",
            "2024-01-14 09:59:58,009 - For batch 300 loss at 500 samples is 1.3176171875\n",
            "2024-01-14 09:59:59,378 - For batch 400 loss at 500 samples is 1.3040234375\n",
            "2024-01-14 10:00:00,673 - For batch 500 loss at 500 samples is 1.3275078125\n",
            "2024-01-14 10:00:01,526 - For batch 600 loss at 500 samples is 1.30746875\n",
            "2024-01-14 10:00:02,388 - For batch 700 loss at 500 samples is 1.26871875\n",
            "2024-01-14 10:00:03,312 - For batch 800 loss at 500 samples is 1.306\n",
            "2024-01-14 10:00:04,161 - For batch 900 loss at 500 samples is 1.291328125\n",
            "2024-01-14 10:00:05,012 - For batch 1000 loss at 500 samples is 1.2929609375\n",
            "2024-01-14 10:00:05,875 - For batch 1100 loss at 500 samples is 1.30271875\n",
            "2024-01-14 10:00:06,732 - For batch 1200 loss at 500 samples is 1.2821015625\n",
            "2024-01-14 10:00:07,640 - For batch 1300 loss at 500 samples is 1.28534375\n",
            "2024-01-14 10:00:08,484 - For batch 1400 loss at 500 samples is 1.2899375\n",
            "2024-01-14 10:00:09,002 - LOSS train 1.2899375 valid 356.5\n",
            "2024-01-14 10:00:09,005 - Starting training at 5\n",
            "2024-01-14 10:00:09,857 - For batch 100 loss at 500 samples is 1.2246640625\n",
            "2024-01-14 10:00:10,790 - For batch 200 loss at 500 samples is 1.221796875\n",
            "2024-01-14 10:00:12,099 - For batch 300 loss at 500 samples is 1.2371484375\n",
            "2024-01-14 10:00:13,451 - For batch 400 loss at 500 samples is 1.22703125\n",
            "2024-01-14 10:00:14,394 - For batch 500 loss at 500 samples is 1.2633125\n",
            "2024-01-14 10:00:15,245 - For batch 600 loss at 500 samples is 1.2286171875\n",
            "2024-01-14 10:00:16,097 - For batch 700 loss at 500 samples is 1.188140625\n",
            "2024-01-14 10:00:16,948 - For batch 800 loss at 500 samples is 1.234015625\n",
            "2024-01-14 10:00:17,822 - For batch 900 loss at 500 samples is 1.213578125\n",
            "2024-01-14 10:00:18,705 - For batch 1000 loss at 500 samples is 1.227359375\n",
            "2024-01-14 10:00:19,567 - For batch 1100 loss at 500 samples is 1.2393125\n",
            "2024-01-14 10:00:20,409 - For batch 1200 loss at 500 samples is 1.2110703125\n",
            "2024-01-14 10:00:21,284 - For batch 1300 loss at 500 samples is 1.2205078125\n",
            "2024-01-14 10:00:22,128 - For batch 1400 loss at 500 samples is 1.227953125\n",
            "2024-01-14 10:00:22,724 - LOSS train 1.227953125 valid 296.75\n",
            "2024-01-14 10:00:22,727 - Starting training at 6\n",
            "2024-01-14 10:00:23,567 - For batch 100 loss at 500 samples is 1.177421875\n",
            "2024-01-14 10:00:24,826 - For batch 200 loss at 500 samples is 1.16840625\n",
            "2024-01-14 10:00:26,153 - For batch 300 loss at 500 samples is 1.1850703125\n",
            "2024-01-14 10:00:27,279 - For batch 400 loss at 500 samples is 1.1794453125\n",
            "2024-01-14 10:00:28,170 - For batch 500 loss at 500 samples is 1.22053515625\n",
            "2024-01-14 10:00:29,041 - For batch 600 loss at 500 samples is 1.1819453125\n",
            "2024-01-14 10:00:29,890 - For batch 700 loss at 500 samples is 1.15078125\n",
            "2024-01-14 10:00:30,732 - For batch 800 loss at 500 samples is 1.1953671875\n",
            "2024-01-14 10:00:31,581 - For batch 900 loss at 500 samples is 1.165875\n",
            "2024-01-14 10:00:32,419 - For batch 1000 loss at 500 samples is 1.195375\n",
            "2024-01-14 10:00:33,284 - For batch 1100 loss at 500 samples is 1.2023046875\n",
            "2024-01-14 10:00:34,129 - For batch 1200 loss at 500 samples is 1.169125\n",
            "2024-01-14 10:00:34,983 - For batch 1300 loss at 500 samples is 1.1765234375\n",
            "2024-01-14 10:00:35,822 - For batch 1400 loss at 500 samples is 1.1792109375\n",
            "2024-01-14 10:00:36,402 - LOSS train 1.1792109375 valid 254.875\n",
            "2024-01-14 10:00:36,405 - Starting training at 7\n",
            "2024-01-14 10:00:37,501 - For batch 100 loss at 500 samples is 1.1449375\n",
            "2024-01-14 10:00:38,860 - For batch 200 loss at 500 samples is 1.13909375\n",
            "2024-01-14 10:00:40,167 - For batch 300 loss at 500 samples is 1.149015625\n",
            "2024-01-14 10:00:41,003 - For batch 400 loss at 500 samples is 1.1505234375\n",
            "2024-01-14 10:00:41,855 - For batch 500 loss at 500 samples is 1.18975\n",
            "2024-01-14 10:00:42,710 - For batch 600 loss at 500 samples is 1.144375\n",
            "2024-01-14 10:00:43,579 - For batch 700 loss at 500 samples is 1.12623046875\n",
            "2024-01-14 10:00:44,439 - For batch 800 loss at 500 samples is 1.180453125\n",
            "2024-01-14 10:00:45,333 - For batch 900 loss at 500 samples is 1.14703125\n",
            "2024-01-14 10:00:46,177 - For batch 1000 loss at 500 samples is 1.17805859375\n",
            "2024-01-14 10:00:47,043 - For batch 1100 loss at 500 samples is 1.1730546875\n",
            "2024-01-14 10:00:47,922 - For batch 1200 loss at 500 samples is 1.1471171875\n",
            "2024-01-14 10:00:48,831 - For batch 1300 loss at 500 samples is 1.151203125\n",
            "2024-01-14 10:00:49,742 - For batch 1400 loss at 500 samples is 1.14292578125\n",
            "2024-01-14 10:00:50,330 - LOSS train 1.14292578125 valid 224.75\n",
            "2024-01-14 10:00:50,332 - Starting training at 8\n",
            "2024-01-14 10:00:51,716 - For batch 100 loss at 500 samples is 1.1312265625\n",
            "2024-01-14 10:00:53,112 - For batch 200 loss at 500 samples is 1.12161328125\n",
            "2024-01-14 10:00:54,055 - For batch 300 loss at 500 samples is 1.155375\n",
            "2024-01-14 10:00:54,931 - For batch 400 loss at 500 samples is 1.16426171875\n",
            "2024-01-14 10:00:55,828 - For batch 500 loss at 500 samples is 1.1815078125\n",
            "2024-01-14 10:00:56,684 - For batch 600 loss at 500 samples is 1.13203125\n",
            "2024-01-14 10:00:57,555 - For batch 700 loss at 500 samples is 1.1066875\n",
            "2024-01-14 10:00:58,415 - For batch 800 loss at 500 samples is 1.16122265625\n",
            "2024-01-14 10:00:59,310 - For batch 900 loss at 500 samples is 1.122953125\n",
            "2024-01-14 10:01:00,194 - For batch 1000 loss at 500 samples is 1.16306640625\n",
            "2024-01-14 10:01:01,048 - For batch 1100 loss at 500 samples is 1.14640625\n",
            "2024-01-14 10:01:01,944 - For batch 1200 loss at 500 samples is 1.130171875\n",
            "2024-01-14 10:01:02,789 - For batch 1300 loss at 500 samples is 1.14742578125\n",
            "2024-01-14 10:01:03,842 - For batch 1400 loss at 500 samples is 1.126234375\n",
            "2024-01-14 10:01:04,711 - LOSS train 1.126234375 valid 200.875\n",
            "2024-01-14 10:01:04,713 - Starting training at 9\n",
            "2024-01-14 10:01:06,065 - For batch 100 loss at 500 samples is 1.1189765625\n",
            "2024-01-14 10:01:07,110 - For batch 200 loss at 500 samples is 1.11108984375\n",
            "2024-01-14 10:01:07,994 - For batch 300 loss at 500 samples is 1.13160546875\n",
            "2024-01-14 10:01:08,846 - For batch 400 loss at 500 samples is 1.1415703125\n",
            "2024-01-14 10:01:09,721 - For batch 500 loss at 500 samples is 1.174890625\n",
            "2024-01-14 10:01:10,624 - For batch 600 loss at 500 samples is 1.11591796875\n",
            "2024-01-14 10:01:11,516 - For batch 700 loss at 500 samples is 1.10637109375\n",
            "2024-01-14 10:01:12,375 - For batch 800 loss at 500 samples is 1.1575703125\n",
            "2024-01-14 10:01:13,225 - For batch 900 loss at 500 samples is 1.111703125\n",
            "2024-01-14 10:01:14,066 - For batch 1000 loss at 500 samples is 1.14788671875\n",
            "2024-01-14 10:01:14,927 - For batch 1100 loss at 500 samples is 1.135015625\n",
            "2024-01-14 10:01:15,783 - For batch 1200 loss at 500 samples is 1.12116796875\n",
            "2024-01-14 10:01:16,723 - For batch 1300 loss at 500 samples is 1.12735546875\n",
            "2024-01-14 10:01:18,088 - For batch 1400 loss at 500 samples is 1.1164375\n",
            "2024-01-14 10:01:18,990 - LOSS train 1.1164375 valid 181.375\n",
            "2024-01-14 10:01:18,995 - Starting training at 10\n",
            "2024-01-14 10:01:20,096 - For batch 100 loss at 500 samples is 1.12286328125\n",
            "2024-01-14 10:01:20,986 - For batch 200 loss at 500 samples is 1.10375390625\n",
            "2024-01-14 10:01:21,861 - For batch 300 loss at 500 samples is 1.10940234375\n",
            "2024-01-14 10:01:22,737 - For batch 400 loss at 500 samples is 1.13987109375\n",
            "2024-01-14 10:01:23,644 - For batch 500 loss at 500 samples is 1.1630703125\n",
            "2024-01-14 10:01:24,521 - For batch 600 loss at 500 samples is 1.10353515625\n",
            "2024-01-14 10:01:25,380 - For batch 700 loss at 500 samples is 1.1020390625\n",
            "2024-01-14 10:01:26,253 - For batch 800 loss at 500 samples is 1.14867578125\n",
            "2024-01-14 10:01:27,107 - For batch 900 loss at 500 samples is 1.12153125\n",
            "2024-01-14 10:01:28,017 - For batch 1000 loss at 500 samples is 1.15284375\n",
            "2024-01-14 10:01:28,962 - For batch 1100 loss at 500 samples is 1.14150390625\n",
            "2024-01-14 10:01:29,926 - For batch 1200 loss at 500 samples is 1.1231640625\n",
            "2024-01-14 10:01:31,276 - For batch 1300 loss at 500 samples is 1.12658203125\n",
            "2024-01-14 10:01:32,619 - For batch 1400 loss at 500 samples is 1.1045390625\n",
            "2024-01-14 10:01:33,287 - LOSS train 1.1045390625 valid 165.375\n",
            "2024-01-14 10:01:33,292 - Starting training at 11\n",
            "2024-01-14 10:01:34,146 - For batch 100 loss at 500 samples is 1.12090234375\n",
            "2024-01-14 10:01:35,004 - For batch 200 loss at 500 samples is 1.11790625\n",
            "2024-01-14 10:01:35,851 - For batch 300 loss at 500 samples is 1.138453125\n",
            "2024-01-14 10:01:36,690 - For batch 400 loss at 500 samples is 1.15539453125\n",
            "2024-01-14 10:01:37,557 - For batch 500 loss at 500 samples is 1.16112109375\n",
            "2024-01-14 10:01:38,399 - For batch 600 loss at 500 samples is 1.09840625\n",
            "2024-01-14 10:01:39,261 - For batch 700 loss at 500 samples is 1.11084765625\n",
            "2024-01-14 10:01:40,113 - For batch 800 loss at 500 samples is 1.16083984375\n",
            "2024-01-14 10:01:40,978 - For batch 900 loss at 500 samples is 1.12977734375\n",
            "2024-01-14 10:01:41,845 - For batch 1000 loss at 500 samples is 1.14796484375\n",
            "2024-01-14 10:01:42,700 - For batch 1100 loss at 500 samples is 1.13226953125\n",
            "2024-01-14 10:01:43,939 - For batch 1200 loss at 500 samples is 1.1114921875\n",
            "2024-01-14 10:01:45,284 - For batch 1300 loss at 500 samples is 1.119953125\n",
            "2024-01-14 10:01:46,409 - For batch 1400 loss at 500 samples is 1.11390234375\n",
            "2024-01-14 10:01:46,912 - LOSS train 1.11390234375 valid 151.25\n",
            "2024-01-14 10:01:46,918 - Starting training at 12\n",
            "2024-01-14 10:01:47,771 - For batch 100 loss at 500 samples is 1.13506640625\n",
            "2024-01-14 10:01:48,653 - For batch 200 loss at 500 samples is 1.1116171875\n",
            "2024-01-14 10:01:49,502 - For batch 300 loss at 500 samples is 1.11646875\n",
            "2024-01-14 10:01:50,332 - For batch 400 loss at 500 samples is 1.14116796875\n",
            "2024-01-14 10:01:51,166 - For batch 500 loss at 500 samples is 1.155609375\n",
            "2024-01-14 10:01:52,015 - For batch 600 loss at 500 samples is 1.09365234375\n",
            "2024-01-14 10:01:52,865 - For batch 700 loss at 500 samples is 1.1111953125\n",
            "2024-01-14 10:01:53,749 - For batch 800 loss at 500 samples is 1.14851171875\n",
            "2024-01-14 10:01:54,597 - For batch 900 loss at 500 samples is 1.117078125\n",
            "2024-01-14 10:01:55,445 - For batch 1000 loss at 500 samples is 1.14134375\n",
            "2024-01-14 10:01:56,434 - For batch 1100 loss at 500 samples is 1.12933203125\n",
            "2024-01-14 10:01:57,786 - For batch 1200 loss at 500 samples is 1.115921875\n",
            "2024-01-14 10:01:59,138 - For batch 1300 loss at 500 samples is 1.10658203125\n",
            "2024-01-14 10:02:00,037 - For batch 1400 loss at 500 samples is 1.09504296875\n",
            "2024-01-14 10:02:00,603 - LOSS train 1.09504296875 valid 139.875\n",
            "2024-01-14 10:02:00,605 - Starting training at 13\n",
            "2024-01-14 10:02:01,507 - For batch 100 loss at 500 samples is 1.12654296875\n",
            "2024-01-14 10:02:02,389 - For batch 200 loss at 500 samples is 1.10421875\n",
            "2024-01-14 10:02:03,268 - For batch 300 loss at 500 samples is 1.11132421875\n",
            "2024-01-14 10:02:04,130 - For batch 400 loss at 500 samples is 1.14083203125\n",
            "2024-01-14 10:02:04,994 - For batch 500 loss at 500 samples is 1.157609375\n",
            "2024-01-14 10:02:05,862 - For batch 600 loss at 500 samples is 1.09044921875\n",
            "2024-01-14 10:02:06,716 - For batch 700 loss at 500 samples is 1.10162890625\n",
            "2024-01-14 10:02:07,573 - For batch 800 loss at 500 samples is 1.14582421875\n",
            "2024-01-14 10:02:08,434 - For batch 900 loss at 500 samples is 1.12028515625\n",
            "2024-01-14 10:02:09,354 - For batch 1000 loss at 500 samples is 1.139703125\n",
            "2024-01-14 10:02:10,713 - For batch 1100 loss at 500 samples is 1.1216640625\n",
            "2024-01-14 10:02:12,049 - For batch 1200 loss at 500 samples is 1.106359375\n",
            "2024-01-14 10:02:13,090 - For batch 1300 loss at 500 samples is 1.10976171875\n",
            "2024-01-14 10:02:13,947 - For batch 1400 loss at 500 samples is 1.11427734375\n",
            "2024-01-14 10:02:14,503 - LOSS train 1.11427734375 valid 130.625\n",
            "2024-01-14 10:02:14,508 - Starting training at 14\n",
            "2024-01-14 10:02:15,374 - For batch 100 loss at 500 samples is 1.1359765625\n",
            "2024-01-14 10:02:16,223 - For batch 200 loss at 500 samples is 1.1111171875\n",
            "2024-01-14 10:02:17,115 - For batch 300 loss at 500 samples is 1.1148046875\n",
            "2024-01-14 10:02:17,975 - For batch 400 loss at 500 samples is 1.13494921875\n",
            "2024-01-14 10:02:18,860 - For batch 500 loss at 500 samples is 1.1385625\n",
            "2024-01-14 10:02:19,727 - For batch 600 loss at 500 samples is 1.09291015625\n",
            "2024-01-14 10:02:20,591 - For batch 700 loss at 500 samples is 1.109296875\n",
            "2024-01-14 10:02:21,467 - For batch 800 loss at 500 samples is 1.1589296875\n",
            "2024-01-14 10:02:22,304 - For batch 900 loss at 500 samples is 1.1172734375\n",
            "2024-01-14 10:02:23,568 - For batch 1000 loss at 500 samples is 1.1404375\n",
            "2024-01-14 10:02:24,965 - For batch 1100 loss at 500 samples is 1.13184375\n",
            "2024-01-14 10:02:26,084 - For batch 1200 loss at 500 samples is 1.1243359375\n",
            "2024-01-14 10:02:26,944 - For batch 1300 loss at 500 samples is 1.11147265625\n",
            "2024-01-14 10:02:27,813 - For batch 1400 loss at 500 samples is 1.11437890625\n",
            "2024-01-14 10:02:28,314 - LOSS train 1.11437890625 valid 121.0625\n",
            "2024-01-14 10:02:28,321 - Starting training at 15\n",
            "2024-01-14 10:02:29,201 - For batch 100 loss at 500 samples is 1.135921875\n",
            "2024-01-14 10:02:30,066 - For batch 200 loss at 500 samples is 1.10792578125\n",
            "2024-01-14 10:02:30,931 - For batch 300 loss at 500 samples is 1.10765625\n",
            "2024-01-14 10:02:31,778 - For batch 400 loss at 500 samples is 1.13878125\n",
            "2024-01-14 10:02:32,628 - For batch 500 loss at 500 samples is 1.14098046875\n",
            "2024-01-14 10:02:33,512 - For batch 600 loss at 500 samples is 1.07991015625\n",
            "2024-01-14 10:02:34,350 - For batch 700 loss at 500 samples is 1.09816796875\n",
            "2024-01-14 10:02:35,222 - For batch 800 loss at 500 samples is 1.14880078125\n",
            "2024-01-14 10:02:36,321 - For batch 900 loss at 500 samples is 1.11737890625\n",
            "2024-01-14 10:02:37,681 - For batch 1000 loss at 500 samples is 1.13270703125\n",
            "2024-01-14 10:02:38,986 - For batch 1100 loss at 500 samples is 1.12524609375\n",
            "2024-01-14 10:02:39,843 - For batch 1200 loss at 500 samples is 1.0922109375\n",
            "2024-01-14 10:02:40,709 - For batch 1300 loss at 500 samples is 1.09826171875\n",
            "2024-01-14 10:02:41,583 - For batch 1400 loss at 500 samples is 1.1075546875\n",
            "2024-01-14 10:02:42,158 - LOSS train 1.1075546875 valid 115.1875\n",
            "2024-01-14 10:02:42,160 - Starting training at 16\n",
            "2024-01-14 10:02:43,025 - For batch 100 loss at 500 samples is 1.1391953125\n",
            "2024-01-14 10:02:43,906 - For batch 200 loss at 500 samples is 1.12405078125\n",
            "2024-01-14 10:02:44,750 - For batch 300 loss at 500 samples is 1.1291328125\n",
            "2024-01-14 10:02:45,617 - For batch 400 loss at 500 samples is 1.162\n",
            "2024-01-14 10:02:46,480 - For batch 500 loss at 500 samples is 1.15783984375\n",
            "2024-01-14 10:02:47,411 - For batch 600 loss at 500 samples is 1.0867578125\n",
            "2024-01-14 10:02:48,307 - For batch 700 loss at 500 samples is 1.11572265625\n",
            "2024-01-14 10:02:49,415 - For batch 800 loss at 500 samples is 1.1814453125\n",
            "2024-01-14 10:02:50,807 - For batch 900 loss at 500 samples is 1.16214453125\n",
            "2024-01-14 10:02:52,160 - For batch 1000 loss at 500 samples is 1.16876171875\n",
            "2024-01-14 10:02:53,012 - For batch 1100 loss at 500 samples is 1.17728515625\n",
            "2024-01-14 10:02:53,884 - For batch 1200 loss at 500 samples is 1.139328125\n",
            "2024-01-14 10:02:54,818 - For batch 1300 loss at 500 samples is 1.14592578125\n",
            "2024-01-14 10:02:55,710 - For batch 1400 loss at 500 samples is 1.1064453125\n",
            "2024-01-14 10:02:56,315 - LOSS train 1.1064453125 valid 105.4375\n",
            "2024-01-14 10:02:56,319 - Starting training at 17\n",
            "2024-01-14 10:02:57,189 - For batch 100 loss at 500 samples is 1.13565625\n",
            "2024-01-14 10:02:58,038 - For batch 200 loss at 500 samples is 1.13509375\n",
            "2024-01-14 10:02:58,915 - For batch 300 loss at 500 samples is 1.133\n",
            "2024-01-14 10:02:59,784 - For batch 400 loss at 500 samples is 1.14941015625\n",
            "2024-01-14 10:03:00,638 - For batch 500 loss at 500 samples is 1.1768125\n",
            "2024-01-14 10:03:01,539 - For batch 600 loss at 500 samples is 1.1016484375\n",
            "2024-01-14 10:03:02,595 - For batch 700 loss at 500 samples is 1.10515234375\n",
            "2024-01-14 10:03:03,947 - For batch 800 loss at 500 samples is 1.148453125\n",
            "2024-01-14 10:03:05,318 - For batch 900 loss at 500 samples is 1.1277734375\n",
            "2024-01-14 10:03:06,175 - For batch 1000 loss at 500 samples is 1.142046875\n",
            "2024-01-14 10:03:07,036 - For batch 1100 loss at 500 samples is 1.1376953125\n",
            "2024-01-14 10:03:07,894 - For batch 1200 loss at 500 samples is 1.1138828125\n",
            "2024-01-14 10:03:08,752 - For batch 1300 loss at 500 samples is 1.113234375\n",
            "2024-01-14 10:03:09,619 - For batch 1400 loss at 500 samples is 1.0931015625\n",
            "2024-01-14 10:03:10,128 - LOSS train 1.0931015625 valid 102.125\n",
            "2024-01-14 10:03:10,130 - Starting training at 18\n",
            "2024-01-14 10:03:11,019 - For batch 100 loss at 500 samples is 1.13951171875\n",
            "2024-01-14 10:03:11,921 - For batch 200 loss at 500 samples is 1.11262109375\n",
            "2024-01-14 10:03:12,866 - For batch 300 loss at 500 samples is 1.12766015625\n",
            "2024-01-14 10:03:13,799 - For batch 400 loss at 500 samples is 1.14546875\n",
            "2024-01-14 10:03:14,710 - For batch 500 loss at 500 samples is 1.162109375\n",
            "2024-01-14 10:03:15,785 - For batch 600 loss at 500 samples is 1.09476171875\n",
            "2024-01-14 10:03:17,147 - For batch 700 loss at 500 samples is 1.1146015625\n",
            "2024-01-14 10:03:18,496 - For batch 800 loss at 500 samples is 1.16111328125\n",
            "2024-01-14 10:03:19,388 - For batch 900 loss at 500 samples is 1.131171875\n",
            "2024-01-14 10:03:20,303 - For batch 1000 loss at 500 samples is 1.16183984375\n",
            "2024-01-14 10:03:21,242 - For batch 1100 loss at 500 samples is 1.16170703125\n",
            "2024-01-14 10:03:22,187 - For batch 1200 loss at 500 samples is 1.12540234375\n",
            "2024-01-14 10:03:23,091 - For batch 1300 loss at 500 samples is 1.12395703125\n",
            "2024-01-14 10:03:24,016 - For batch 1400 loss at 500 samples is 1.095984375\n",
            "2024-01-14 10:03:24,618 - LOSS train 1.095984375 valid 95.4375\n",
            "2024-01-14 10:03:24,621 - Starting training at 19\n",
            "2024-01-14 10:03:25,519 - For batch 100 loss at 500 samples is 1.14489453125\n",
            "2024-01-14 10:03:26,386 - For batch 200 loss at 500 samples is 1.13454296875\n",
            "2024-01-14 10:03:27,233 - For batch 300 loss at 500 samples is 1.15326171875\n",
            "2024-01-14 10:03:28,095 - For batch 400 loss at 500 samples is 1.17940625\n",
            "2024-01-14 10:03:29,272 - For batch 500 loss at 500 samples is 1.20717578125\n",
            "2024-01-14 10:03:30,630 - For batch 600 loss at 500 samples is 1.13532421875\n",
            "2024-01-14 10:03:31,881 - For batch 700 loss at 500 samples is 1.13004296875\n",
            "2024-01-14 10:03:32,730 - For batch 800 loss at 500 samples is 1.17956640625\n",
            "2024-01-14 10:03:33,581 - For batch 900 loss at 500 samples is 1.1437578125\n",
            "2024-01-14 10:03:34,440 - For batch 1000 loss at 500 samples is 1.15590234375\n",
            "2024-01-14 10:03:35,285 - For batch 1100 loss at 500 samples is 1.1518515625\n",
            "2024-01-14 10:03:36,156 - For batch 1200 loss at 500 samples is 1.14103515625\n",
            "2024-01-14 10:03:37,016 - For batch 1300 loss at 500 samples is 1.13745703125\n",
            "2024-01-14 10:03:37,866 - For batch 1400 loss at 500 samples is 1.1275703125\n",
            "2024-01-14 10:03:38,419 - LOSS train 1.1275703125 valid 90.5625\n",
            "2024-01-14 10:03:38,421 - Starting training at 20\n",
            "2024-01-14 10:03:39,299 - For batch 100 loss at 500 samples is 1.16530078125\n",
            "2024-01-14 10:03:40,143 - For batch 200 loss at 500 samples is 1.1636015625\n",
            "2024-01-14 10:03:41,037 - For batch 300 loss at 500 samples is 1.15040625\n",
            "2024-01-14 10:03:42,044 - For batch 400 loss at 500 samples is 1.170046875\n",
            "2024-01-14 10:03:43,393 - For batch 500 loss at 500 samples is 1.18401953125\n",
            "2024-01-14 10:03:44,753 - For batch 600 loss at 500 samples is 1.09376953125\n",
            "2024-01-14 10:03:45,665 - For batch 700 loss at 500 samples is 1.111640625\n",
            "2024-01-14 10:03:46,519 - For batch 800 loss at 500 samples is 1.18762109375\n",
            "2024-01-14 10:03:47,395 - For batch 900 loss at 500 samples is 1.16333984375\n",
            "2024-01-14 10:03:48,283 - For batch 1000 loss at 500 samples is 1.16617578125\n",
            "2024-01-14 10:03:49,170 - For batch 1100 loss at 500 samples is 1.14637109375\n",
            "2024-01-14 10:03:50,031 - For batch 1200 loss at 500 samples is 1.12406640625\n",
            "2024-01-14 10:03:50,889 - For batch 1300 loss at 500 samples is 1.11128125\n",
            "2024-01-14 10:03:51,738 - For batch 1400 loss at 500 samples is 1.09866015625\n",
            "2024-01-14 10:03:52,315 - LOSS train 1.09866015625 valid 86.125\n",
            "2024-01-14 10:03:52,318 - Starting training at 21\n",
            "2024-01-14 10:03:53,184 - For batch 100 loss at 500 samples is 1.162046875\n",
            "2024-01-14 10:03:54,035 - For batch 200 loss at 500 samples is 1.14175390625\n",
            "2024-01-14 10:03:54,901 - For batch 300 loss at 500 samples is 1.1551953125\n",
            "2024-01-14 10:03:56,283 - For batch 400 loss at 500 samples is 1.16554296875\n",
            "2024-01-14 10:03:57,673 - For batch 500 loss at 500 samples is 1.18515234375\n",
            "2024-01-14 10:03:58,712 - For batch 600 loss at 500 samples is 1.11401171875\n",
            "2024-01-14 10:03:59,592 - For batch 700 loss at 500 samples is 1.12747265625\n",
            "2024-01-14 10:04:00,452 - For batch 800 loss at 500 samples is 1.209359375\n",
            "2024-01-14 10:04:01,332 - For batch 900 loss at 500 samples is 1.1909765625\n",
            "2024-01-14 10:04:02,166 - For batch 1000 loss at 500 samples is 1.18092578125\n",
            "2024-01-14 10:04:03,024 - For batch 1100 loss at 500 samples is 1.1848984375\n",
            "2024-01-14 10:04:03,885 - For batch 1200 loss at 500 samples is 1.1552578125\n",
            "2024-01-14 10:04:04,772 - For batch 1300 loss at 500 samples is 1.177921875\n",
            "2024-01-14 10:04:05,775 - For batch 1400 loss at 500 samples is 1.14434765625\n",
            "2024-01-14 10:04:06,458 - LOSS train 1.14434765625 valid 81.375\n",
            "2024-01-14 10:04:06,461 - Starting training at 22\n",
            "2024-01-14 10:04:07,354 - For batch 100 loss at 500 samples is 1.1885234375\n",
            "2024-01-14 10:04:08,302 - For batch 200 loss at 500 samples is 1.1883671875\n",
            "2024-01-14 10:04:09,693 - For batch 300 loss at 500 samples is 1.1595234375\n",
            "2024-01-14 10:04:11,089 - For batch 400 loss at 500 samples is 1.21396484375\n",
            "2024-01-14 10:04:12,034 - For batch 500 loss at 500 samples is 1.170171875\n",
            "2024-01-14 10:04:12,902 - For batch 600 loss at 500 samples is 1.124734375\n",
            "2024-01-14 10:04:13,746 - For batch 700 loss at 500 samples is 1.14419921875\n",
            "2024-01-14 10:04:14,583 - For batch 800 loss at 500 samples is 1.1964375\n",
            "2024-01-14 10:04:15,435 - For batch 900 loss at 500 samples is 1.16776171875\n",
            "2024-01-14 10:04:16,305 - For batch 1000 loss at 500 samples is 1.15479296875\n",
            "2024-01-14 10:04:17,194 - For batch 1100 loss at 500 samples is 1.1634609375\n",
            "2024-01-14 10:04:18,053 - For batch 1200 loss at 500 samples is 1.12150390625\n",
            "2024-01-14 10:04:18,926 - For batch 1300 loss at 500 samples is 1.12365625\n",
            "2024-01-14 10:04:19,759 - For batch 1400 loss at 500 samples is 1.11735546875\n",
            "2024-01-14 10:04:20,338 - LOSS train 1.11735546875 valid 77.75\n",
            "2024-01-14 10:04:20,340 - Starting training at 23\n",
            "2024-01-14 10:04:21,238 - For batch 100 loss at 500 samples is 1.17875\n",
            "2024-01-14 10:04:22,529 - For batch 200 loss at 500 samples is 1.16166015625\n",
            "2024-01-14 10:04:23,909 - For batch 300 loss at 500 samples is 1.15703125\n",
            "2024-01-14 10:04:24,992 - For batch 400 loss at 500 samples is 1.210484375\n",
            "2024-01-14 10:04:25,857 - For batch 500 loss at 500 samples is 1.1879140625\n",
            "2024-01-14 10:04:26,698 - For batch 600 loss at 500 samples is 1.13066015625\n",
            "2024-01-14 10:04:27,586 - For batch 700 loss at 500 samples is 1.16198046875\n",
            "2024-01-14 10:04:28,461 - For batch 800 loss at 500 samples is 1.20992578125\n",
            "2024-01-14 10:04:29,329 - For batch 900 loss at 500 samples is 1.17586328125\n",
            "2024-01-14 10:04:30,191 - For batch 1000 loss at 500 samples is 1.15922265625\n",
            "2024-01-14 10:04:31,079 - For batch 1100 loss at 500 samples is 1.1730390625\n",
            "2024-01-14 10:04:31,959 - For batch 1200 loss at 500 samples is 1.13865625\n",
            "2024-01-14 10:04:32,841 - For batch 1300 loss at 500 samples is 1.13204296875\n",
            "2024-01-14 10:04:33,710 - For batch 1400 loss at 500 samples is 1.11765625\n",
            "2024-01-14 10:04:34,296 - LOSS train 1.11765625 valid 75.0625\n",
            "2024-01-14 10:04:34,298 - Starting training at 24\n",
            "2024-01-14 10:04:35,542 - For batch 100 loss at 500 samples is 1.16441796875\n",
            "2024-01-14 10:04:36,919 - For batch 200 loss at 500 samples is 1.16773046875\n",
            "2024-01-14 10:04:38,063 - For batch 300 loss at 500 samples is 1.15983203125\n",
            "2024-01-14 10:04:38,929 - For batch 400 loss at 500 samples is 1.2131015625\n",
            "2024-01-14 10:04:39,802 - For batch 500 loss at 500 samples is 1.1987890625\n",
            "2024-01-14 10:04:40,728 - For batch 600 loss at 500 samples is 1.13097265625\n",
            "2024-01-14 10:04:41,635 - For batch 700 loss at 500 samples is 1.1480078125\n",
            "2024-01-14 10:04:42,519 - For batch 800 loss at 500 samples is 1.19230859375\n",
            "2024-01-14 10:04:43,419 - For batch 900 loss at 500 samples is 1.17325390625\n",
            "2024-01-14 10:04:44,328 - For batch 1000 loss at 500 samples is 1.1863125\n",
            "2024-01-14 10:04:45,211 - For batch 1100 loss at 500 samples is 1.1784375\n",
            "2024-01-14 10:04:46,066 - For batch 1200 loss at 500 samples is 1.15213671875\n",
            "2024-01-14 10:04:46,954 - For batch 1300 loss at 500 samples is 1.137859375\n",
            "2024-01-14 10:04:47,981 - For batch 1400 loss at 500 samples is 1.12699609375\n",
            "2024-01-14 10:04:48,783 - LOSS train 1.12699609375 valid 71.8125\n",
            "2024-01-14 10:04:48,796 - Starting training at 25\n",
            "2024-01-14 10:04:50,163 - For batch 100 loss at 500 samples is 1.182296875\n",
            "2024-01-14 10:04:51,323 - For batch 200 loss at 500 samples is 1.17131640625\n",
            "2024-01-14 10:04:52,183 - For batch 300 loss at 500 samples is 1.16719921875\n",
            "2024-01-14 10:04:53,053 - For batch 400 loss at 500 samples is 1.202453125\n",
            "2024-01-14 10:04:53,971 - For batch 500 loss at 500 samples is 1.2055390625\n",
            "2024-01-14 10:04:54,814 - For batch 600 loss at 500 samples is 1.13584765625\n",
            "2024-01-14 10:04:55,706 - For batch 700 loss at 500 samples is 1.14133984375\n",
            "2024-01-14 10:04:56,592 - For batch 800 loss at 500 samples is 1.2089140625\n",
            "2024-01-14 10:04:57,458 - For batch 900 loss at 500 samples is 1.18124609375\n",
            "2024-01-14 10:04:58,340 - For batch 1000 loss at 500 samples is 1.171859375\n",
            "2024-01-14 10:04:59,228 - For batch 1100 loss at 500 samples is 1.17828125\n",
            "2024-01-14 10:05:00,106 - For batch 1200 loss at 500 samples is 1.1570390625\n",
            "2024-01-14 10:05:00,992 - For batch 1300 loss at 500 samples is 1.1420390625\n",
            "2024-01-14 10:05:02,346 - For batch 1400 loss at 500 samples is 1.146953125\n",
            "2024-01-14 10:05:03,260 - LOSS train 1.146953125 valid 69.625\n",
            "2024-01-14 10:05:03,263 - Starting training at 26\n",
            "2024-01-14 10:05:04,437 - For batch 100 loss at 500 samples is 1.177484375\n",
            "2024-01-14 10:05:05,300 - For batch 200 loss at 500 samples is 1.18708203125\n",
            "2024-01-14 10:05:06,184 - For batch 300 loss at 500 samples is 1.14565625\n",
            "2024-01-14 10:05:07,050 - For batch 400 loss at 500 samples is 1.2162421875\n",
            "2024-01-14 10:05:07,895 - For batch 500 loss at 500 samples is 1.196359375\n",
            "2024-01-14 10:05:08,750 - For batch 600 loss at 500 samples is 1.1178359375\n",
            "2024-01-14 10:05:09,637 - For batch 700 loss at 500 samples is 1.14783984375\n",
            "2024-01-14 10:05:10,502 - For batch 800 loss at 500 samples is 1.1964921875\n",
            "2024-01-14 10:05:11,355 - For batch 900 loss at 500 samples is 1.18003515625\n",
            "2024-01-14 10:05:12,224 - For batch 1000 loss at 500 samples is 1.16314453125\n",
            "2024-01-14 10:05:13,094 - For batch 1100 loss at 500 samples is 1.1913828125\n",
            "2024-01-14 10:05:13,940 - For batch 1200 loss at 500 samples is 1.13853125\n",
            "2024-01-14 10:05:15,179 - For batch 1300 loss at 500 samples is 1.1343515625\n",
            "2024-01-14 10:05:16,496 - For batch 1400 loss at 500 samples is 1.14020703125\n",
            "2024-01-14 10:05:17,358 - LOSS train 1.14020703125 valid 67.1875\n",
            "2024-01-14 10:05:17,363 - Starting training at 27\n",
            "2024-01-14 10:05:18,247 - For batch 100 loss at 500 samples is 1.181\n",
            "2024-01-14 10:05:19,134 - For batch 200 loss at 500 samples is 1.1746640625\n",
            "2024-01-14 10:05:19,987 - For batch 300 loss at 500 samples is 1.176484375\n",
            "2024-01-14 10:05:20,855 - For batch 400 loss at 500 samples is 1.2159140625\n",
            "2024-01-14 10:05:21,706 - For batch 500 loss at 500 samples is 1.17799609375\n",
            "2024-01-14 10:05:22,536 - For batch 600 loss at 500 samples is 1.138484375\n",
            "2024-01-14 10:05:23,426 - For batch 700 loss at 500 samples is 1.17031640625\n",
            "2024-01-14 10:05:24,275 - For batch 800 loss at 500 samples is 1.21633203125\n",
            "2024-01-14 10:05:25,127 - For batch 900 loss at 500 samples is 1.1925390625\n",
            "2024-01-14 10:05:25,972 - For batch 1000 loss at 500 samples is 1.188484375\n",
            "2024-01-14 10:05:26,846 - For batch 1100 loss at 500 samples is 1.203984375\n",
            "2024-01-14 10:05:27,945 - For batch 1200 loss at 500 samples is 1.15653515625\n",
            "2024-01-14 10:05:29,297 - For batch 1300 loss at 500 samples is 1.14684765625\n",
            "2024-01-14 10:05:30,603 - For batch 1400 loss at 500 samples is 1.14314453125\n",
            "2024-01-14 10:05:31,145 - LOSS train 1.14314453125 valid 64.0625\n",
            "2024-01-14 10:05:31,150 - Starting training at 28\n",
            "2024-01-14 10:05:32,000 - For batch 100 loss at 500 samples is 1.1945703125\n",
            "2024-01-14 10:05:32,853 - For batch 200 loss at 500 samples is 1.18894921875\n",
            "2024-01-14 10:05:33,700 - For batch 300 loss at 500 samples is 1.17671484375\n",
            "2024-01-14 10:05:34,543 - For batch 400 loss at 500 samples is 1.232390625\n",
            "2024-01-14 10:05:35,393 - For batch 500 loss at 500 samples is 1.21178515625\n",
            "2024-01-14 10:05:36,263 - For batch 600 loss at 500 samples is 1.13247265625\n",
            "2024-01-14 10:05:37,161 - For batch 700 loss at 500 samples is 1.17724609375\n",
            "2024-01-14 10:05:38,010 - For batch 800 loss at 500 samples is 1.21130859375\n",
            "2024-01-14 10:05:38,862 - For batch 900 loss at 500 samples is 1.1927265625\n",
            "2024-01-14 10:05:39,726 - For batch 1000 loss at 500 samples is 1.17766796875\n",
            "2024-01-14 10:05:40,626 - For batch 1100 loss at 500 samples is 1.1980859375\n",
            "2024-01-14 10:05:41,970 - For batch 1200 loss at 500 samples is 1.17001953125\n",
            "2024-01-14 10:05:43,314 - For batch 1300 loss at 500 samples is 1.14459765625\n",
            "2024-01-14 10:05:44,305 - For batch 1400 loss at 500 samples is 1.13184375\n",
            "2024-01-14 10:05:44,861 - LOSS train 1.13184375 valid 61.71875\n",
            "2024-01-14 10:05:44,863 - Starting training at 29\n",
            "2024-01-14 10:05:45,771 - For batch 100 loss at 500 samples is 1.17259375\n",
            "2024-01-14 10:05:46,635 - For batch 200 loss at 500 samples is 1.18222265625\n",
            "2024-01-14 10:05:47,495 - For batch 300 loss at 500 samples is 1.17530078125\n",
            "2024-01-14 10:05:48,387 - For batch 400 loss at 500 samples is 1.2238828125\n",
            "2024-01-14 10:05:49,314 - For batch 500 loss at 500 samples is 1.1943359375\n",
            "2024-01-14 10:05:50,170 - For batch 600 loss at 500 samples is 1.14801953125\n",
            "2024-01-14 10:05:51,094 - For batch 700 loss at 500 samples is 1.1685234375\n",
            "2024-01-14 10:05:51,930 - For batch 800 loss at 500 samples is 1.20746875\n",
            "2024-01-14 10:05:52,780 - For batch 900 loss at 500 samples is 1.2073125\n",
            "2024-01-14 10:05:53,612 - For batch 1000 loss at 500 samples is 1.1951640625\n",
            "2024-01-14 10:05:54,872 - For batch 1100 loss at 500 samples is 1.18040625\n",
            "2024-01-14 10:05:56,290 - For batch 1200 loss at 500 samples is 1.19069921875\n",
            "2024-01-14 10:05:57,467 - For batch 1300 loss at 500 samples is 1.1645859375\n",
            "2024-01-14 10:05:58,348 - For batch 1400 loss at 500 samples is 1.12580859375\n",
            "2024-01-14 10:05:58,922 - LOSS train 1.12580859375 valid 60.28125\n",
            "2024-01-14 10:05:58,928 - Starting training at 30\n",
            "2024-01-14 10:05:59,778 - For batch 100 loss at 500 samples is 1.1926484375\n",
            "2024-01-14 10:06:00,732 - For batch 200 loss at 500 samples is 1.1785078125\n",
            "2024-01-14 10:06:01,615 - For batch 300 loss at 500 samples is 1.17566796875\n",
            "2024-01-14 10:06:02,487 - For batch 400 loss at 500 samples is 1.22008984375\n",
            "2024-01-14 10:06:03,381 - For batch 500 loss at 500 samples is 1.21745703125\n",
            "2024-01-14 10:06:04,269 - For batch 600 loss at 500 samples is 1.1442109375\n",
            "2024-01-14 10:06:05,165 - For batch 700 loss at 500 samples is 1.16821484375\n",
            "2024-01-14 10:06:06,024 - For batch 800 loss at 500 samples is 1.2262265625\n",
            "2024-01-14 10:06:06,878 - For batch 900 loss at 500 samples is 1.1873203125\n",
            "2024-01-14 10:06:08,170 - For batch 1000 loss at 500 samples is 1.1839609375\n",
            "2024-01-14 10:06:09,524 - For batch 1100 loss at 500 samples is 1.22296484375\n",
            "2024-01-14 10:06:10,650 - For batch 1200 loss at 500 samples is 1.16033984375\n",
            "2024-01-14 10:06:11,517 - For batch 1300 loss at 500 samples is 1.1695859375\n",
            "2024-01-14 10:06:12,423 - For batch 1400 loss at 500 samples is 1.13936328125\n",
            "2024-01-14 10:06:12,984 - LOSS train 1.13936328125 valid 57.875\n",
            "2024-01-14 10:06:12,988 - Starting training at 31\n",
            "2024-01-14 10:06:13,879 - For batch 100 loss at 500 samples is 1.19137109375\n",
            "2024-01-14 10:06:14,764 - For batch 200 loss at 500 samples is 1.17306640625\n",
            "2024-01-14 10:06:15,657 - For batch 300 loss at 500 samples is 1.2131953125\n",
            "2024-01-14 10:06:16,537 - For batch 400 loss at 500 samples is 1.22709375\n",
            "2024-01-14 10:06:17,441 - For batch 500 loss at 500 samples is 1.2303828125\n",
            "2024-01-14 10:06:18,302 - For batch 600 loss at 500 samples is 1.172609375\n",
            "2024-01-14 10:06:19,234 - For batch 700 loss at 500 samples is 1.1872109375\n",
            "2024-01-14 10:06:20,097 - For batch 800 loss at 500 samples is 1.21625390625\n",
            "2024-01-14 10:06:21,435 - For batch 900 loss at 500 samples is 1.18471484375\n",
            "2024-01-14 10:06:22,807 - For batch 1000 loss at 500 samples is 1.1746875\n",
            "2024-01-14 10:06:23,877 - For batch 1100 loss at 500 samples is 1.20797265625\n",
            "2024-01-14 10:06:24,748 - For batch 1200 loss at 500 samples is 1.18162890625\n",
            "2024-01-14 10:06:25,589 - For batch 1300 loss at 500 samples is 1.16890234375\n",
            "2024-01-14 10:06:26,472 - For batch 1400 loss at 500 samples is 1.14284765625\n",
            "2024-01-14 10:06:27,017 - LOSS train 1.14284765625 valid 56.03125\n",
            "2024-01-14 10:06:27,023 - Starting training at 32\n",
            "2024-01-14 10:06:27,919 - For batch 100 loss at 500 samples is 1.1930625\n",
            "2024-01-14 10:06:28,803 - For batch 200 loss at 500 samples is 1.1893125\n",
            "2024-01-14 10:06:29,663 - For batch 300 loss at 500 samples is 1.199375\n",
            "2024-01-14 10:06:30,545 - For batch 400 loss at 500 samples is 1.2105234375\n",
            "2024-01-14 10:06:31,420 - For batch 500 loss at 500 samples is 1.22994921875\n",
            "2024-01-14 10:06:32,296 - For batch 600 loss at 500 samples is 1.16675390625\n",
            "2024-01-14 10:06:33,176 - For batch 700 loss at 500 samples is 1.17170703125\n",
            "2024-01-14 10:06:34,389 - For batch 800 loss at 500 samples is 1.2165078125\n",
            "2024-01-14 10:06:35,726 - For batch 900 loss at 500 samples is 1.1950703125\n",
            "2024-01-14 10:06:36,884 - For batch 1000 loss at 500 samples is 1.1874609375\n",
            "2024-01-14 10:06:37,739 - For batch 1100 loss at 500 samples is 1.2075703125\n",
            "2024-01-14 10:06:38,590 - For batch 1200 loss at 500 samples is 1.1651015625\n",
            "2024-01-14 10:06:39,470 - For batch 1300 loss at 500 samples is 1.17001953125\n",
            "2024-01-14 10:06:40,343 - For batch 1400 loss at 500 samples is 1.1453828125\n",
            "2024-01-14 10:06:40,901 - LOSS train 1.1453828125 valid 54.375\n",
            "2024-01-14 10:06:40,903 - Starting training at 33\n",
            "2024-01-14 10:06:41,751 - For batch 100 loss at 500 samples is 1.199640625\n",
            "2024-01-14 10:06:42,608 - For batch 200 loss at 500 samples is 1.22979296875\n",
            "2024-01-14 10:06:43,475 - For batch 300 loss at 500 samples is 1.2395078125\n",
            "2024-01-14 10:06:44,344 - For batch 400 loss at 500 samples is 1.258\n",
            "2024-01-14 10:06:45,242 - For batch 500 loss at 500 samples is 1.2579921875\n",
            "2024-01-14 10:06:46,148 - For batch 600 loss at 500 samples is 1.18216796875\n",
            "2024-01-14 10:06:47,340 - For batch 700 loss at 500 samples is 1.19800390625\n",
            "2024-01-14 10:06:48,721 - For batch 800 loss at 500 samples is 1.27441015625\n",
            "2024-01-14 10:06:49,968 - For batch 900 loss at 500 samples is 1.234765625\n",
            "2024-01-14 10:06:50,823 - For batch 1000 loss at 500 samples is 1.221109375\n",
            "2024-01-14 10:06:51,771 - For batch 1100 loss at 500 samples is 1.23779296875\n",
            "2024-01-14 10:06:52,625 - For batch 1200 loss at 500 samples is 1.19898828125\n",
            "2024-01-14 10:06:53,499 - For batch 1300 loss at 500 samples is 1.21532421875\n",
            "2024-01-14 10:06:54,351 - For batch 1400 loss at 500 samples is 1.17074609375\n",
            "2024-01-14 10:06:54,931 - LOSS train 1.17074609375 valid 51.84375\n",
            "2024-01-14 10:06:54,934 - Starting training at 34\n",
            "2024-01-14 10:06:55,822 - For batch 100 loss at 500 samples is 1.2331796875\n",
            "2024-01-14 10:06:56,695 - For batch 200 loss at 500 samples is 1.2238046875\n",
            "2024-01-14 10:06:57,607 - For batch 300 loss at 500 samples is 1.2243671875\n",
            "2024-01-14 10:06:58,478 - For batch 400 loss at 500 samples is 1.23934375\n",
            "2024-01-14 10:06:59,335 - For batch 500 loss at 500 samples is 1.25201953125\n",
            "2024-01-14 10:07:00,442 - For batch 600 loss at 500 samples is 1.19425390625\n",
            "2024-01-14 10:07:01,820 - For batch 700 loss at 500 samples is 1.2076328125\n",
            "2024-01-14 10:07:03,112 - For batch 800 loss at 500 samples is 1.24312890625\n",
            "2024-01-14 10:07:03,955 - For batch 900 loss at 500 samples is 1.218390625\n",
            "2024-01-14 10:07:04,797 - For batch 1000 loss at 500 samples is 1.2157578125\n",
            "2024-01-14 10:07:05,630 - For batch 1100 loss at 500 samples is 1.23466796875\n",
            "2024-01-14 10:07:06,468 - For batch 1200 loss at 500 samples is 1.22036328125\n",
            "2024-01-14 10:07:07,309 - For batch 1300 loss at 500 samples is 1.206484375\n",
            "2024-01-14 10:07:08,148 - For batch 1400 loss at 500 samples is 1.197765625\n",
            "2024-01-14 10:07:08,639 - LOSS train 1.197765625 valid 49.78125\n",
            "2024-01-14 10:07:08,642 - Starting training at 35\n",
            "2024-01-14 10:07:09,519 - For batch 100 loss at 500 samples is 1.2438046875\n",
            "2024-01-14 10:07:10,376 - For batch 200 loss at 500 samples is 1.2390625\n",
            "2024-01-14 10:07:11,221 - For batch 300 loss at 500 samples is 1.23990625\n",
            "2024-01-14 10:07:12,075 - For batch 400 loss at 500 samples is 1.247921875\n",
            "2024-01-14 10:07:12,927 - For batch 500 loss at 500 samples is 1.2656953125\n",
            "2024-01-14 10:07:14,267 - For batch 600 loss at 500 samples is 1.220359375\n",
            "2024-01-14 10:07:15,621 - For batch 700 loss at 500 samples is 1.22305078125\n",
            "2024-01-14 10:07:16,686 - For batch 800 loss at 500 samples is 1.269390625\n",
            "2024-01-14 10:07:17,569 - For batch 900 loss at 500 samples is 1.20684765625\n",
            "2024-01-14 10:07:18,480 - For batch 1000 loss at 500 samples is 1.239734375\n",
            "2024-01-14 10:07:19,342 - For batch 1100 loss at 500 samples is 1.2673671875\n",
            "2024-01-14 10:07:20,256 - For batch 1200 loss at 500 samples is 1.2331875\n",
            "2024-01-14 10:07:21,180 - For batch 1300 loss at 500 samples is 1.2266015625\n",
            "2024-01-14 10:07:22,094 - For batch 1400 loss at 500 samples is 1.1815\n",
            "2024-01-14 10:07:22,684 - LOSS train 1.1815 valid 48.84375\n",
            "2024-01-14 10:07:22,686 - Starting training at 36\n",
            "2024-01-14 10:07:23,638 - For batch 100 loss at 500 samples is 1.246953125\n",
            "2024-01-14 10:07:24,587 - For batch 200 loss at 500 samples is 1.235015625\n",
            "2024-01-14 10:07:25,477 - For batch 300 loss at 500 samples is 1.2215859375\n",
            "2024-01-14 10:07:26,454 - For batch 400 loss at 500 samples is 1.23299609375\n",
            "2024-01-14 10:07:27,876 - For batch 500 loss at 500 samples is 1.25107421875\n",
            "2024-01-14 10:07:29,247 - For batch 600 loss at 500 samples is 1.1958125\n",
            "2024-01-14 10:07:30,251 - For batch 700 loss at 500 samples is 1.2078125\n",
            "2024-01-14 10:07:31,167 - For batch 800 loss at 500 samples is 1.25271875\n",
            "2024-01-14 10:07:32,071 - For batch 900 loss at 500 samples is 1.22473046875\n",
            "2024-01-14 10:07:32,938 - For batch 1000 loss at 500 samples is 1.234515625\n",
            "2024-01-14 10:07:33,825 - For batch 1100 loss at 500 samples is 1.24302734375\n",
            "2024-01-14 10:07:34,675 - For batch 1200 loss at 500 samples is 1.23243359375\n",
            "2024-01-14 10:07:35,559 - For batch 1300 loss at 500 samples is 1.2213046875\n",
            "2024-01-14 10:07:36,470 - For batch 1400 loss at 500 samples is 1.18553515625\n",
            "2024-01-14 10:07:37,071 - LOSS train 1.18553515625 valid 47.34375\n",
            "2024-01-14 10:07:37,074 - Starting training at 37\n",
            "2024-01-14 10:07:37,963 - For batch 100 loss at 500 samples is 1.25014453125\n",
            "2024-01-14 10:07:38,840 - For batch 200 loss at 500 samples is 1.26446875\n",
            "2024-01-14 10:07:39,877 - For batch 300 loss at 500 samples is 1.260234375\n",
            "2024-01-14 10:07:41,232 - For batch 400 loss at 500 samples is 1.2686015625\n",
            "2024-01-14 10:07:42,568 - For batch 500 loss at 500 samples is 1.27557421875\n",
            "2024-01-14 10:07:43,470 - For batch 600 loss at 500 samples is 1.2386015625\n",
            "2024-01-14 10:07:44,367 - For batch 700 loss at 500 samples is 1.22561328125\n",
            "2024-01-14 10:07:45,271 - For batch 800 loss at 500 samples is 1.251609375\n",
            "2024-01-14 10:07:46,189 - For batch 900 loss at 500 samples is 1.21974609375\n",
            "2024-01-14 10:07:47,114 - For batch 1000 loss at 500 samples is 1.254765625\n",
            "2024-01-14 10:07:47,974 - For batch 1100 loss at 500 samples is 1.2547265625\n",
            "2024-01-14 10:07:48,930 - For batch 1200 loss at 500 samples is 1.23659375\n",
            "2024-01-14 10:07:49,861 - For batch 1300 loss at 500 samples is 1.2187265625\n",
            "2024-01-14 10:07:50,756 - For batch 1400 loss at 500 samples is 1.191546875\n",
            "2024-01-14 10:07:51,290 - LOSS train 1.191546875 valid 46.1875\n",
            "2024-01-14 10:07:51,292 - Starting training at 38\n",
            "2024-01-14 10:07:52,153 - For batch 100 loss at 500 samples is 1.25665625\n",
            "2024-01-14 10:07:53,282 - For batch 200 loss at 500 samples is 1.25134375\n",
            "2024-01-14 10:07:54,640 - For batch 300 loss at 500 samples is 1.242296875\n",
            "2024-01-14 10:07:55,915 - For batch 400 loss at 500 samples is 1.236890625\n",
            "2024-01-14 10:07:56,777 - For batch 500 loss at 500 samples is 1.2689765625\n",
            "2024-01-14 10:07:57,672 - For batch 600 loss at 500 samples is 1.19355859375\n",
            "2024-01-14 10:07:58,579 - For batch 700 loss at 500 samples is 1.21162109375\n",
            "2024-01-14 10:07:59,477 - For batch 800 loss at 500 samples is 1.239125\n",
            "2024-01-14 10:08:00,324 - For batch 900 loss at 500 samples is 1.2334296875\n",
            "2024-01-14 10:08:01,200 - For batch 1000 loss at 500 samples is 1.2561328125\n",
            "2024-01-14 10:08:02,097 - For batch 1100 loss at 500 samples is 1.254703125\n",
            "2024-01-14 10:08:03,011 - For batch 1200 loss at 500 samples is 1.23634765625\n",
            "2024-01-14 10:08:03,876 - For batch 1300 loss at 500 samples is 1.24712890625\n",
            "2024-01-14 10:08:04,755 - For batch 1400 loss at 500 samples is 1.19494921875\n",
            "2024-01-14 10:08:05,340 - LOSS train 1.19494921875 valid 44.9375\n",
            "2024-01-14 10:08:05,342 - Starting training at 39\n",
            "2024-01-14 10:08:06,443 - For batch 100 loss at 500 samples is 1.2667734375\n",
            "2024-01-14 10:08:07,785 - For batch 200 loss at 500 samples is 1.26\n",
            "2024-01-14 10:08:09,084 - For batch 300 loss at 500 samples is 1.2811953125\n",
            "2024-01-14 10:08:09,948 - For batch 400 loss at 500 samples is 1.2683046875\n",
            "2024-01-14 10:08:10,798 - For batch 500 loss at 500 samples is 1.306078125\n",
            "2024-01-14 10:08:11,637 - For batch 600 loss at 500 samples is 1.25526171875\n",
            "2024-01-14 10:08:12,560 - For batch 700 loss at 500 samples is 1.26586328125\n",
            "2024-01-14 10:08:13,474 - For batch 800 loss at 500 samples is 1.31226171875\n",
            "2024-01-14 10:08:14,319 - For batch 900 loss at 500 samples is 1.2586484375\n",
            "2024-01-14 10:08:15,207 - For batch 1000 loss at 500 samples is 1.2829296875\n",
            "2024-01-14 10:08:16,042 - For batch 1100 loss at 500 samples is 1.3028671875\n",
            "2024-01-14 10:08:16,875 - For batch 1200 loss at 500 samples is 1.2742421875\n",
            "2024-01-14 10:08:17,718 - For batch 1300 loss at 500 samples is 1.259859375\n",
            "2024-01-14 10:08:18,608 - For batch 1400 loss at 500 samples is 1.22714453125\n",
            "2024-01-14 10:08:19,299 - LOSS train 1.22714453125 valid 43.34375\n",
            "2024-01-14 10:08:19,304 - Starting training at 40\n",
            "2024-01-14 10:08:20,655 - For batch 100 loss at 500 samples is 1.25928125\n",
            "2024-01-14 10:08:22,003 - For batch 200 loss at 500 samples is 1.3056796875\n",
            "2024-01-14 10:08:22,975 - For batch 300 loss at 500 samples is 1.273890625\n",
            "2024-01-14 10:08:23,875 - For batch 400 loss at 500 samples is 1.2891953125\n",
            "2024-01-14 10:08:24,720 - For batch 500 loss at 500 samples is 1.2871796875\n",
            "2024-01-14 10:08:25,624 - For batch 600 loss at 500 samples is 1.2269140625\n",
            "2024-01-14 10:08:26,496 - For batch 700 loss at 500 samples is 1.25271484375\n",
            "2024-01-14 10:08:27,351 - For batch 800 loss at 500 samples is 1.2650703125\n",
            "2024-01-14 10:08:28,238 - For batch 900 loss at 500 samples is 1.27642578125\n",
            "2024-01-14 10:08:29,156 - For batch 1000 loss at 500 samples is 1.27778125\n",
            "2024-01-14 10:08:30,077 - For batch 1100 loss at 500 samples is 1.2819140625\n",
            "2024-01-14 10:08:30,958 - For batch 1200 loss at 500 samples is 1.276734375\n",
            "2024-01-14 10:08:31,817 - For batch 1300 loss at 500 samples is 1.26146875\n",
            "2024-01-14 10:08:32,921 - For batch 1400 loss at 500 samples is 1.2213984375\n",
            "2024-01-14 10:08:33,832 - LOSS train 1.2213984375 valid 42.15625\n",
            "2024-01-14 10:08:33,837 - Starting training at 41\n",
            "2024-01-14 10:08:35,170 - For batch 100 loss at 500 samples is 1.2988203125\n",
            "2024-01-14 10:08:36,149 - For batch 200 loss at 500 samples is 1.3135859375\n",
            "2024-01-14 10:08:37,032 - For batch 300 loss at 500 samples is 1.2749296875\n",
            "2024-01-14 10:08:37,880 - For batch 400 loss at 500 samples is 1.3025703125\n",
            "2024-01-14 10:08:38,755 - For batch 500 loss at 500 samples is 1.2969453125\n",
            "2024-01-14 10:08:39,617 - For batch 600 loss at 500 samples is 1.26716796875\n",
            "2024-01-14 10:08:40,482 - For batch 700 loss at 500 samples is 1.2623515625\n",
            "2024-01-14 10:08:41,349 - For batch 800 loss at 500 samples is 1.294421875\n",
            "2024-01-14 10:08:42,194 - For batch 900 loss at 500 samples is 1.25672265625\n",
            "2024-01-14 10:08:43,046 - For batch 1000 loss at 500 samples is 1.3161328125\n",
            "2024-01-14 10:08:43,918 - For batch 1100 loss at 500 samples is 1.3050625\n",
            "2024-01-14 10:08:44,781 - For batch 1200 loss at 500 samples is 1.2999453125\n",
            "2024-01-14 10:08:45,764 - For batch 1300 loss at 500 samples is 1.2875625\n",
            "2024-01-14 10:08:47,109 - For batch 1400 loss at 500 samples is 1.262421875\n",
            "2024-01-14 10:08:47,894 - LOSS train 1.262421875 valid 40.125\n",
            "2024-01-14 10:08:47,901 - Starting training at 42\n",
            "2024-01-14 10:08:49,085 - For batch 100 loss at 500 samples is 1.31301953125\n",
            "2024-01-14 10:08:49,965 - For batch 200 loss at 500 samples is 1.32120703125\n",
            "2024-01-14 10:08:50,835 - For batch 300 loss at 500 samples is 1.32578125\n",
            "2024-01-14 10:08:51,749 - For batch 400 loss at 500 samples is 1.322828125\n",
            "2024-01-14 10:08:52,664 - For batch 500 loss at 500 samples is 1.357953125\n",
            "2024-01-14 10:08:53,519 - For batch 600 loss at 500 samples is 1.29338671875\n",
            "2024-01-14 10:08:54,389 - For batch 700 loss at 500 samples is 1.2802265625\n",
            "2024-01-14 10:08:55,273 - For batch 800 loss at 500 samples is 1.325609375\n",
            "2024-01-14 10:08:56,135 - For batch 900 loss at 500 samples is 1.31440625\n",
            "2024-01-14 10:08:57,008 - For batch 1000 loss at 500 samples is 1.3208359375\n",
            "2024-01-14 10:08:57,882 - For batch 1100 loss at 500 samples is 1.3315859375\n",
            "2024-01-14 10:08:58,780 - For batch 1200 loss at 500 samples is 1.31821875\n",
            "2024-01-14 10:09:00,112 - For batch 1300 loss at 500 samples is 1.3156796875\n",
            "2024-01-14 10:09:01,463 - For batch 1400 loss at 500 samples is 1.28354296875\n",
            "2024-01-14 10:09:02,202 - LOSS train 1.28354296875 valid 38.4375\n",
            "2024-01-14 10:09:02,205 - Starting training at 43\n",
            "2024-01-14 10:09:03,066 - For batch 100 loss at 500 samples is 1.3426015625\n",
            "2024-01-14 10:09:03,908 - For batch 200 loss at 500 samples is 1.33246875\n",
            "2024-01-14 10:09:04,747 - For batch 300 loss at 500 samples is 1.31075\n",
            "2024-01-14 10:09:05,597 - For batch 400 loss at 500 samples is 1.32275\n",
            "2024-01-14 10:09:06,423 - For batch 500 loss at 500 samples is 1.323453125\n",
            "2024-01-14 10:09:07,283 - For batch 600 loss at 500 samples is 1.306734375\n",
            "2024-01-14 10:09:08,131 - For batch 700 loss at 500 samples is 1.28913671875\n",
            "2024-01-14 10:09:08,983 - For batch 800 loss at 500 samples is 1.3557734375\n",
            "2024-01-14 10:09:09,822 - For batch 900 loss at 500 samples is 1.3334140625\n",
            "2024-01-14 10:09:10,654 - For batch 1000 loss at 500 samples is 1.3331953125\n",
            "2024-01-14 10:09:11,490 - For batch 1100 loss at 500 samples is 1.3551875\n",
            "2024-01-14 10:09:12,614 - For batch 1200 loss at 500 samples is 1.3095078125\n",
            "2024-01-14 10:09:13,957 - For batch 1300 loss at 500 samples is 1.3115078125\n",
            "2024-01-14 10:09:15,203 - For batch 1400 loss at 500 samples is 1.29852734375\n",
            "2024-01-14 10:09:15,761 - LOSS train 1.29852734375 valid 37.21875\n",
            "2024-01-14 10:09:15,763 - Starting training at 44\n",
            "2024-01-14 10:09:16,607 - For batch 100 loss at 500 samples is 1.328390625\n",
            "2024-01-14 10:09:17,470 - For batch 200 loss at 500 samples is 1.3235703125\n",
            "2024-01-14 10:09:18,359 - For batch 300 loss at 500 samples is 1.322015625\n",
            "2024-01-14 10:09:19,235 - For batch 400 loss at 500 samples is 1.34680859375\n",
            "2024-01-14 10:09:20,111 - For batch 500 loss at 500 samples is 1.344125\n",
            "2024-01-14 10:09:20,977 - For batch 600 loss at 500 samples is 1.305125\n",
            "2024-01-14 10:09:21,865 - For batch 700 loss at 500 samples is 1.32421875\n",
            "2024-01-14 10:09:22,733 - For batch 800 loss at 500 samples is 1.35109375\n",
            "2024-01-14 10:09:23,630 - For batch 900 loss at 500 samples is 1.31546875\n",
            "2024-01-14 10:09:24,514 - For batch 1000 loss at 500 samples is 1.3406796875\n",
            "2024-01-14 10:09:25,549 - For batch 1100 loss at 500 samples is 1.3519296875\n",
            "2024-01-14 10:09:26,884 - For batch 1200 loss at 500 samples is 1.34356640625\n",
            "2024-01-14 10:09:28,260 - For batch 1300 loss at 500 samples is 1.3420546875\n",
            "2024-01-14 10:09:29,130 - For batch 1400 loss at 500 samples is 1.30447265625\n",
            "2024-01-14 10:09:29,658 - LOSS train 1.30447265625 valid 36.46875\n",
            "2024-01-14 10:09:29,660 - Starting training at 45\n",
            "2024-01-14 10:09:30,538 - For batch 100 loss at 500 samples is 1.337\n",
            "2024-01-14 10:09:31,412 - For batch 200 loss at 500 samples is 1.3335859375\n",
            "2024-01-14 10:09:32,337 - For batch 300 loss at 500 samples is 1.3244765625\n",
            "2024-01-14 10:09:33,301 - For batch 400 loss at 500 samples is 1.3355546875\n",
            "2024-01-14 10:09:34,240 - For batch 500 loss at 500 samples is 1.3483828125\n",
            "2024-01-14 10:09:35,111 - For batch 600 loss at 500 samples is 1.3370390625\n",
            "2024-01-14 10:09:36,008 - For batch 700 loss at 500 samples is 1.3508515625\n",
            "2024-01-14 10:09:36,882 - For batch 800 loss at 500 samples is 1.361796875\n",
            "2024-01-14 10:09:37,781 - For batch 900 loss at 500 samples is 1.327421875\n",
            "2024-01-14 10:09:38,875 - For batch 1000 loss at 500 samples is 1.34178125\n",
            "2024-01-14 10:09:40,226 - For batch 1100 loss at 500 samples is 1.3820234375\n",
            "2024-01-14 10:09:41,513 - For batch 1200 loss at 500 samples is 1.35474609375\n",
            "2024-01-14 10:09:42,384 - For batch 1300 loss at 500 samples is 1.349140625\n",
            "2024-01-14 10:09:43,238 - For batch 1400 loss at 500 samples is 1.3261171875\n",
            "2024-01-14 10:09:43,820 - LOSS train 1.3261171875 valid 34.46875\n",
            "2024-01-14 10:09:43,822 - Starting training at 46\n",
            "2024-01-14 10:09:44,732 - For batch 100 loss at 500 samples is 1.356515625\n",
            "2024-01-14 10:09:45,622 - For batch 200 loss at 500 samples is 1.3414140625\n",
            "2024-01-14 10:09:46,509 - For batch 300 loss at 500 samples is 1.3454765625\n",
            "2024-01-14 10:09:47,420 - For batch 400 loss at 500 samples is 1.36781640625\n",
            "2024-01-14 10:09:48,375 - For batch 500 loss at 500 samples is 1.3811484375\n",
            "2024-01-14 10:09:49,300 - For batch 600 loss at 500 samples is 1.3424453125\n",
            "2024-01-14 10:09:50,173 - For batch 700 loss at 500 samples is 1.3484609375\n",
            "2024-01-14 10:09:51,074 - For batch 800 loss at 500 samples is 1.382796875\n",
            "2024-01-14 10:09:52,270 - For batch 900 loss at 500 samples is 1.378953125\n",
            "2024-01-14 10:09:53,625 - For batch 1000 loss at 500 samples is 1.3831640625\n",
            "2024-01-14 10:09:54,824 - For batch 1100 loss at 500 samples is 1.3810625\n",
            "2024-01-14 10:09:55,724 - For batch 1200 loss at 500 samples is 1.36936328125\n",
            "2024-01-14 10:09:56,622 - For batch 1300 loss at 500 samples is 1.3859921875\n",
            "2024-01-14 10:09:57,508 - For batch 1400 loss at 500 samples is 1.3528828125\n",
            "2024-01-14 10:09:58,130 - LOSS train 1.3528828125 valid 33.75\n",
            "2024-01-14 10:09:58,133 - Starting training at 47\n",
            "2024-01-14 10:09:59,040 - For batch 100 loss at 500 samples is 1.375578125\n",
            "2024-01-14 10:09:59,931 - For batch 200 loss at 500 samples is 1.3703984375\n",
            "2024-01-14 10:10:00,840 - For batch 300 loss at 500 samples is 1.38303125\n",
            "2024-01-14 10:10:01,727 - For batch 400 loss at 500 samples is 1.408015625\n",
            "2024-01-14 10:10:02,648 - For batch 500 loss at 500 samples is 1.39415625\n",
            "2024-01-14 10:10:03,531 - For batch 600 loss at 500 samples is 1.390265625\n",
            "2024-01-14 10:10:04,432 - For batch 700 loss at 500 samples is 1.3837890625\n",
            "2024-01-14 10:10:05,730 - For batch 800 loss at 500 samples is 1.3878515625\n",
            "2024-01-14 10:10:07,105 - For batch 900 loss at 500 samples is 1.3861484375\n",
            "2024-01-14 10:10:08,204 - For batch 1000 loss at 500 samples is 1.3960078125\n",
            "2024-01-14 10:10:09,101 - For batch 1100 loss at 500 samples is 1.401859375\n",
            "2024-01-14 10:10:09,989 - For batch 1200 loss at 500 samples is 1.38413671875\n",
            "2024-01-14 10:10:10,909 - For batch 1300 loss at 500 samples is 1.3839296875\n",
            "2024-01-14 10:10:11,822 - For batch 1400 loss at 500 samples is 1.3513125\n",
            "2024-01-14 10:10:12,333 - LOSS train 1.3513125 valid 32.5625\n",
            "2024-01-14 10:10:12,339 - Starting training at 48\n",
            "2024-01-14 10:10:13,232 - For batch 100 loss at 500 samples is 1.3812109375\n",
            "2024-01-14 10:10:14,103 - For batch 200 loss at 500 samples is 1.3926640625\n",
            "2024-01-14 10:10:14,992 - For batch 300 loss at 500 samples is 1.392546875\n",
            "2024-01-14 10:10:15,877 - For batch 400 loss at 500 samples is 1.415171875\n",
            "2024-01-14 10:10:16,777 - For batch 500 loss at 500 samples is 1.42121875\n",
            "2024-01-14 10:10:17,674 - For batch 600 loss at 500 samples is 1.39371875\n",
            "2024-01-14 10:10:19,017 - For batch 700 loss at 500 samples is 1.3856015625\n",
            "2024-01-14 10:10:20,373 - For batch 800 loss at 500 samples is 1.4270625\n",
            "2024-01-14 10:10:21,446 - For batch 900 loss at 500 samples is 1.4191796875\n",
            "2024-01-14 10:10:22,306 - For batch 1000 loss at 500 samples is 1.394\n",
            "2024-01-14 10:10:23,182 - For batch 1100 loss at 500 samples is 1.4231328125\n",
            "2024-01-14 10:10:24,096 - For batch 1200 loss at 500 samples is 1.3992109375\n",
            "2024-01-14 10:10:25,003 - For batch 1300 loss at 500 samples is 1.4116015625\n",
            "2024-01-14 10:10:25,884 - For batch 1400 loss at 500 samples is 1.3743125\n",
            "2024-01-14 10:10:26,464 - LOSS train 1.3743125 valid 31.8125\n",
            "2024-01-14 10:10:26,466 - Starting training at 49\n",
            "2024-01-14 10:10:27,379 - For batch 100 loss at 500 samples is 1.4187265625\n",
            "2024-01-14 10:10:28,275 - For batch 200 loss at 500 samples is 1.408765625\n",
            "2024-01-14 10:10:29,168 - For batch 300 loss at 500 samples is 1.408765625\n",
            "2024-01-14 10:10:30,043 - For batch 400 loss at 500 samples is 1.4223984375\n",
            "2024-01-14 10:10:30,926 - For batch 500 loss at 500 samples is 1.4325078125\n",
            "2024-01-14 10:10:32,308 - For batch 600 loss at 500 samples is 1.3983203125\n",
            "2024-01-14 10:10:33,709 - For batch 700 loss at 500 samples is 1.395328125\n",
            "2024-01-14 10:10:34,787 - For batch 800 loss at 500 samples is 1.430890625\n",
            "2024-01-14 10:10:35,679 - For batch 900 loss at 500 samples is 1.408453125\n",
            "2024-01-14 10:10:36,560 - For batch 1000 loss at 500 samples is 1.390890625\n",
            "2024-01-14 10:10:37,457 - For batch 1100 loss at 500 samples is 1.4308359375\n",
            "2024-01-14 10:10:38,313 - For batch 1200 loss at 500 samples is 1.4108984375\n",
            "2024-01-14 10:10:39,217 - For batch 1300 loss at 500 samples is 1.399578125\n",
            "2024-01-14 10:10:40,124 - For batch 1400 loss at 500 samples is 1.3855703125\n",
            "2024-01-14 10:10:40,711 - LOSS train 1.3855703125 valid 30.34375\n",
            "2024-01-14 10:10:40,713 - Starting training at 50\n",
            "2024-01-14 10:10:41,634 - For batch 100 loss at 500 samples is 1.4210078125\n",
            "2024-01-14 10:10:42,519 - For batch 200 loss at 500 samples is 1.40671875\n",
            "2024-01-14 10:10:43,399 - For batch 300 loss at 500 samples is 1.424484375\n",
            "2024-01-14 10:10:44,271 - For batch 400 loss at 500 samples is 1.4445\n",
            "2024-01-14 10:10:45,651 - For batch 500 loss at 500 samples is 1.448640625\n",
            "2024-01-14 10:10:47,016 - For batch 600 loss at 500 samples is 1.4440078125\n",
            "2024-01-14 10:10:48,028 - For batch 700 loss at 500 samples is 1.39853125\n",
            "2024-01-14 10:10:48,907 - For batch 800 loss at 500 samples is 1.4426953125\n",
            "2024-01-14 10:10:49,777 - For batch 900 loss at 500 samples is 1.421578125\n",
            "2024-01-14 10:10:50,636 - For batch 1000 loss at 500 samples is 1.4378203125\n",
            "2024-01-14 10:10:51,551 - For batch 1100 loss at 500 samples is 1.433203125\n",
            "2024-01-14 10:10:52,458 - For batch 1200 loss at 500 samples is 1.4075859375\n",
            "2024-01-14 10:10:53,359 - For batch 1300 loss at 500 samples is 1.4211015625\n",
            "2024-01-14 10:10:54,247 - For batch 1400 loss at 500 samples is 1.4060546875\n",
            "2024-01-14 10:10:54,858 - LOSS train 1.4060546875 valid 30.09375\n",
            "2024-01-14 10:10:54,862 - Starting training at 51\n",
            "2024-01-14 10:10:55,782 - For batch 100 loss at 500 samples is 1.4467578125\n",
            "2024-01-14 10:10:56,680 - For batch 200 loss at 500 samples is 1.4215\n",
            "2024-01-14 10:10:57,690 - For batch 300 loss at 500 samples is 1.4296171875\n",
            "2024-01-14 10:10:59,061 - For batch 400 loss at 500 samples is 1.4465859375\n",
            "2024-01-14 10:11:00,424 - For batch 500 loss at 500 samples is 1.4450859375\n",
            "2024-01-14 10:11:01,349 - For batch 600 loss at 500 samples is 1.4284140625\n",
            "2024-01-14 10:11:02,251 - For batch 700 loss at 500 samples is 1.441546875\n",
            "2024-01-14 10:11:03,204 - For batch 800 loss at 500 samples is 1.4428671875\n",
            "2024-01-14 10:11:04,071 - For batch 900 loss at 500 samples is 1.4367265625\n",
            "2024-01-14 10:11:04,979 - For batch 1000 loss at 500 samples is 1.41971875\n",
            "2024-01-14 10:11:05,880 - For batch 1100 loss at 500 samples is 1.4320703125\n",
            "2024-01-14 10:11:06,770 - For batch 1200 loss at 500 samples is 1.42415625\n",
            "2024-01-14 10:11:07,657 - For batch 1300 loss at 500 samples is 1.4232890625\n",
            "2024-01-14 10:11:08,540 - For batch 1400 loss at 500 samples is 1.420484375\n",
            "2024-01-14 10:11:09,044 - LOSS train 1.420484375 valid 29.53125\n",
            "2024-01-14 10:11:09,048 - Starting training at 52\n",
            "2024-01-14 10:11:09,944 - For batch 100 loss at 500 samples is 1.4484375\n",
            "2024-01-14 10:11:10,921 - For batch 200 loss at 500 samples is 1.4368125\n",
            "2024-01-14 10:11:12,275 - For batch 300 loss at 500 samples is 1.4242421875\n",
            "2024-01-14 10:11:13,620 - For batch 400 loss at 500 samples is 1.456625\n",
            "2024-01-14 10:11:14,559 - For batch 500 loss at 500 samples is 1.4545234375\n",
            "2024-01-14 10:11:15,415 - For batch 600 loss at 500 samples is 1.4479453125\n",
            "2024-01-14 10:11:16,274 - For batch 700 loss at 500 samples is 1.425203125\n",
            "2024-01-14 10:11:17,134 - For batch 800 loss at 500 samples is 1.4498515625\n",
            "2024-01-14 10:11:18,020 - For batch 900 loss at 500 samples is 1.4390703125\n",
            "2024-01-14 10:11:18,900 - For batch 1000 loss at 500 samples is 1.4400390625\n",
            "2024-01-14 10:11:19,772 - For batch 1100 loss at 500 samples is 1.4374609375\n",
            "2024-01-14 10:11:20,652 - For batch 1200 loss at 500 samples is 1.42963671875\n",
            "2024-01-14 10:11:21,495 - For batch 1300 loss at 500 samples is 1.4320390625\n",
            "2024-01-14 10:11:22,368 - For batch 1400 loss at 500 samples is 1.4178359375\n",
            "2024-01-14 10:11:22,950 - LOSS train 1.4178359375 valid 28.6875\n",
            "2024-01-14 10:11:22,955 - Starting training at 53\n",
            "2024-01-14 10:11:23,900 - For batch 100 loss at 500 samples is 1.4494375\n",
            "2024-01-14 10:11:25,302 - For batch 200 loss at 500 samples is 1.444953125\n",
            "2024-01-14 10:11:26,671 - For batch 300 loss at 500 samples is 1.4486171875\n",
            "2024-01-14 10:11:27,704 - For batch 400 loss at 500 samples is 1.4515703125\n",
            "2024-01-14 10:11:28,593 - For batch 500 loss at 500 samples is 1.4762265625\n",
            "2024-01-14 10:11:29,518 - For batch 600 loss at 500 samples is 1.442609375\n",
            "2024-01-14 10:11:30,390 - For batch 700 loss at 500 samples is 1.439953125\n",
            "2024-01-14 10:11:31,288 - For batch 800 loss at 500 samples is 1.469546875\n",
            "2024-01-14 10:11:32,154 - For batch 900 loss at 500 samples is 1.4628671875\n",
            "2024-01-14 10:11:33,039 - For batch 1000 loss at 500 samples is 1.4558203125\n",
            "2024-01-14 10:11:34,006 - For batch 1100 loss at 500 samples is 1.4447890625\n",
            "2024-01-14 10:11:34,933 - For batch 1200 loss at 500 samples is 1.4474375\n",
            "2024-01-14 10:11:35,821 - For batch 1300 loss at 500 samples is 1.4370859375\n",
            "2024-01-14 10:11:36,708 - For batch 1400 loss at 500 samples is 1.4471171875\n",
            "2024-01-14 10:11:37,497 - LOSS train 1.4471171875 valid 27.96875\n",
            "2024-01-14 10:11:37,500 - Starting training at 54\n",
            "2024-01-14 10:11:38,913 - For batch 100 loss at 500 samples is 1.474515625\n",
            "2024-01-14 10:11:40,282 - For batch 200 loss at 500 samples is 1.4632265625\n",
            "2024-01-14 10:11:41,198 - For batch 300 loss at 500 samples is 1.4738671875\n",
            "2024-01-14 10:11:42,107 - For batch 400 loss at 500 samples is 1.4749921875\n",
            "2024-01-14 10:11:43,023 - For batch 500 loss at 500 samples is 1.4812421875\n",
            "2024-01-14 10:11:43,932 - For batch 600 loss at 500 samples is 1.460296875\n",
            "2024-01-14 10:11:44,795 - For batch 700 loss at 500 samples is 1.44125\n",
            "2024-01-14 10:11:45,666 - For batch 800 loss at 500 samples is 1.476015625\n",
            "2024-01-14 10:11:46,538 - For batch 900 loss at 500 samples is 1.4463359375\n",
            "2024-01-14 10:11:47,445 - For batch 1000 loss at 500 samples is 1.4698125\n",
            "2024-01-14 10:11:48,337 - For batch 1100 loss at 500 samples is 1.472421875\n",
            "2024-01-14 10:11:49,251 - For batch 1200 loss at 500 samples is 1.45409375\n",
            "2024-01-14 10:11:50,142 - For batch 1300 loss at 500 samples is 1.4707578125\n",
            "2024-01-14 10:11:51,417 - For batch 1400 loss at 500 samples is 1.449484375\n",
            "2024-01-14 10:11:52,187 - LOSS train 1.449484375 valid 27.359375\n",
            "2024-01-14 10:11:52,200 - Starting training at 55\n",
            "2024-01-14 10:11:53,525 - For batch 100 loss at 500 samples is 1.463921875\n",
            "2024-01-14 10:11:54,376 - For batch 200 loss at 500 samples is 1.4841328125\n",
            "2024-01-14 10:11:55,240 - For batch 300 loss at 500 samples is 1.4846015625\n",
            "2024-01-14 10:11:56,121 - For batch 400 loss at 500 samples is 1.4785859375\n",
            "2024-01-14 10:11:56,980 - For batch 500 loss at 500 samples is 1.50221875\n",
            "2024-01-14 10:11:57,847 - For batch 600 loss at 500 samples is 1.4800546875\n",
            "2024-01-14 10:11:58,717 - For batch 700 loss at 500 samples is 1.465875\n",
            "2024-01-14 10:11:59,554 - For batch 800 loss at 500 samples is 1.474859375\n",
            "2024-01-14 10:12:00,422 - For batch 900 loss at 500 samples is 1.4686640625\n",
            "2024-01-14 10:12:01,321 - For batch 1000 loss at 500 samples is 1.478390625\n",
            "2024-01-14 10:12:02,193 - For batch 1100 loss at 500 samples is 1.4624375\n",
            "2024-01-14 10:12:03,145 - For batch 1200 loss at 500 samples is 1.4543046875\n",
            "2024-01-14 10:12:04,379 - For batch 1300 loss at 500 samples is 1.4669296875\n",
            "2024-01-14 10:12:05,724 - For batch 1400 loss at 500 samples is 1.4472578125\n",
            "2024-01-14 10:12:06,608 - LOSS train 1.4472578125 valid 25.90625\n",
            "2024-01-14 10:12:06,610 - Starting training at 56\n",
            "2024-01-14 10:12:07,517 - For batch 100 loss at 500 samples is 1.469046875\n",
            "2024-01-14 10:12:08,400 - For batch 200 loss at 500 samples is 1.48396875\n",
            "2024-01-14 10:12:09,272 - For batch 300 loss at 500 samples is 1.480109375\n",
            "2024-01-14 10:12:10,151 - For batch 400 loss at 500 samples is 1.4694140625\n",
            "2024-01-14 10:12:11,006 - For batch 500 loss at 500 samples is 1.4984453125\n",
            "2024-01-14 10:12:11,859 - For batch 600 loss at 500 samples is 1.46425\n",
            "2024-01-14 10:12:12,712 - For batch 700 loss at 500 samples is 1.463625\n",
            "2024-01-14 10:12:13,573 - For batch 800 loss at 500 samples is 1.4789921875\n",
            "2024-01-14 10:12:14,475 - For batch 900 loss at 500 samples is 1.46140625\n",
            "2024-01-14 10:12:15,387 - For batch 1000 loss at 500 samples is 1.4785859375\n",
            "2024-01-14 10:12:16,294 - For batch 1100 loss at 500 samples is 1.466265625\n",
            "2024-01-14 10:12:17,446 - For batch 1200 loss at 500 samples is 1.4528046875\n",
            "2024-01-14 10:12:18,836 - For batch 1300 loss at 500 samples is 1.46525\n",
            "2024-01-14 10:12:20,071 - For batch 1400 loss at 500 samples is 1.459640625\n",
            "2024-01-14 10:12:20,666 - LOSS train 1.459640625 valid 26.171875\n",
            "2024-01-14 10:12:20,669 - Starting training at 57\n",
            "2024-01-14 10:12:21,577 - For batch 100 loss at 500 samples is 1.4880390625\n",
            "2024-01-14 10:12:22,485 - For batch 200 loss at 500 samples is 1.490046875\n",
            "2024-01-14 10:12:23,376 - For batch 300 loss at 500 samples is 1.481640625\n",
            "2024-01-14 10:12:24,249 - For batch 400 loss at 500 samples is 1.49378125\n",
            "2024-01-14 10:12:25,178 - For batch 500 loss at 500 samples is 1.5013125\n",
            "2024-01-14 10:12:26,086 - For batch 600 loss at 500 samples is 1.46928125\n",
            "2024-01-14 10:12:26,986 - For batch 700 loss at 500 samples is 1.4603125\n",
            "2024-01-14 10:12:27,918 - For batch 800 loss at 500 samples is 1.4755234375\n",
            "2024-01-14 10:12:28,848 - For batch 900 loss at 500 samples is 1.4776015625\n",
            "2024-01-14 10:12:29,753 - For batch 1000 loss at 500 samples is 1.4779765625\n",
            "2024-01-14 10:12:31,070 - For batch 1100 loss at 500 samples is 1.468296875\n",
            "2024-01-14 10:12:32,442 - For batch 1200 loss at 500 samples is 1.469203125\n",
            "2024-01-14 10:12:33,515 - For batch 1300 loss at 500 samples is 1.4674609375\n",
            "2024-01-14 10:12:34,433 - For batch 1400 loss at 500 samples is 1.4682265625\n",
            "2024-01-14 10:12:34,956 - LOSS train 1.4682265625 valid 25.375\n",
            "2024-01-14 10:12:34,958 - Starting training at 58\n",
            "2024-01-14 10:12:35,858 - For batch 100 loss at 500 samples is 1.49\n",
            "2024-01-14 10:12:36,737 - For batch 200 loss at 500 samples is 1.4940234375\n",
            "2024-01-14 10:12:37,632 - For batch 300 loss at 500 samples is 1.49328125\n",
            "2024-01-14 10:12:38,554 - For batch 400 loss at 500 samples is 1.4896875\n",
            "2024-01-14 10:12:39,439 - For batch 500 loss at 500 samples is 1.497359375\n",
            "2024-01-14 10:12:40,314 - For batch 600 loss at 500 samples is 1.4698046875\n",
            "2024-01-14 10:12:41,153 - For batch 700 loss at 500 samples is 1.4804296875\n",
            "2024-01-14 10:12:41,981 - For batch 800 loss at 500 samples is 1.479578125\n",
            "2024-01-14 10:12:42,833 - For batch 900 loss at 500 samples is 1.4974140625\n",
            "2024-01-14 10:12:44,094 - For batch 1000 loss at 500 samples is 1.499828125\n",
            "2024-01-14 10:12:45,475 - For batch 1100 loss at 500 samples is 1.4869296875\n",
            "2024-01-14 10:12:46,571 - For batch 1200 loss at 500 samples is 1.47071875\n",
            "2024-01-14 10:12:47,447 - For batch 1300 loss at 500 samples is 1.4879765625\n",
            "2024-01-14 10:12:48,399 - For batch 1400 loss at 500 samples is 1.4799609375\n",
            "2024-01-14 10:12:48,998 - LOSS train 1.4799609375 valid 24.6875\n",
            "2024-01-14 10:12:49,005 - Starting training at 59\n",
            "2024-01-14 10:12:49,898 - For batch 100 loss at 500 samples is 1.4960234375\n",
            "2024-01-14 10:12:50,856 - For batch 200 loss at 500 samples is 1.4929609375\n",
            "2024-01-14 10:12:51,744 - For batch 300 loss at 500 samples is 1.4908203125\n",
            "2024-01-14 10:12:52,622 - For batch 400 loss at 500 samples is 1.5003671875\n",
            "2024-01-14 10:12:53,520 - For batch 500 loss at 500 samples is 1.5204609375\n",
            "2024-01-14 10:12:54,375 - For batch 600 loss at 500 samples is 1.4938671875\n",
            "2024-01-14 10:12:55,271 - For batch 700 loss at 500 samples is 1.491125\n",
            "2024-01-14 10:12:56,181 - For batch 800 loss at 500 samples is 1.5000390625\n",
            "2024-01-14 10:12:57,646 - For batch 900 loss at 500 samples is 1.493046875\n",
            "2024-01-14 10:12:59,033 - For batch 1000 loss at 500 samples is 1.5071328125\n",
            "2024-01-14 10:13:00,014 - For batch 1100 loss at 500 samples is 1.4914921875\n",
            "2024-01-14 10:13:00,901 - For batch 1200 loss at 500 samples is 1.4800703125\n",
            "2024-01-14 10:13:01,828 - For batch 1300 loss at 500 samples is 1.4807265625\n",
            "2024-01-14 10:13:02,760 - For batch 1400 loss at 500 samples is 1.4799921875\n",
            "2024-01-14 10:13:03,348 - LOSS train 1.4799921875 valid 23.859375\n",
            "2024-01-14 10:13:03,350 - Starting training at 60\n",
            "2024-01-14 10:13:04,239 - For batch 100 loss at 500 samples is 1.4983515625\n",
            "2024-01-14 10:13:05,174 - For batch 200 loss at 500 samples is 1.4987890625\n",
            "2024-01-14 10:13:06,063 - For batch 300 loss at 500 samples is 1.4886171875\n",
            "2024-01-14 10:13:06,973 - For batch 400 loss at 500 samples is 1.505421875\n",
            "2024-01-14 10:13:07,890 - For batch 500 loss at 500 samples is 1.5112890625\n",
            "2024-01-14 10:13:08,803 - For batch 600 loss at 500 samples is 1.4873125\n",
            "2024-01-14 10:13:09,880 - For batch 700 loss at 500 samples is 1.485984375\n",
            "2024-01-14 10:13:11,235 - For batch 800 loss at 500 samples is 1.4904453125\n",
            "2024-01-14 10:13:12,581 - For batch 900 loss at 500 samples is 1.4978046875\n",
            "2024-01-14 10:13:13,490 - For batch 1000 loss at 500 samples is 1.5009609375\n",
            "2024-01-14 10:13:14,396 - For batch 1100 loss at 500 samples is 1.5027109375\n",
            "2024-01-14 10:13:15,317 - For batch 1200 loss at 500 samples is 1.4731953125\n",
            "2024-01-14 10:13:16,244 - For batch 1300 loss at 500 samples is 1.4956953125\n",
            "2024-01-14 10:13:17,134 - For batch 1400 loss at 500 samples is 1.4689140625\n",
            "2024-01-14 10:13:17,716 - LOSS train 1.4689140625 valid 23.765625\n",
            "2024-01-14 10:13:17,719 - Starting training at 61\n",
            "2024-01-14 10:13:18,632 - For batch 100 loss at 500 samples is 1.5060859375\n",
            "2024-01-14 10:13:19,520 - For batch 200 loss at 500 samples is 1.5104921875\n",
            "2024-01-14 10:13:20,393 - For batch 300 loss at 500 samples is 1.5008125\n",
            "2024-01-14 10:13:21,307 - For batch 400 loss at 500 samples is 1.51596875\n",
            "2024-01-14 10:13:22,192 - For batch 500 loss at 500 samples is 1.508203125\n",
            "2024-01-14 10:13:23,375 - For batch 600 loss at 500 samples is 1.4992578125\n",
            "2024-01-14 10:13:24,817 - For batch 700 loss at 500 samples is 1.485953125\n",
            "2024-01-14 10:13:26,052 - For batch 800 loss at 500 samples is 1.497734375\n",
            "2024-01-14 10:13:26,917 - For batch 900 loss at 500 samples is 1.4938203125\n",
            "2024-01-14 10:13:27,815 - For batch 1000 loss at 500 samples is 1.508046875\n",
            "2024-01-14 10:13:28,693 - For batch 1100 loss at 500 samples is 1.504953125\n",
            "2024-01-14 10:13:29,572 - For batch 1200 loss at 500 samples is 1.482484375\n",
            "2024-01-14 10:13:30,461 - For batch 1300 loss at 500 samples is 1.5059375\n",
            "2024-01-14 10:13:31,336 - For batch 1400 loss at 500 samples is 1.491421875\n",
            "2024-01-14 10:13:31,852 - LOSS train 1.491421875 valid 23.1875\n",
            "2024-01-14 10:13:31,855 - Starting training at 62\n",
            "2024-01-14 10:13:32,778 - For batch 100 loss at 500 samples is 1.5049375\n",
            "2024-01-14 10:13:33,661 - For batch 200 loss at 500 samples is 1.508703125\n",
            "2024-01-14 10:13:34,492 - For batch 300 loss at 500 samples is 1.5067578125\n",
            "2024-01-14 10:13:35,378 - For batch 400 loss at 500 samples is 1.5196015625\n",
            "2024-01-14 10:13:36,571 - For batch 500 loss at 500 samples is 1.5356796875\n",
            "2024-01-14 10:13:37,937 - For batch 600 loss at 500 samples is 1.5119453125\n",
            "2024-01-14 10:13:39,178 - For batch 700 loss at 500 samples is 1.4909765625\n",
            "2024-01-14 10:13:40,031 - For batch 800 loss at 500 samples is 1.4954453125\n",
            "2024-01-14 10:13:40,893 - For batch 900 loss at 500 samples is 1.504765625\n",
            "2024-01-14 10:13:41,772 - For batch 1000 loss at 500 samples is 1.507171875\n",
            "2024-01-14 10:13:42,634 - For batch 1100 loss at 500 samples is 1.5006875\n",
            "2024-01-14 10:13:43,498 - For batch 1200 loss at 500 samples is 1.48846875\n",
            "2024-01-14 10:13:44,370 - For batch 1300 loss at 500 samples is 1.51528125\n",
            "2024-01-14 10:13:45,263 - For batch 1400 loss at 500 samples is 1.49853125\n",
            "2024-01-14 10:13:45,829 - LOSS train 1.49853125 valid 22.8125\n",
            "2024-01-14 10:13:45,831 - Starting training at 63\n",
            "2024-01-14 10:13:46,777 - For batch 100 loss at 500 samples is 1.5093046875\n",
            "2024-01-14 10:13:47,669 - For batch 200 loss at 500 samples is 1.5219609375\n",
            "2024-01-14 10:13:48,587 - For batch 300 loss at 500 samples is 1.5228125\n",
            "2024-01-14 10:13:49,756 - For batch 400 loss at 500 samples is 1.5186015625\n",
            "2024-01-14 10:13:51,150 - For batch 500 loss at 500 samples is 1.52878125\n",
            "2024-01-14 10:13:52,384 - For batch 600 loss at 500 samples is 1.510046875\n",
            "2024-01-14 10:13:53,247 - For batch 700 loss at 500 samples is 1.5035625\n",
            "2024-01-14 10:13:54,115 - For batch 800 loss at 500 samples is 1.5016796875\n",
            "2024-01-14 10:13:54,975 - For batch 900 loss at 500 samples is 1.5142109375\n",
            "2024-01-14 10:13:55,862 - For batch 1000 loss at 500 samples is 1.50946875\n",
            "2024-01-14 10:13:56,765 - For batch 1100 loss at 500 samples is 1.5103828125\n",
            "2024-01-14 10:13:57,674 - For batch 1200 loss at 500 samples is 1.5051796875\n",
            "2024-01-14 10:13:58,550 - For batch 1300 loss at 500 samples is 1.5145703125\n",
            "2024-01-14 10:13:59,417 - For batch 1400 loss at 500 samples is 1.4951328125\n",
            "2024-01-14 10:13:59,989 - LOSS train 1.4951328125 valid 22.28125\n",
            "2024-01-14 10:13:59,993 - Starting training at 64\n",
            "2024-01-14 10:14:00,873 - For batch 100 loss at 500 samples is 1.5293984375\n",
            "2024-01-14 10:14:01,741 - For batch 200 loss at 500 samples is 1.5144453125\n",
            "2024-01-14 10:14:02,930 - For batch 300 loss at 500 samples is 1.5174375\n",
            "2024-01-14 10:14:04,272 - For batch 400 loss at 500 samples is 1.5073125\n",
            "2024-01-14 10:14:05,528 - For batch 500 loss at 500 samples is 1.52696875\n",
            "2024-01-14 10:14:06,399 - For batch 600 loss at 500 samples is 1.5110625\n",
            "2024-01-14 10:14:07,279 - For batch 700 loss at 500 samples is 1.50103125\n",
            "2024-01-14 10:14:08,192 - For batch 800 loss at 500 samples is 1.5014453125\n",
            "2024-01-14 10:14:09,092 - For batch 900 loss at 500 samples is 1.50925\n",
            "2024-01-14 10:14:09,984 - For batch 1000 loss at 500 samples is 1.51703125\n",
            "2024-01-14 10:14:10,846 - For batch 1100 loss at 500 samples is 1.50896875\n",
            "2024-01-14 10:14:11,738 - For batch 1200 loss at 500 samples is 1.5002265625\n",
            "2024-01-14 10:14:12,640 - For batch 1300 loss at 500 samples is 1.515046875\n",
            "2024-01-14 10:14:13,545 - For batch 1400 loss at 500 samples is 1.4947578125\n",
            "2024-01-14 10:14:14,067 - LOSS train 1.4947578125 valid 21.875\n",
            "2024-01-14 10:14:14,075 - Starting training at 65\n",
            "2024-01-14 10:14:14,944 - For batch 100 loss at 500 samples is 1.5248125\n",
            "2024-01-14 10:14:16,087 - For batch 200 loss at 500 samples is 1.5058828125\n",
            "2024-01-14 10:14:17,450 - For batch 300 loss at 500 samples is 1.5165546875\n",
            "2024-01-14 10:14:18,713 - For batch 400 loss at 500 samples is 1.517875\n",
            "2024-01-14 10:14:19,655 - For batch 500 loss at 500 samples is 1.5282109375\n",
            "2024-01-14 10:14:20,568 - For batch 600 loss at 500 samples is 1.5141015625\n",
            "2024-01-14 10:14:21,513 - For batch 700 loss at 500 samples is 1.5086171875\n",
            "2024-01-14 10:14:22,497 - For batch 800 loss at 500 samples is 1.4998515625\n",
            "2024-01-14 10:14:23,427 - For batch 900 loss at 500 samples is 1.5131171875\n",
            "2024-01-14 10:14:24,372 - For batch 1000 loss at 500 samples is 1.5168359375\n",
            "2024-01-14 10:14:25,300 - For batch 1100 loss at 500 samples is 1.5095390625\n",
            "2024-01-14 10:14:26,272 - For batch 1200 loss at 500 samples is 1.4952421875\n",
            "2024-01-14 10:14:27,226 - For batch 1300 loss at 500 samples is 1.519125\n",
            "2024-01-14 10:14:28,190 - For batch 1400 loss at 500 samples is 1.50309375\n",
            "2024-01-14 10:14:29,016 - LOSS train 1.50309375 valid 21.40625\n",
            "2024-01-14 10:14:29,019 - Starting training at 66\n",
            "2024-01-14 10:14:30,413 - For batch 100 loss at 500 samples is 1.52409375\n",
            "2024-01-14 10:14:31,744 - For batch 200 loss at 500 samples is 1.51\n",
            "2024-01-14 10:14:32,649 - For batch 300 loss at 500 samples is 1.5210625\n",
            "2024-01-14 10:14:33,548 - For batch 400 loss at 500 samples is 1.5165859375\n",
            "2024-01-14 10:14:34,405 - For batch 500 loss at 500 samples is 1.523703125\n",
            "2024-01-14 10:14:35,335 - For batch 600 loss at 500 samples is 1.521921875\n",
            "2024-01-14 10:14:36,320 - For batch 700 loss at 500 samples is 1.5118515625\n",
            "2024-01-14 10:14:37,253 - For batch 800 loss at 500 samples is 1.5162265625\n",
            "2024-01-14 10:14:38,170 - For batch 900 loss at 500 samples is 1.51503125\n",
            "2024-01-14 10:14:39,099 - For batch 1000 loss at 500 samples is 1.5217890625\n",
            "2024-01-14 10:14:40,058 - For batch 1100 loss at 500 samples is 1.513125\n",
            "2024-01-14 10:14:40,987 - For batch 1200 loss at 500 samples is 1.503921875\n",
            "2024-01-14 10:14:42,025 - For batch 1300 loss at 500 samples is 1.5198203125\n",
            "2024-01-14 10:14:43,388 - For batch 1400 loss at 500 samples is 1.5009921875\n",
            "2024-01-14 10:14:44,280 - LOSS train 1.5009921875 valid 20.984375\n",
            "2024-01-14 10:14:44,285 - Starting training at 67\n",
            "2024-01-14 10:14:45,353 - For batch 100 loss at 500 samples is 1.5208125\n",
            "2024-01-14 10:14:46,282 - For batch 200 loss at 500 samples is 1.5204609375\n",
            "2024-01-14 10:14:47,196 - For batch 300 loss at 500 samples is 1.528390625\n",
            "2024-01-14 10:14:48,172 - For batch 400 loss at 500 samples is 1.5239765625\n",
            "2024-01-14 10:14:49,064 - For batch 500 loss at 500 samples is 1.52675\n",
            "2024-01-14 10:14:49,986 - For batch 600 loss at 500 samples is 1.513859375\n",
            "2024-01-14 10:14:50,848 - For batch 700 loss at 500 samples is 1.5195234375\n",
            "2024-01-14 10:14:51,755 - For batch 800 loss at 500 samples is 1.5159296875\n",
            "2024-01-14 10:14:52,648 - For batch 900 loss at 500 samples is 1.513640625\n",
            "2024-01-14 10:14:53,546 - For batch 1000 loss at 500 samples is 1.5166328125\n",
            "2024-01-14 10:14:54,424 - For batch 1100 loss at 500 samples is 1.5146640625\n",
            "2024-01-14 10:14:55,558 - For batch 1200 loss at 500 samples is 1.5004453125\n",
            "2024-01-14 10:14:56,998 - For batch 1300 loss at 500 samples is 1.5203515625\n",
            "2024-01-14 10:14:58,260 - For batch 1400 loss at 500 samples is 1.5107734375\n",
            "2024-01-14 10:14:58,788 - LOSS train 1.5107734375 valid 20.84375\n",
            "2024-01-14 10:14:58,791 - Starting training at 68\n",
            "2024-01-14 10:14:59,720 - For batch 100 loss at 500 samples is 1.522703125\n",
            "2024-01-14 10:15:00,623 - For batch 200 loss at 500 samples is 1.520765625\n",
            "2024-01-14 10:15:01,500 - For batch 300 loss at 500 samples is 1.530375\n",
            "2024-01-14 10:15:02,358 - For batch 400 loss at 500 samples is 1.533359375\n",
            "2024-01-14 10:15:03,249 - For batch 500 loss at 500 samples is 1.538890625\n",
            "2024-01-14 10:15:04,124 - For batch 600 loss at 500 samples is 1.5095546875\n",
            "2024-01-14 10:15:05,008 - For batch 700 loss at 500 samples is 1.517734375\n",
            "2024-01-14 10:15:05,894 - For batch 800 loss at 500 samples is 1.513546875\n",
            "2024-01-14 10:15:06,814 - For batch 900 loss at 500 samples is 1.501\n",
            "2024-01-14 10:15:07,688 - For batch 1000 loss at 500 samples is 1.5276015625\n",
            "2024-01-14 10:15:08,868 - For batch 1100 loss at 500 samples is 1.51815625\n",
            "2024-01-14 10:15:10,201 - For batch 1200 loss at 500 samples is 1.5036328125\n",
            "2024-01-14 10:15:11,436 - For batch 1300 loss at 500 samples is 1.5160546875\n",
            "2024-01-14 10:15:12,284 - For batch 1400 loss at 500 samples is 1.504203125\n",
            "2024-01-14 10:15:12,853 - LOSS train 1.504203125 valid 20.453125\n",
            "2024-01-14 10:15:12,855 - Starting training at 69\n",
            "2024-01-14 10:15:13,731 - For batch 100 loss at 500 samples is 1.5254375\n",
            "2024-01-14 10:15:14,588 - For batch 200 loss at 500 samples is 1.5254765625\n",
            "2024-01-14 10:15:15,428 - For batch 300 loss at 500 samples is 1.5388359375\n",
            "2024-01-14 10:15:16,286 - For batch 400 loss at 500 samples is 1.5288359375\n",
            "2024-01-14 10:15:17,163 - For batch 500 loss at 500 samples is 1.54290625\n",
            "2024-01-14 10:15:18,040 - For batch 600 loss at 500 samples is 1.5161328125\n",
            "2024-01-14 10:15:18,911 - For batch 700 loss at 500 samples is 1.5151015625\n",
            "2024-01-14 10:15:19,755 - For batch 800 loss at 500 samples is 1.5120078125\n",
            "2024-01-14 10:15:20,612 - For batch 900 loss at 500 samples is 1.508703125\n",
            "2024-01-14 10:15:21,642 - For batch 1000 loss at 500 samples is 1.5286328125\n",
            "2024-01-14 10:15:23,040 - For batch 1100 loss at 500 samples is 1.5243671875\n",
            "2024-01-14 10:15:24,430 - For batch 1200 loss at 500 samples is 1.5048359375\n",
            "2024-01-14 10:15:25,309 - For batch 1300 loss at 500 samples is 1.5137578125\n",
            "2024-01-14 10:15:26,167 - For batch 1400 loss at 500 samples is 1.509359375\n",
            "2024-01-14 10:15:26,762 - LOSS train 1.509359375 valid 19.78125\n",
            "2024-01-14 10:15:26,765 - Starting training at 70\n",
            "2024-01-14 10:15:27,671 - For batch 100 loss at 500 samples is 1.5296640625\n",
            "2024-01-14 10:15:28,537 - For batch 200 loss at 500 samples is 1.5306015625\n",
            "2024-01-14 10:15:29,433 - For batch 300 loss at 500 samples is 1.5361484375\n",
            "2024-01-14 10:15:30,312 - For batch 400 loss at 500 samples is 1.530984375\n",
            "2024-01-14 10:15:31,174 - For batch 500 loss at 500 samples is 1.5416171875\n",
            "2024-01-14 10:15:32,039 - For batch 600 loss at 500 samples is 1.516\n",
            "2024-01-14 10:15:32,929 - For batch 700 loss at 500 samples is 1.5140078125\n",
            "2024-01-14 10:15:33,852 - For batch 800 loss at 500 samples is 1.529546875\n",
            "2024-01-14 10:15:34,929 - For batch 900 loss at 500 samples is 1.515546875\n",
            "2024-01-14 10:15:36,287 - For batch 1000 loss at 500 samples is 1.5321953125\n",
            "2024-01-14 10:15:37,695 - For batch 1100 loss at 500 samples is 1.5280234375\n",
            "2024-01-14 10:15:38,611 - For batch 1200 loss at 500 samples is 1.50972265625\n",
            "2024-01-14 10:15:39,509 - For batch 1300 loss at 500 samples is 1.515953125\n",
            "2024-01-14 10:15:40,417 - For batch 1400 loss at 500 samples is 1.5095625\n",
            "2024-01-14 10:15:41,021 - LOSS train 1.5095625 valid 19.796875\n",
            "2024-01-14 10:15:41,023 - Starting training at 71\n",
            "2024-01-14 10:15:41,879 - For batch 100 loss at 500 samples is 1.527578125\n",
            "2024-01-14 10:15:42,771 - For batch 200 loss at 500 samples is 1.533625\n",
            "2024-01-14 10:15:43,649 - For batch 300 loss at 500 samples is 1.533890625\n",
            "2024-01-14 10:15:44,501 - For batch 400 loss at 500 samples is 1.5291875\n",
            "2024-01-14 10:15:45,427 - For batch 500 loss at 500 samples is 1.539984375\n",
            "2024-01-14 10:15:46,296 - For batch 600 loss at 500 samples is 1.5273984375\n",
            "2024-01-14 10:15:47,189 - For batch 700 loss at 500 samples is 1.5192578125\n",
            "2024-01-14 10:15:48,350 - For batch 800 loss at 500 samples is 1.5260078125\n",
            "2024-01-14 10:15:49,777 - For batch 900 loss at 500 samples is 1.5170234375\n",
            "2024-01-14 10:15:51,092 - For batch 1000 loss at 500 samples is 1.5330703125\n",
            "2024-01-14 10:15:51,959 - For batch 1100 loss at 500 samples is 1.52225\n",
            "2024-01-14 10:15:52,861 - For batch 1200 loss at 500 samples is 1.51069921875\n",
            "2024-01-14 10:15:53,756 - For batch 1300 loss at 500 samples is 1.521296875\n",
            "2024-01-14 10:15:54,668 - For batch 1400 loss at 500 samples is 1.5082265625\n",
            "2024-01-14 10:15:55,215 - LOSS train 1.5082265625 valid 19.328125\n",
            "2024-01-14 10:15:55,217 - Starting training at 72\n",
            "2024-01-14 10:15:56,109 - For batch 100 loss at 500 samples is 1.5274453125\n",
            "2024-01-14 10:15:57,013 - For batch 200 loss at 500 samples is 1.5286875\n",
            "2024-01-14 10:15:57,937 - For batch 300 loss at 500 samples is 1.53865625\n",
            "2024-01-14 10:15:58,877 - For batch 400 loss at 500 samples is 1.536296875\n",
            "2024-01-14 10:15:59,763 - For batch 500 loss at 500 samples is 1.5429296875\n",
            "2024-01-14 10:16:00,635 - For batch 600 loss at 500 samples is 1.5315546875\n",
            "2024-01-14 10:16:01,822 - For batch 700 loss at 500 samples is 1.5194609375\n",
            "2024-01-14 10:16:03,204 - For batch 800 loss at 500 samples is 1.525828125\n",
            "2024-01-14 10:16:04,398 - For batch 900 loss at 500 samples is 1.5191796875\n",
            "2024-01-14 10:16:05,255 - For batch 1000 loss at 500 samples is 1.5351640625\n",
            "2024-01-14 10:16:06,093 - For batch 1100 loss at 500 samples is 1.5227734375\n",
            "2024-01-14 10:16:06,964 - For batch 1200 loss at 500 samples is 1.5111171875\n",
            "2024-01-14 10:16:07,889 - For batch 1300 loss at 500 samples is 1.522765625\n",
            "2024-01-14 10:16:08,839 - For batch 1400 loss at 500 samples is 1.5237890625\n",
            "2024-01-14 10:16:09,437 - LOSS train 1.5237890625 valid 19.0625\n",
            "2024-01-14 10:16:09,439 - Starting training at 73\n",
            "2024-01-14 10:16:10,332 - For batch 100 loss at 500 samples is 1.5287265625\n",
            "2024-01-14 10:16:11,211 - For batch 200 loss at 500 samples is 1.5362265625\n",
            "2024-01-14 10:16:12,060 - For batch 300 loss at 500 samples is 1.53928125\n",
            "2024-01-14 10:16:12,974 - For batch 400 loss at 500 samples is 1.5376875\n",
            "2024-01-14 10:16:13,872 - For batch 500 loss at 500 samples is 1.54453125\n",
            "2024-01-14 10:16:15,085 - For batch 600 loss at 500 samples is 1.531265625\n",
            "2024-01-14 10:16:16,454 - For batch 700 loss at 500 samples is 1.5248828125\n",
            "2024-01-14 10:16:17,645 - For batch 800 loss at 500 samples is 1.5242578125\n",
            "2024-01-14 10:16:18,537 - For batch 900 loss at 500 samples is 1.534953125\n",
            "2024-01-14 10:16:19,446 - For batch 1000 loss at 500 samples is 1.538859375\n",
            "2024-01-14 10:16:20,357 - For batch 1100 loss at 500 samples is 1.5198671875\n",
            "2024-01-14 10:16:21,266 - For batch 1200 loss at 500 samples is 1.5123046875\n",
            "2024-01-14 10:16:22,132 - For batch 1300 loss at 500 samples is 1.5271796875\n",
            "2024-01-14 10:16:23,031 - For batch 1400 loss at 500 samples is 1.5164453125\n",
            "2024-01-14 10:16:23,636 - LOSS train 1.5164453125 valid 18.703125\n",
            "2024-01-14 10:16:23,638 - Starting training at 74\n",
            "2024-01-14 10:16:24,521 - For batch 100 loss at 500 samples is 1.53578125\n",
            "2024-01-14 10:16:25,443 - For batch 200 loss at 500 samples is 1.5393046875\n",
            "2024-01-14 10:16:26,316 - For batch 300 loss at 500 samples is 1.5452890625\n",
            "2024-01-14 10:16:27,186 - For batch 400 loss at 500 samples is 1.5371796875\n",
            "2024-01-14 10:16:28,546 - For batch 500 loss at 500 samples is 1.5495625\n",
            "2024-01-14 10:16:29,939 - For batch 600 loss at 500 samples is 1.536515625\n",
            "2024-01-14 10:16:31,036 - For batch 700 loss at 500 samples is 1.52625\n",
            "2024-01-14 10:16:31,896 - For batch 800 loss at 500 samples is 1.5260859375\n",
            "2024-01-14 10:16:32,768 - For batch 900 loss at 500 samples is 1.5356640625\n",
            "2024-01-14 10:16:33,683 - For batch 1000 loss at 500 samples is 1.5413671875\n",
            "2024-01-14 10:16:34,578 - For batch 1100 loss at 500 samples is 1.529046875\n",
            "2024-01-14 10:16:35,433 - For batch 1200 loss at 500 samples is 1.5149765625\n",
            "2024-01-14 10:16:36,330 - For batch 1300 loss at 500 samples is 1.5343984375\n",
            "2024-01-14 10:16:37,231 - For batch 1400 loss at 500 samples is 1.5229296875\n",
            "2024-01-14 10:16:37,787 - LOSS train 1.5229296875 valid 18.25\n",
            "2024-01-14 10:16:37,789 - Starting training at 75\n",
            "2024-01-14 10:16:38,672 - For batch 100 loss at 500 samples is 1.544890625\n",
            "2024-01-14 10:16:39,567 - For batch 200 loss at 500 samples is 1.5402734375\n",
            "2024-01-14 10:16:40,490 - For batch 300 loss at 500 samples is 1.543453125\n",
            "2024-01-14 10:16:41,824 - For batch 400 loss at 500 samples is 1.5338671875\n",
            "2024-01-14 10:16:43,199 - For batch 500 loss at 500 samples is 1.550421875\n",
            "2024-01-14 10:16:44,257 - For batch 600 loss at 500 samples is 1.5379921875\n",
            "2024-01-14 10:16:45,159 - For batch 700 loss at 500 samples is 1.528421875\n",
            "2024-01-14 10:16:46,044 - For batch 800 loss at 500 samples is 1.5261171875\n",
            "2024-01-14 10:16:46,915 - For batch 900 loss at 500 samples is 1.5260703125\n",
            "2024-01-14 10:16:47,799 - For batch 1000 loss at 500 samples is 1.5401484375\n",
            "2024-01-14 10:16:48,706 - For batch 1100 loss at 500 samples is 1.5244453125\n",
            "2024-01-14 10:16:49,601 - For batch 1200 loss at 500 samples is 1.5133828125\n",
            "2024-01-14 10:16:50,480 - For batch 1300 loss at 500 samples is 1.5342265625\n",
            "2024-01-14 10:16:51,412 - For batch 1400 loss at 500 samples is 1.5246171875\n",
            "2024-01-14 10:16:51,982 - LOSS train 1.5246171875 valid 18.3125\n",
            "2024-01-14 10:16:51,984 - Starting training at 76\n",
            "2024-01-14 10:16:52,958 - For batch 100 loss at 500 samples is 1.5418046875\n",
            "2024-01-14 10:16:53,972 - For batch 200 loss at 500 samples is 1.5499921875\n",
            "2024-01-14 10:16:55,400 - For batch 300 loss at 500 samples is 1.544\n",
            "2024-01-14 10:16:56,800 - For batch 400 loss at 500 samples is 1.534125\n",
            "2024-01-14 10:16:57,766 - For batch 500 loss at 500 samples is 1.5441796875\n",
            "2024-01-14 10:16:58,685 - For batch 600 loss at 500 samples is 1.5347265625\n",
            "2024-01-14 10:16:59,678 - For batch 700 loss at 500 samples is 1.527921875\n",
            "2024-01-14 10:17:00,632 - For batch 800 loss at 500 samples is 1.522\n",
            "2024-01-14 10:17:01,553 - For batch 900 loss at 500 samples is 1.5266484375\n",
            "2024-01-14 10:17:02,493 - For batch 1000 loss at 500 samples is 1.5478359375\n",
            "2024-01-14 10:17:03,426 - For batch 1100 loss at 500 samples is 1.5256953125\n",
            "2024-01-14 10:17:04,335 - For batch 1200 loss at 500 samples is 1.513390625\n",
            "2024-01-14 10:17:05,272 - For batch 1300 loss at 500 samples is 1.536625\n",
            "2024-01-14 10:17:06,212 - For batch 1400 loss at 500 samples is 1.5271640625\n",
            "2024-01-14 10:17:06,859 - LOSS train 1.5271640625 valid 17.640625\n",
            "2024-01-14 10:17:06,865 - Starting training at 77\n",
            "2024-01-14 10:17:08,252 - For batch 100 loss at 500 samples is 1.5371796875\n",
            "2024-01-14 10:17:09,616 - For batch 200 loss at 500 samples is 1.5488984375\n",
            "2024-01-14 10:17:10,712 - For batch 300 loss at 500 samples is 1.5521015625\n",
            "2024-01-14 10:17:11,647 - For batch 400 loss at 500 samples is 1.5380078125\n",
            "2024-01-14 10:17:12,492 - For batch 500 loss at 500 samples is 1.5512578125\n",
            "2024-01-14 10:17:13,396 - For batch 600 loss at 500 samples is 1.5384609375\n",
            "2024-01-14 10:17:14,265 - For batch 700 loss at 500 samples is 1.520375\n",
            "2024-01-14 10:17:15,154 - For batch 800 loss at 500 samples is 1.535453125\n",
            "2024-01-14 10:17:16,006 - For batch 900 loss at 500 samples is 1.5307578125\n",
            "2024-01-14 10:17:16,857 - For batch 1000 loss at 500 samples is 1.54575\n",
            "2024-01-14 10:17:17,737 - For batch 1100 loss at 500 samples is 1.5358125\n",
            "2024-01-14 10:17:18,644 - For batch 1200 loss at 500 samples is 1.5242109375\n",
            "2024-01-14 10:17:19,536 - For batch 1300 loss at 500 samples is 1.52665625\n",
            "2024-01-14 10:17:20,584 - For batch 1400 loss at 500 samples is 1.5268125\n",
            "2024-01-14 10:17:21,379 - LOSS train 1.5268125 valid 17.515625\n",
            "2024-01-14 10:17:21,386 - Starting training at 78\n",
            "2024-01-14 10:17:22,769 - For batch 100 loss at 500 samples is 1.5392734375\n",
            "2024-01-14 10:17:23,873 - For batch 200 loss at 500 samples is 1.55015625\n",
            "2024-01-14 10:17:24,743 - For batch 300 loss at 500 samples is 1.5469921875\n",
            "2024-01-14 10:17:25,632 - For batch 400 loss at 500 samples is 1.5369765625\n",
            "2024-01-14 10:17:26,518 - For batch 500 loss at 500 samples is 1.5478046875\n",
            "2024-01-14 10:17:27,425 - For batch 600 loss at 500 samples is 1.53734375\n",
            "2024-01-14 10:17:28,277 - For batch 700 loss at 500 samples is 1.5280859375\n",
            "2024-01-14 10:17:29,178 - For batch 800 loss at 500 samples is 1.5361953125\n",
            "2024-01-14 10:17:30,018 - For batch 900 loss at 500 samples is 1.5307734375\n",
            "2024-01-14 10:17:30,899 - For batch 1000 loss at 500 samples is 1.541\n",
            "2024-01-14 10:17:31,757 - For batch 1100 loss at 500 samples is 1.5327734375\n",
            "2024-01-14 10:17:32,638 - For batch 1200 loss at 500 samples is 1.5202421875\n",
            "2024-01-14 10:17:33,591 - For batch 1300 loss at 500 samples is 1.52734375\n",
            "2024-01-14 10:17:34,901 - For batch 1400 loss at 500 samples is 1.5358125\n",
            "2024-01-14 10:17:35,832 - LOSS train 1.5358125 valid 17.328125\n",
            "2024-01-14 10:17:35,835 - Starting training at 79\n",
            "2024-01-14 10:17:36,950 - For batch 100 loss at 500 samples is 1.5393515625\n",
            "2024-01-14 10:17:37,809 - For batch 200 loss at 500 samples is 1.548765625\n",
            "2024-01-14 10:17:38,768 - For batch 300 loss at 500 samples is 1.5555234375\n",
            "2024-01-14 10:17:39,689 - For batch 400 loss at 500 samples is 1.542015625\n",
            "2024-01-14 10:17:40,579 - For batch 500 loss at 500 samples is 1.5487890625\n",
            "2024-01-14 10:17:41,464 - For batch 600 loss at 500 samples is 1.5428046875\n",
            "2024-01-14 10:17:42,383 - For batch 700 loss at 500 samples is 1.523109375\n",
            "2024-01-14 10:17:43,288 - For batch 800 loss at 500 samples is 1.5368203125\n",
            "2024-01-14 10:17:44,200 - For batch 900 loss at 500 samples is 1.535375\n",
            "2024-01-14 10:17:45,067 - For batch 1000 loss at 500 samples is 1.5445859375\n",
            "2024-01-14 10:17:45,934 - For batch 1100 loss at 500 samples is 1.5385625\n",
            "2024-01-14 10:17:46,948 - For batch 1200 loss at 500 samples is 1.5227890625\n",
            "2024-01-14 10:17:48,344 - For batch 1300 loss at 500 samples is 1.53384375\n",
            "2024-01-14 10:17:49,713 - For batch 1400 loss at 500 samples is 1.5247890625\n",
            "2024-01-14 10:17:50,303 - LOSS train 1.5247890625 valid 17.09375\n",
            "2024-01-14 10:17:50,306 - Starting training at 80\n",
            "2024-01-14 10:17:51,181 - For batch 100 loss at 500 samples is 1.538796875\n",
            "2024-01-14 10:17:52,117 - For batch 200 loss at 500 samples is 1.5520859375\n",
            "2024-01-14 10:17:53,000 - For batch 300 loss at 500 samples is 1.5474609375\n",
            "2024-01-14 10:17:53,855 - For batch 400 loss at 500 samples is 1.5439921875\n",
            "2024-01-14 10:17:54,701 - For batch 500 loss at 500 samples is 1.550578125\n",
            "2024-01-14 10:17:55,551 - For batch 600 loss at 500 samples is 1.54390625\n",
            "2024-01-14 10:17:56,414 - For batch 700 loss at 500 samples is 1.5269765625\n",
            "2024-01-14 10:17:57,297 - For batch 800 loss at 500 samples is 1.5413203125\n",
            "2024-01-14 10:17:58,154 - For batch 900 loss at 500 samples is 1.54196875\n",
            "2024-01-14 10:17:59,005 - For batch 1000 loss at 500 samples is 1.545515625\n",
            "2024-01-14 10:17:59,999 - For batch 1100 loss at 500 samples is 1.5367734375\n",
            "2024-01-14 10:18:01,391 - For batch 1200 loss at 500 samples is 1.523453125\n",
            "2024-01-14 10:18:02,781 - For batch 1300 loss at 500 samples is 1.5386484375\n",
            "2024-01-14 10:18:03,658 - For batch 1400 loss at 500 samples is 1.5314453125\n",
            "2024-01-14 10:18:04,265 - LOSS train 1.5314453125 valid 16.84375\n",
            "2024-01-14 10:18:04,268 - Starting training at 81\n",
            "2024-01-14 10:18:05,167 - For batch 100 loss at 500 samples is 1.54025\n",
            "2024-01-14 10:18:06,094 - For batch 200 loss at 500 samples is 1.54759375\n",
            "2024-01-14 10:18:07,023 - For batch 300 loss at 500 samples is 1.55346875\n",
            "2024-01-14 10:18:07,864 - For batch 400 loss at 500 samples is 1.5484296875\n",
            "2024-01-14 10:18:08,725 - For batch 500 loss at 500 samples is 1.5499140625\n",
            "2024-01-14 10:18:09,618 - For batch 600 loss at 500 samples is 1.5528359375\n",
            "2024-01-14 10:18:10,481 - For batch 700 loss at 500 samples is 1.5299609375\n",
            "2024-01-14 10:18:11,379 - For batch 800 loss at 500 samples is 1.539953125\n",
            "2024-01-14 10:18:12,240 - For batch 900 loss at 500 samples is 1.5401015625\n",
            "2024-01-14 10:18:13,291 - For batch 1000 loss at 500 samples is 1.5480625\n",
            "2024-01-14 10:18:14,635 - For batch 1100 loss at 500 samples is 1.5405546875\n",
            "2024-01-14 10:18:15,975 - For batch 1200 loss at 500 samples is 1.5145390625\n",
            "2024-01-14 10:18:16,825 - For batch 1300 loss at 500 samples is 1.5385546875\n",
            "2024-01-14 10:18:17,699 - For batch 1400 loss at 500 samples is 1.5380625\n",
            "2024-01-14 10:18:18,244 - LOSS train 1.5380625 valid 16.734375\n",
            "2024-01-14 10:18:18,246 - Starting training at 82\n",
            "2024-01-14 10:18:19,140 - For batch 100 loss at 500 samples is 1.545171875\n",
            "2024-01-14 10:18:19,999 - For batch 200 loss at 500 samples is 1.55534375\n",
            "2024-01-14 10:18:20,852 - For batch 300 loss at 500 samples is 1.5495859375\n",
            "2024-01-14 10:18:21,726 - For batch 400 loss at 500 samples is 1.5442890625\n",
            "2024-01-14 10:18:22,574 - For batch 500 loss at 500 samples is 1.555421875\n",
            "2024-01-14 10:18:23,469 - For batch 600 loss at 500 samples is 1.5480703125\n",
            "2024-01-14 10:18:24,385 - For batch 700 loss at 500 samples is 1.5321953125\n",
            "2024-01-14 10:18:25,271 - For batch 800 loss at 500 samples is 1.5409921875\n",
            "2024-01-14 10:18:26,249 - For batch 900 loss at 500 samples is 1.5364453125\n",
            "2024-01-14 10:18:27,603 - For batch 1000 loss at 500 samples is 1.5443671875\n",
            "2024-01-14 10:18:28,909 - For batch 1100 loss at 500 samples is 1.536515625\n",
            "2024-01-14 10:18:29,889 - For batch 1200 loss at 500 samples is 1.526609375\n",
            "2024-01-14 10:18:30,738 - For batch 1300 loss at 500 samples is 1.53771875\n",
            "2024-01-14 10:18:31,611 - For batch 1400 loss at 500 samples is 1.5339375\n",
            "2024-01-14 10:18:32,208 - LOSS train 1.5339375 valid 16.578125\n",
            "2024-01-14 10:18:32,210 - Starting training at 83\n",
            "2024-01-14 10:18:33,162 - For batch 100 loss at 500 samples is 1.548796875\n",
            "2024-01-14 10:18:34,097 - For batch 200 loss at 500 samples is 1.5486796875\n",
            "2024-01-14 10:18:34,989 - For batch 300 loss at 500 samples is 1.5535625\n",
            "2024-01-14 10:18:36,010 - For batch 400 loss at 500 samples is 1.54225\n",
            "2024-01-14 10:18:36,924 - For batch 500 loss at 500 samples is 1.5526796875\n",
            "2024-01-14 10:18:37,843 - For batch 600 loss at 500 samples is 1.547703125\n",
            "2024-01-14 10:18:38,765 - For batch 700 loss at 500 samples is 1.5316640625\n",
            "2024-01-14 10:18:39,954 - For batch 800 loss at 500 samples is 1.5359609375\n",
            "2024-01-14 10:18:41,408 - For batch 900 loss at 500 samples is 1.5362109375\n",
            "2024-01-14 10:18:42,699 - For batch 1000 loss at 500 samples is 1.5412265625\n",
            "2024-01-14 10:18:43,582 - For batch 1100 loss at 500 samples is 1.5386484375\n",
            "2024-01-14 10:18:44,512 - For batch 1200 loss at 500 samples is 1.524015625\n",
            "2024-01-14 10:18:45,460 - For batch 1300 loss at 500 samples is 1.5397265625\n",
            "2024-01-14 10:18:46,385 - For batch 1400 loss at 500 samples is 1.5285859375\n",
            "2024-01-14 10:18:47,001 - LOSS train 1.5285859375 valid 16.078125\n",
            "2024-01-14 10:18:47,006 - Starting training at 84\n",
            "2024-01-14 10:18:47,947 - For batch 100 loss at 500 samples is 1.5471171875\n",
            "2024-01-14 10:18:48,828 - For batch 200 loss at 500 samples is 1.5496640625\n",
            "2024-01-14 10:18:49,705 - For batch 300 loss at 500 samples is 1.55346875\n",
            "2024-01-14 10:18:50,616 - For batch 400 loss at 500 samples is 1.5380078125\n",
            "2024-01-14 10:18:51,576 - For batch 500 loss at 500 samples is 1.5505625\n",
            "2024-01-14 10:18:52,562 - For batch 600 loss at 500 samples is 1.5617734375\n",
            "2024-01-14 10:18:53,981 - For batch 700 loss at 500 samples is 1.538890625\n",
            "2024-01-14 10:18:55,357 - For batch 800 loss at 500 samples is 1.5415546875\n",
            "2024-01-14 10:18:56,370 - For batch 900 loss at 500 samples is 1.532171875\n",
            "2024-01-14 10:18:57,322 - For batch 1000 loss at 500 samples is 1.54215625\n",
            "2024-01-14 10:18:58,245 - For batch 1100 loss at 500 samples is 1.552\n",
            "2024-01-14 10:18:59,125 - For batch 1200 loss at 500 samples is 1.5278125\n",
            "2024-01-14 10:19:00,070 - For batch 1300 loss at 500 samples is 1.544015625\n",
            "2024-01-14 10:19:00,933 - For batch 1400 loss at 500 samples is 1.54059375\n",
            "2024-01-14 10:19:01,437 - LOSS train 1.54059375 valid 16.28125\n",
            "2024-01-14 10:19:01,439 - Starting training at 85\n",
            "2024-01-14 10:19:02,353 - For batch 100 loss at 500 samples is 1.54740625\n",
            "2024-01-14 10:19:03,236 - For batch 200 loss at 500 samples is 1.555875\n",
            "2024-01-14 10:19:04,094 - For batch 300 loss at 500 samples is 1.558484375\n",
            "2024-01-14 10:19:04,957 - For batch 400 loss at 500 samples is 1.546328125\n",
            "2024-01-14 10:19:06,027 - For batch 500 loss at 500 samples is 1.5558828125\n",
            "2024-01-14 10:19:07,431 - For batch 600 loss at 500 samples is 1.555796875\n",
            "2024-01-14 10:19:08,801 - For batch 700 loss at 500 samples is 1.5445703125\n",
            "2024-01-14 10:19:09,661 - For batch 800 loss at 500 samples is 1.5383203125\n",
            "2024-01-14 10:19:10,534 - For batch 900 loss at 500 samples is 1.5446484375\n",
            "2024-01-14 10:19:11,399 - For batch 1000 loss at 500 samples is 1.5529921875\n",
            "2024-01-14 10:19:12,312 - For batch 1100 loss at 500 samples is 1.547546875\n",
            "2024-01-14 10:19:13,183 - For batch 1200 loss at 500 samples is 1.520640625\n",
            "2024-01-14 10:19:14,024 - For batch 1300 loss at 500 samples is 1.5483828125\n",
            "2024-01-14 10:19:14,889 - For batch 1400 loss at 500 samples is 1.5375\n",
            "2024-01-14 10:19:15,452 - LOSS train 1.5375 valid 15.796875\n",
            "2024-01-14 10:19:15,457 - Starting training at 86\n",
            "2024-01-14 10:19:16,293 - For batch 100 loss at 500 samples is 1.5414296875\n",
            "2024-01-14 10:19:17,151 - For batch 200 loss at 500 samples is 1.560640625\n",
            "2024-01-14 10:19:18,053 - For batch 300 loss at 500 samples is 1.555265625\n",
            "2024-01-14 10:19:18,963 - For batch 400 loss at 500 samples is 1.54734375\n",
            "2024-01-14 10:19:20,281 - For batch 500 loss at 500 samples is 1.549296875\n",
            "2024-01-14 10:19:21,662 - For batch 600 loss at 500 samples is 1.5558984375\n",
            "2024-01-14 10:19:22,638 - For batch 700 loss at 500 samples is 1.53546875\n",
            "2024-01-14 10:19:23,549 - For batch 800 loss at 500 samples is 1.5426328125\n",
            "2024-01-14 10:19:24,469 - For batch 900 loss at 500 samples is 1.5385859375\n",
            "2024-01-14 10:19:25,352 - For batch 1000 loss at 500 samples is 1.554984375\n",
            "2024-01-14 10:19:26,208 - For batch 1100 loss at 500 samples is 1.543265625\n",
            "2024-01-14 10:19:27,124 - For batch 1200 loss at 500 samples is 1.52653125\n",
            "2024-01-14 10:19:28,029 - For batch 1300 loss at 500 samples is 1.5468359375\n",
            "2024-01-14 10:19:28,870 - For batch 1400 loss at 500 samples is 1.5427421875\n",
            "2024-01-14 10:19:29,444 - LOSS train 1.5427421875 valid 15.4453125\n",
            "2024-01-14 10:19:29,446 - Starting training at 87\n",
            "2024-01-14 10:19:30,313 - For batch 100 loss at 500 samples is 1.54625\n",
            "2024-01-14 10:19:31,222 - For batch 200 loss at 500 samples is 1.5580078125\n",
            "2024-01-14 10:19:32,143 - For batch 300 loss at 500 samples is 1.5565\n",
            "2024-01-14 10:19:33,511 - For batch 400 loss at 500 samples is 1.5453359375\n",
            "2024-01-14 10:19:34,843 - For batch 500 loss at 500 samples is 1.5545703125\n",
            "2024-01-14 10:19:35,832 - For batch 600 loss at 500 samples is 1.551484375\n",
            "2024-01-14 10:19:36,698 - For batch 700 loss at 500 samples is 1.534734375\n",
            "2024-01-14 10:19:37,592 - For batch 800 loss at 500 samples is 1.5431484375\n",
            "2024-01-14 10:19:38,462 - For batch 900 loss at 500 samples is 1.540484375\n",
            "2024-01-14 10:19:39,310 - For batch 1000 loss at 500 samples is 1.5589765625\n",
            "2024-01-14 10:19:40,126 - For batch 1100 loss at 500 samples is 1.5429921875\n",
            "2024-01-14 10:19:40,972 - For batch 1200 loss at 500 samples is 1.5339140625\n",
            "2024-01-14 10:19:41,803 - For batch 1300 loss at 500 samples is 1.54384375\n",
            "2024-01-14 10:19:42,707 - For batch 1400 loss at 500 samples is 1.5303671875\n",
            "2024-01-14 10:19:43,209 - LOSS train 1.5303671875 valid 15.2890625\n",
            "2024-01-14 10:19:43,212 - Starting training at 88\n",
            "2024-01-14 10:19:44,074 - For batch 100 loss at 500 samples is 1.5537265625\n",
            "2024-01-14 10:19:44,985 - For batch 200 loss at 500 samples is 1.5566953125\n",
            "2024-01-14 10:19:46,222 - For batch 300 loss at 500 samples is 1.5580703125\n",
            "2024-01-14 10:19:47,571 - For batch 400 loss at 500 samples is 1.5415234375\n",
            "2024-01-14 10:19:48,793 - For batch 500 loss at 500 samples is 1.553578125\n",
            "2024-01-14 10:19:49,685 - For batch 600 loss at 500 samples is 1.5563515625\n",
            "2024-01-14 10:19:50,561 - For batch 700 loss at 500 samples is 1.538265625\n",
            "2024-01-14 10:19:51,464 - For batch 800 loss at 500 samples is 1.540921875\n",
            "2024-01-14 10:19:52,390 - For batch 900 loss at 500 samples is 1.5454296875\n",
            "2024-01-14 10:19:53,320 - For batch 1000 loss at 500 samples is 1.5554765625\n",
            "2024-01-14 10:19:54,175 - For batch 1100 loss at 500 samples is 1.5410859375\n",
            "2024-01-14 10:19:55,057 - For batch 1200 loss at 500 samples is 1.530953125\n",
            "2024-01-14 10:19:55,968 - For batch 1300 loss at 500 samples is 1.544484375\n",
            "2024-01-14 10:19:56,818 - For batch 1400 loss at 500 samples is 1.5344140625\n",
            "2024-01-14 10:19:57,407 - LOSS train 1.5344140625 valid 15.09375\n",
            "2024-01-14 10:19:57,411 - Starting training at 89\n",
            "2024-01-14 10:19:58,346 - For batch 100 loss at 500 samples is 1.5546015625\n",
            "2024-01-14 10:19:59,635 - For batch 200 loss at 500 samples is 1.556765625\n",
            "2024-01-14 10:20:00,966 - For batch 300 loss at 500 samples is 1.5554375\n",
            "2024-01-14 10:20:02,095 - For batch 400 loss at 500 samples is 1.5439453125\n",
            "2024-01-14 10:20:02,995 - For batch 500 loss at 500 samples is 1.553890625\n",
            "2024-01-14 10:20:03,853 - For batch 600 loss at 500 samples is 1.558875\n",
            "2024-01-14 10:20:04,760 - For batch 700 loss at 500 samples is 1.541078125\n",
            "2024-01-14 10:20:05,637 - For batch 800 loss at 500 samples is 1.543328125\n",
            "2024-01-14 10:20:06,533 - For batch 900 loss at 500 samples is 1.541265625\n",
            "2024-01-14 10:20:07,424 - For batch 1000 loss at 500 samples is 1.56028125\n",
            "2024-01-14 10:20:08,315 - For batch 1100 loss at 500 samples is 1.5386484375\n",
            "2024-01-14 10:20:09,193 - For batch 1200 loss at 500 samples is 1.5259453125\n",
            "2024-01-14 10:20:10,075 - For batch 1300 loss at 500 samples is 1.5444296875\n",
            "2024-01-14 10:20:10,901 - For batch 1400 loss at 500 samples is 1.5345625\n",
            "2024-01-14 10:20:11,481 - LOSS train 1.5345625 valid 14.9765625\n",
            "2024-01-14 10:20:11,485 - Starting training at 90\n",
            "2024-01-14 10:20:12,885 - For batch 100 loss at 500 samples is 1.5523984375\n",
            "2024-01-14 10:20:14,330 - For batch 200 loss at 500 samples is 1.560078125\n",
            "2024-01-14 10:20:15,397 - For batch 300 loss at 500 samples is 1.5595703125\n",
            "2024-01-14 10:20:16,306 - For batch 400 loss at 500 samples is 1.5456015625\n",
            "2024-01-14 10:20:17,200 - For batch 500 loss at 500 samples is 1.5597890625\n",
            "2024-01-14 10:20:18,051 - For batch 600 loss at 500 samples is 1.561640625\n",
            "2024-01-14 10:20:18,935 - For batch 700 loss at 500 samples is 1.5432265625\n",
            "2024-01-14 10:20:19,802 - For batch 800 loss at 500 samples is 1.5515390625\n",
            "2024-01-14 10:20:20,679 - For batch 900 loss at 500 samples is 1.5449453125\n",
            "2024-01-14 10:20:21,597 - For batch 1000 loss at 500 samples is 1.5561484375\n",
            "2024-01-14 10:20:22,517 - For batch 1100 loss at 500 samples is 1.54328125\n",
            "2024-01-14 10:20:23,383 - For batch 1200 loss at 500 samples is 1.5284140625\n",
            "2024-01-14 10:20:24,274 - For batch 1300 loss at 500 samples is 1.544984375\n",
            "2024-01-14 10:20:25,322 - For batch 1400 loss at 500 samples is 1.5377890625\n",
            "2024-01-14 10:20:26,168 - LOSS train 1.5377890625 valid 14.78125\n",
            "2024-01-14 10:20:26,176 - Starting training at 91\n",
            "2024-01-14 10:20:27,489 - For batch 100 loss at 500 samples is 1.5540703125\n",
            "2024-01-14 10:20:28,596 - For batch 200 loss at 500 samples is 1.55946875\n",
            "2024-01-14 10:20:29,432 - For batch 300 loss at 500 samples is 1.5622578125\n",
            "2024-01-14 10:20:30,306 - For batch 400 loss at 500 samples is 1.548890625\n",
            "2024-01-14 10:20:31,246 - For batch 500 loss at 500 samples is 1.560453125\n",
            "2024-01-14 10:20:32,135 - For batch 600 loss at 500 samples is 1.5655\n",
            "2024-01-14 10:20:33,009 - For batch 700 loss at 500 samples is 1.54253125\n",
            "2024-01-14 10:20:33,938 - For batch 800 loss at 500 samples is 1.5484453125\n",
            "2024-01-14 10:20:34,839 - For batch 900 loss at 500 samples is 1.5517578125\n",
            "2024-01-14 10:20:35,663 - For batch 1000 loss at 500 samples is 1.557875\n",
            "2024-01-14 10:20:36,573 - For batch 1100 loss at 500 samples is 1.5409140625\n",
            "2024-01-14 10:20:37,481 - For batch 1200 loss at 500 samples is 1.5260078125\n",
            "2024-01-14 10:20:38,490 - For batch 1300 loss at 500 samples is 1.5413046875\n",
            "2024-01-14 10:20:39,913 - For batch 1400 loss at 500 samples is 1.535\n",
            "2024-01-14 10:20:40,691 - LOSS train 1.535 valid 14.546875\n",
            "2024-01-14 10:20:40,697 - Starting training at 92\n",
            "2024-01-14 10:20:41,806 - For batch 100 loss at 500 samples is 1.5537578125\n",
            "2024-01-14 10:20:42,720 - For batch 200 loss at 500 samples is 1.55571875\n",
            "2024-01-14 10:20:43,647 - For batch 300 loss at 500 samples is 1.5665546875\n",
            "2024-01-14 10:20:44,575 - For batch 400 loss at 500 samples is 1.54853125\n",
            "2024-01-14 10:20:45,476 - For batch 500 loss at 500 samples is 1.5540546875\n",
            "2024-01-14 10:20:46,298 - For batch 600 loss at 500 samples is 1.5583515625\n",
            "2024-01-14 10:20:47,213 - For batch 700 loss at 500 samples is 1.5455859375\n",
            "2024-01-14 10:20:48,088 - For batch 800 loss at 500 samples is 1.54703125\n",
            "2024-01-14 10:20:49,017 - For batch 900 loss at 500 samples is 1.5515078125\n",
            "2024-01-14 10:20:49,876 - For batch 1000 loss at 500 samples is 1.5570625\n",
            "2024-01-14 10:20:50,696 - For batch 1100 loss at 500 samples is 1.54303125\n",
            "2024-01-14 10:20:51,716 - For batch 1200 loss at 500 samples is 1.52996875\n",
            "2024-01-14 10:20:53,106 - For batch 1300 loss at 500 samples is 1.54446875\n",
            "2024-01-14 10:20:54,367 - For batch 1400 loss at 500 samples is 1.5379296875\n",
            "2024-01-14 10:20:55,016 - LOSS train 1.5379296875 valid 14.453125\n",
            "2024-01-14 10:20:55,019 - Starting training at 93\n",
            "2024-01-14 10:20:55,964 - For batch 100 loss at 500 samples is 1.55465625\n",
            "2024-01-14 10:20:56,888 - For batch 200 loss at 500 samples is 1.5594296875\n",
            "2024-01-14 10:20:57,813 - For batch 300 loss at 500 samples is 1.5651171875\n",
            "2024-01-14 10:20:58,726 - For batch 400 loss at 500 samples is 1.5507890625\n",
            "2024-01-14 10:20:59,644 - For batch 500 loss at 500 samples is 1.5622109375\n",
            "2024-01-14 10:21:00,523 - For batch 600 loss at 500 samples is 1.5556328125\n",
            "2024-01-14 10:21:01,437 - For batch 700 loss at 500 samples is 1.53884375\n",
            "2024-01-14 10:21:02,429 - For batch 800 loss at 500 samples is 1.5555\n",
            "2024-01-14 10:21:03,346 - For batch 900 loss at 500 samples is 1.5588359375\n",
            "2024-01-14 10:21:04,308 - For batch 1000 loss at 500 samples is 1.5599609375\n",
            "2024-01-14 10:21:05,684 - For batch 1100 loss at 500 samples is 1.54109375\n",
            "2024-01-14 10:21:07,284 - For batch 1200 loss at 500 samples is 1.53315625\n",
            "2024-01-14 10:21:08,516 - For batch 1300 loss at 500 samples is 1.54471875\n",
            "2024-01-14 10:21:09,516 - For batch 1400 loss at 500 samples is 1.5399296875\n",
            "2024-01-14 10:21:10,181 - LOSS train 1.5399296875 valid 14.1484375\n",
            "2024-01-14 10:21:10,188 - Starting training at 94\n",
            "2024-01-14 10:21:11,189 - For batch 100 loss at 500 samples is 1.5602578125\n",
            "2024-01-14 10:21:12,164 - For batch 200 loss at 500 samples is 1.559765625\n",
            "2024-01-14 10:21:13,110 - For batch 300 loss at 500 samples is 1.5622421875\n",
            "2024-01-14 10:21:14,085 - For batch 400 loss at 500 samples is 1.5494765625\n",
            "2024-01-14 10:21:14,989 - For batch 500 loss at 500 samples is 1.5556875\n",
            "2024-01-14 10:21:15,921 - For batch 600 loss at 500 samples is 1.556609375\n",
            "2024-01-14 10:21:16,854 - For batch 700 loss at 500 samples is 1.5410546875\n",
            "2024-01-14 10:21:17,765 - For batch 800 loss at 500 samples is 1.5471640625\n",
            "2024-01-14 10:21:19,077 - For batch 900 loss at 500 samples is 1.5474140625\n",
            "2024-01-14 10:21:20,538 - For batch 1000 loss at 500 samples is 1.5629453125\n",
            "2024-01-14 10:21:21,835 - For batch 1100 loss at 500 samples is 1.543765625\n",
            "2024-01-14 10:21:22,788 - For batch 1200 loss at 500 samples is 1.5298515625\n",
            "2024-01-14 10:21:23,818 - For batch 1300 loss at 500 samples is 1.544671875\n",
            "2024-01-14 10:21:24,787 - For batch 1400 loss at 500 samples is 1.542234375\n",
            "2024-01-14 10:21:25,306 - LOSS train 1.542234375 valid 14.203125\n",
            "2024-01-14 10:21:25,309 - Starting training at 95\n",
            "2024-01-14 10:21:26,264 - For batch 100 loss at 500 samples is 1.557671875\n",
            "2024-01-14 10:21:27,238 - For batch 200 loss at 500 samples is 1.5623046875\n",
            "2024-01-14 10:21:28,188 - For batch 300 loss at 500 samples is 1.5651640625\n",
            "2024-01-14 10:21:29,190 - For batch 400 loss at 500 samples is 1.550328125\n",
            "2024-01-14 10:21:30,167 - For batch 500 loss at 500 samples is 1.55715625\n",
            "2024-01-14 10:21:31,140 - For batch 600 loss at 500 samples is 1.558046875\n",
            "2024-01-14 10:21:32,361 - For batch 700 loss at 500 samples is 1.5388359375\n",
            "2024-01-14 10:21:33,758 - For batch 800 loss at 500 samples is 1.55175\n",
            "2024-01-14 10:21:35,421 - For batch 900 loss at 500 samples is 1.5506015625\n",
            "2024-01-14 10:21:36,427 - For batch 1000 loss at 500 samples is 1.5593359375\n",
            "2024-01-14 10:21:37,338 - For batch 1100 loss at 500 samples is 1.5426484375\n",
            "2024-01-14 10:21:38,285 - For batch 1200 loss at 500 samples is 1.530734375\n",
            "2024-01-14 10:21:39,260 - For batch 1300 loss at 500 samples is 1.5454375\n",
            "2024-01-14 10:21:40,224 - For batch 1400 loss at 500 samples is 1.546265625\n",
            "2024-01-14 10:21:40,820 - LOSS train 1.546265625 valid 13.875\n",
            "2024-01-14 10:21:40,822 - Starting training at 96\n",
            "2024-01-14 10:21:41,739 - For batch 100 loss at 500 samples is 1.560265625\n",
            "2024-01-14 10:21:42,628 - For batch 200 loss at 500 samples is 1.5644765625\n",
            "2024-01-14 10:21:43,446 - For batch 300 loss at 500 samples is 1.5675078125\n",
            "2024-01-14 10:21:44,365 - For batch 400 loss at 500 samples is 1.5513984375\n",
            "2024-01-14 10:21:45,228 - For batch 500 loss at 500 samples is 1.5609140625\n",
            "2024-01-14 10:21:46,400 - For batch 600 loss at 500 samples is 1.5633984375\n",
            "2024-01-14 10:21:47,664 - For batch 700 loss at 500 samples is 1.547125\n",
            "2024-01-14 10:21:48,951 - For batch 800 loss at 500 samples is 1.54771875\n",
            "2024-01-14 10:21:49,788 - For batch 900 loss at 500 samples is 1.552046875\n",
            "2024-01-14 10:21:50,652 - For batch 1000 loss at 500 samples is 1.559453125\n",
            "2024-01-14 10:21:51,475 - For batch 1100 loss at 500 samples is 1.5448203125\n",
            "2024-01-14 10:21:52,398 - For batch 1200 loss at 500 samples is 1.5314453125\n",
            "2024-01-14 10:21:53,243 - For batch 1300 loss at 500 samples is 1.5453515625\n",
            "2024-01-14 10:21:54,134 - For batch 1400 loss at 500 samples is 1.5493671875\n",
            "2024-01-14 10:21:54,751 - LOSS train 1.5493671875 valid 13.9765625\n",
            "2024-01-14 10:21:54,761 - Starting training at 97\n",
            "2024-01-14 10:21:55,532 - For batch 100 loss at 500 samples is 1.5585\n",
            "2024-01-14 10:21:56,420 - For batch 200 loss at 500 samples is 1.565484375\n",
            "2024-01-14 10:21:57,330 - For batch 300 loss at 500 samples is 1.568546875\n",
            "2024-01-14 10:21:58,155 - For batch 400 loss at 500 samples is 1.553421875\n",
            "2024-01-14 10:21:59,064 - For batch 500 loss at 500 samples is 1.5578203125\n",
            "2024-01-14 10:22:00,384 - For batch 600 loss at 500 samples is 1.5598671875\n",
            "2024-01-14 10:22:01,627 - For batch 700 loss at 500 samples is 1.5485390625\n",
            "2024-01-14 10:22:02,616 - For batch 800 loss at 500 samples is 1.5444140625\n",
            "2024-01-14 10:22:03,507 - For batch 900 loss at 500 samples is 1.5506796875\n",
            "2024-01-14 10:22:04,373 - For batch 1000 loss at 500 samples is 1.563078125\n",
            "2024-01-14 10:22:05,169 - For batch 1100 loss at 500 samples is 1.5463359375\n",
            "2024-01-14 10:22:06,005 - For batch 1200 loss at 500 samples is 1.530125\n",
            "2024-01-14 10:22:06,890 - For batch 1300 loss at 500 samples is 1.550765625\n",
            "2024-01-14 10:22:07,768 - For batch 1400 loss at 500 samples is 1.54515625\n",
            "2024-01-14 10:22:08,312 - LOSS train 1.54515625 valid 13.703125\n",
            "2024-01-14 10:22:08,314 - Starting training at 98\n",
            "2024-01-14 10:22:09,160 - For batch 100 loss at 500 samples is 1.558734375\n",
            "2024-01-14 10:22:10,036 - For batch 200 loss at 500 samples is 1.5665234375\n",
            "2024-01-14 10:22:10,895 - For batch 300 loss at 500 samples is 1.5687578125\n",
            "2024-01-14 10:22:11,772 - For batch 400 loss at 500 samples is 1.5561484375\n",
            "2024-01-14 10:22:12,856 - For batch 500 loss at 500 samples is 1.5602890625\n",
            "2024-01-14 10:22:14,269 - For batch 600 loss at 500 samples is 1.5596796875\n",
            "2024-01-14 10:22:15,407 - For batch 700 loss at 500 samples is 1.5465859375\n",
            "2024-01-14 10:22:16,260 - For batch 800 loss at 500 samples is 1.542625\n",
            "2024-01-14 10:22:17,148 - For batch 900 loss at 500 samples is 1.552234375\n",
            "2024-01-14 10:22:17,995 - For batch 1000 loss at 500 samples is 1.5623125\n",
            "2024-01-14 10:22:18,903 - For batch 1100 loss at 500 samples is 1.544046875\n",
            "2024-01-14 10:22:19,723 - For batch 1200 loss at 500 samples is 1.5378984375\n",
            "2024-01-14 10:22:20,600 - For batch 1300 loss at 500 samples is 1.550765625\n",
            "2024-01-14 10:22:21,482 - For batch 1400 loss at 500 samples is 1.541578125\n",
            "2024-01-14 10:22:22,042 - LOSS train 1.541578125 valid 13.4921875\n",
            "2024-01-14 10:22:22,044 - Starting training at 99\n",
            "2024-01-14 10:22:22,894 - For batch 100 loss at 500 samples is 1.5590703125\n",
            "2024-01-14 10:22:23,756 - For batch 200 loss at 500 samples is 1.5650625\n",
            "2024-01-14 10:22:24,661 - For batch 300 loss at 500 samples is 1.5674765625\n",
            "2024-01-14 10:22:25,762 - For batch 400 loss at 500 samples is 1.553828125\n",
            "2024-01-14 10:22:27,049 - For batch 500 loss at 500 samples is 1.5621875\n",
            "2024-01-14 10:22:28,408 - For batch 600 loss at 500 samples is 1.559109375\n",
            "2024-01-14 10:22:29,327 - For batch 700 loss at 500 samples is 1.5463828125\n",
            "2024-01-14 10:22:30,238 - For batch 800 loss at 500 samples is 1.5474140625\n",
            "2024-01-14 10:22:31,088 - For batch 900 loss at 500 samples is 1.5581015625\n",
            "2024-01-14 10:22:31,945 - For batch 1000 loss at 500 samples is 1.5630625\n",
            "2024-01-14 10:22:32,842 - For batch 1100 loss at 500 samples is 1.544796875\n",
            "2024-01-14 10:22:33,664 - For batch 1200 loss at 500 samples is 1.537734375\n",
            "2024-01-14 10:22:34,488 - For batch 1300 loss at 500 samples is 1.55140625\n",
            "2024-01-14 10:22:35,269 - For batch 1400 loss at 500 samples is 1.547328125\n",
            "2024-01-14 10:22:35,811 - LOSS train 1.547328125 valid 13.34375\n",
            "2024-01-14 10:22:35,819 - Starting training at 100\n",
            "2024-01-14 10:22:36,711 - For batch 100 loss at 500 samples is 1.5606953125\n",
            "2024-01-14 10:22:37,569 - For batch 200 loss at 500 samples is 1.5714609375\n",
            "2024-01-14 10:22:38,387 - For batch 300 loss at 500 samples is 1.5701796875\n",
            "2024-01-14 10:22:39,690 - For batch 400 loss at 500 samples is 1.553453125\n",
            "2024-01-14 10:22:40,958 - For batch 500 loss at 500 samples is 1.5593984375\n",
            "2024-01-14 10:22:42,064 - For batch 600 loss at 500 samples is 1.5581015625\n",
            "2024-01-14 10:22:42,926 - For batch 700 loss at 500 samples is 1.5509140625\n",
            "2024-01-14 10:22:43,802 - For batch 800 loss at 500 samples is 1.54709375\n",
            "2024-01-14 10:22:44,684 - For batch 900 loss at 500 samples is 1.5538046875\n",
            "2024-01-14 10:22:45,569 - For batch 1000 loss at 500 samples is 1.566515625\n",
            "2024-01-14 10:22:46,403 - For batch 1100 loss at 500 samples is 1.548546875\n",
            "2024-01-14 10:22:47,292 - For batch 1200 loss at 500 samples is 1.5328828125\n",
            "2024-01-14 10:22:48,164 - For batch 1300 loss at 500 samples is 1.5515625\n",
            "2024-01-14 10:22:49,047 - For batch 1400 loss at 500 samples is 1.5415390625\n",
            "2024-01-14 10:22:49,627 - LOSS train 1.5415390625 valid 13.203125\n",
            "2024-01-14 10:22:49,633 - Starting training at 101\n",
            "2024-01-14 10:22:50,393 - For batch 100 loss at 500 samples is 1.56\n",
            "2024-01-14 10:22:51,237 - For batch 200 loss at 500 samples is 1.5661875\n",
            "2024-01-14 10:22:52,312 - For batch 300 loss at 500 samples is 1.569984375\n",
            "2024-01-14 10:22:53,621 - For batch 400 loss at 500 samples is 1.55546875\n",
            "2024-01-14 10:22:54,890 - For batch 500 loss at 500 samples is 1.5600703125\n",
            "2024-01-14 10:22:55,717 - For batch 600 loss at 500 samples is 1.5591015625\n",
            "2024-01-14 10:22:56,507 - For batch 700 loss at 500 samples is 1.547828125\n",
            "2024-01-14 10:22:57,370 - For batch 800 loss at 500 samples is 1.54646875\n",
            "2024-01-14 10:22:58,255 - For batch 900 loss at 500 samples is 1.556921875\n",
            "2024-01-14 10:22:59,158 - For batch 1000 loss at 500 samples is 1.5589140625\n",
            "2024-01-14 10:23:00,041 - For batch 1100 loss at 500 samples is 1.5435390625\n",
            "2024-01-14 10:23:00,918 - For batch 1200 loss at 500 samples is 1.5366328125\n",
            "2024-01-14 10:23:01,761 - For batch 1300 loss at 500 samples is 1.5510078125\n",
            "2024-01-14 10:23:02,680 - For batch 1400 loss at 500 samples is 1.5531796875\n",
            "2024-01-14 10:23:03,138 - LOSS train 1.5531796875 valid 13.1875\n",
            "2024-01-14 10:23:03,143 - Starting training at 102\n",
            "2024-01-14 10:23:03,989 - For batch 100 loss at 500 samples is 1.56175\n",
            "2024-01-14 10:23:04,874 - For batch 200 loss at 500 samples is 1.56828125\n",
            "2024-01-14 10:23:06,116 - For batch 300 loss at 500 samples is 1.5658359375\n",
            "2024-01-14 10:23:07,316 - For batch 400 loss at 500 samples is 1.5572890625\n",
            "2024-01-14 10:23:08,450 - For batch 500 loss at 500 samples is 1.561609375\n",
            "2024-01-14 10:23:09,294 - For batch 600 loss at 500 samples is 1.5645390625\n",
            "2024-01-14 10:23:10,197 - For batch 700 loss at 500 samples is 1.5543984375\n",
            "2024-01-14 10:23:11,034 - For batch 800 loss at 500 samples is 1.54515625\n",
            "2024-01-14 10:23:11,886 - For batch 900 loss at 500 samples is 1.55609375\n",
            "2024-01-14 10:23:12,762 - For batch 1000 loss at 500 samples is 1.5602265625\n",
            "2024-01-14 10:23:13,573 - For batch 1100 loss at 500 samples is 1.5419296875\n",
            "2024-01-14 10:23:14,418 - For batch 1200 loss at 500 samples is 1.5363359375\n",
            "2024-01-14 10:23:15,291 - For batch 1300 loss at 500 samples is 1.5535078125\n",
            "2024-01-14 10:23:16,106 - For batch 1400 loss at 500 samples is 1.540484375\n",
            "2024-01-14 10:23:16,718 - LOSS train 1.540484375 valid 12.875\n",
            "2024-01-14 10:23:16,730 - Starting training at 103\n",
            "2024-01-14 10:23:17,559 - For batch 100 loss at 500 samples is 1.561203125\n",
            "2024-01-14 10:23:18,638 - For batch 200 loss at 500 samples is 1.5692109375\n",
            "2024-01-14 10:23:19,906 - For batch 300 loss at 500 samples is 1.567328125\n",
            "2024-01-14 10:23:21,236 - For batch 400 loss at 500 samples is 1.5593125\n",
            "2024-01-14 10:23:22,088 - For batch 500 loss at 500 samples is 1.55671875\n",
            "2024-01-14 10:23:22,991 - For batch 600 loss at 500 samples is 1.571234375\n",
            "2024-01-14 10:23:23,843 - For batch 700 loss at 500 samples is 1.55284375\n",
            "2024-01-14 10:23:24,683 - For batch 800 loss at 500 samples is 1.5506328125\n",
            "2024-01-14 10:23:25,553 - For batch 900 loss at 500 samples is 1.5555234375\n",
            "2024-01-14 10:23:26,355 - For batch 1000 loss at 500 samples is 1.5646796875\n",
            "2024-01-14 10:23:27,244 - For batch 1100 loss at 500 samples is 1.5437578125\n",
            "2024-01-14 10:23:28,157 - For batch 1200 loss at 500 samples is 1.538453125\n",
            "2024-01-14 10:23:29,021 - For batch 1300 loss at 500 samples is 1.552890625\n",
            "2024-01-14 10:23:29,856 - For batch 1400 loss at 500 samples is 1.5471953125\n",
            "2024-01-14 10:23:30,449 - LOSS train 1.5471953125 valid 12.828125\n",
            "2024-01-14 10:23:30,454 - Starting training at 104\n",
            "2024-01-14 10:23:31,391 - For batch 100 loss at 500 samples is 1.563578125\n",
            "2024-01-14 10:23:32,712 - For batch 200 loss at 500 samples is 1.5715390625\n",
            "2024-01-14 10:23:34,031 - For batch 300 loss at 500 samples is 1.56446875\n",
            "2024-01-14 10:23:34,960 - For batch 400 loss at 500 samples is 1.5590546875\n",
            "2024-01-14 10:23:35,756 - For batch 500 loss at 500 samples is 1.55890625\n",
            "2024-01-14 10:23:36,572 - For batch 600 loss at 500 samples is 1.564984375\n",
            "2024-01-14 10:23:37,479 - For batch 700 loss at 500 samples is 1.5489765625\n",
            "2024-01-14 10:23:38,289 - For batch 800 loss at 500 samples is 1.5577578125\n",
            "2024-01-14 10:23:39,106 - For batch 900 loss at 500 samples is 1.5538203125\n",
            "2024-01-14 10:23:39,944 - For batch 1000 loss at 500 samples is 1.56403125\n",
            "2024-01-14 10:23:40,708 - For batch 1100 loss at 500 samples is 1.550921875\n",
            "2024-01-14 10:23:41,496 - For batch 1200 loss at 500 samples is 1.53696875\n",
            "2024-01-14 10:23:42,345 - For batch 1300 loss at 500 samples is 1.551125\n",
            "2024-01-14 10:23:43,244 - For batch 1400 loss at 500 samples is 1.5444765625\n",
            "2024-01-14 10:23:43,783 - LOSS train 1.5444765625 valid 12.71875\n",
            "2024-01-14 10:23:43,786 - Starting training at 105\n",
            "2024-01-14 10:23:44,671 - For batch 100 loss at 500 samples is 1.56565625\n",
            "2024-01-14 10:23:46,158 - For batch 200 loss at 500 samples is 1.569671875\n",
            "2024-01-14 10:23:47,378 - For batch 300 loss at 500 samples is 1.566703125\n",
            "2024-01-14 10:23:48,274 - For batch 400 loss at 500 samples is 1.55734375\n",
            "2024-01-14 10:23:49,091 - For batch 500 loss at 500 samples is 1.561953125\n",
            "2024-01-14 10:23:49,968 - For batch 600 loss at 500 samples is 1.5659921875\n",
            "2024-01-14 10:23:50,869 - For batch 700 loss at 500 samples is 1.548703125\n",
            "2024-01-14 10:23:51,725 - For batch 800 loss at 500 samples is 1.54984375\n",
            "2024-01-14 10:23:52,587 - For batch 900 loss at 500 samples is 1.558375\n",
            "2024-01-14 10:23:53,471 - For batch 1000 loss at 500 samples is 1.5617265625\n",
            "2024-01-14 10:23:54,314 - For batch 1100 loss at 500 samples is 1.5459140625\n",
            "2024-01-14 10:23:55,169 - For batch 1200 loss at 500 samples is 1.534703125\n",
            "2024-01-14 10:23:55,964 - For batch 1300 loss at 500 samples is 1.5527109375\n",
            "2024-01-14 10:23:56,792 - For batch 1400 loss at 500 samples is 1.5463671875\n",
            "2024-01-14 10:23:57,413 - LOSS train 1.5463671875 valid 12.578125\n",
            "2024-01-14 10:23:57,416 - Starting training at 106\n",
            "2024-01-14 10:23:58,483 - For batch 100 loss at 500 samples is 1.5659140625\n",
            "2024-01-14 10:23:59,782 - For batch 200 loss at 500 samples is 1.5710859375\n",
            "2024-01-14 10:24:00,968 - For batch 300 loss at 500 samples is 1.5698125\n",
            "2024-01-14 10:24:01,779 - For batch 400 loss at 500 samples is 1.559828125\n",
            "2024-01-14 10:24:02,602 - For batch 500 loss at 500 samples is 1.5610546875\n",
            "2024-01-14 10:24:03,491 - For batch 600 loss at 500 samples is 1.5678671875\n",
            "2024-01-14 10:24:04,303 - For batch 700 loss at 500 samples is 1.553609375\n",
            "2024-01-14 10:24:05,233 - For batch 800 loss at 500 samples is 1.550984375\n",
            "2024-01-14 10:24:06,034 - For batch 900 loss at 500 samples is 1.5583828125\n",
            "2024-01-14 10:24:06,964 - For batch 1000 loss at 500 samples is 1.5649453125\n",
            "2024-01-14 10:24:07,813 - For batch 1100 loss at 500 samples is 1.54940625\n",
            "2024-01-14 10:24:08,635 - For batch 1200 loss at 500 samples is 1.5413984375\n",
            "2024-01-14 10:24:09,508 - For batch 1300 loss at 500 samples is 1.5518984375\n",
            "2024-01-14 10:24:10,373 - For batch 1400 loss at 500 samples is 1.548890625\n",
            "2024-01-14 10:24:10,950 - LOSS train 1.548890625 valid 12.4609375\n",
            "2024-01-14 10:24:10,952 - Starting training at 107\n",
            "2024-01-14 10:24:12,126 - For batch 100 loss at 500 samples is 1.566484375\n",
            "2024-01-14 10:24:13,409 - For batch 200 loss at 500 samples is 1.5734375\n",
            "2024-01-14 10:24:14,551 - For batch 300 loss at 500 samples is 1.570484375\n",
            "2024-01-14 10:24:15,319 - For batch 400 loss at 500 samples is 1.5581015625\n",
            "2024-01-14 10:24:16,183 - For batch 500 loss at 500 samples is 1.556203125\n",
            "2024-01-14 10:24:17,124 - For batch 600 loss at 500 samples is 1.5747109375\n",
            "2024-01-14 10:24:17,963 - For batch 700 loss at 500 samples is 1.5531796875\n",
            "2024-01-14 10:24:18,744 - For batch 800 loss at 500 samples is 1.558421875\n",
            "2024-01-14 10:24:19,600 - For batch 900 loss at 500 samples is 1.5579296875\n",
            "2024-01-14 10:24:20,458 - For batch 1000 loss at 500 samples is 1.5635859375\n",
            "2024-01-14 10:24:21,280 - For batch 1100 loss at 500 samples is 1.55140625\n",
            "2024-01-14 10:24:22,143 - For batch 1200 loss at 500 samples is 1.54646875\n",
            "2024-01-14 10:24:23,099 - For batch 1300 loss at 500 samples is 1.55359375\n",
            "2024-01-14 10:24:24,040 - For batch 1400 loss at 500 samples is 1.5524765625\n",
            "2024-01-14 10:24:24,781 - LOSS train 1.5524765625 valid 12.2421875\n",
            "2024-01-14 10:24:24,789 - Starting training at 108\n",
            "2024-01-14 10:24:25,857 - For batch 100 loss at 500 samples is 1.565609375\n",
            "2024-01-14 10:24:27,057 - For batch 200 loss at 500 samples is 1.5741953125\n",
            "2024-01-14 10:24:28,016 - For batch 300 loss at 500 samples is 1.5683828125\n",
            "2024-01-14 10:24:28,823 - For batch 400 loss at 500 samples is 1.5605625\n",
            "2024-01-14 10:24:29,568 - For batch 500 loss at 500 samples is 1.560359375\n",
            "2024-01-14 10:24:30,398 - For batch 600 loss at 500 samples is 1.56690625\n",
            "2024-01-14 10:24:31,170 - For batch 700 loss at 500 samples is 1.54934375\n",
            "2024-01-14 10:24:32,077 - For batch 800 loss at 500 samples is 1.5521015625\n",
            "2024-01-14 10:24:32,886 - For batch 900 loss at 500 samples is 1.5603046875\n",
            "2024-01-14 10:24:33,666 - For batch 1000 loss at 500 samples is 1.56359375\n",
            "2024-01-14 10:24:34,527 - For batch 1100 loss at 500 samples is 1.5467578125\n",
            "2024-01-14 10:24:35,366 - For batch 1200 loss at 500 samples is 1.537609375\n",
            "2024-01-14 10:24:36,199 - For batch 1300 loss at 500 samples is 1.555078125\n",
            "2024-01-14 10:24:37,025 - For batch 1400 loss at 500 samples is 1.5454453125\n",
            "2024-01-14 10:24:37,753 - LOSS train 1.5454453125 valid 12.234375\n",
            "2024-01-14 10:24:37,756 - Starting training at 109\n",
            "2024-01-14 10:24:39,049 - For batch 100 loss at 500 samples is 1.566125\n",
            "2024-01-14 10:24:40,283 - For batch 200 loss at 500 samples is 1.57265625\n",
            "2024-01-14 10:24:41,120 - For batch 300 loss at 500 samples is 1.5671171875\n",
            "2024-01-14 10:24:41,946 - For batch 400 loss at 500 samples is 1.560140625\n",
            "2024-01-14 10:24:42,829 - For batch 500 loss at 500 samples is 1.561\n",
            "2024-01-14 10:24:43,679 - For batch 600 loss at 500 samples is 1.567703125\n",
            "2024-01-14 10:24:44,525 - For batch 700 loss at 500 samples is 1.553234375\n",
            "2024-01-14 10:24:45,385 - For batch 800 loss at 500 samples is 1.55284375\n",
            "2024-01-14 10:24:46,298 - For batch 900 loss at 500 samples is 1.5610703125\n",
            "2024-01-14 10:24:47,195 - For batch 1000 loss at 500 samples is 1.56640625\n",
            "2024-01-14 10:24:47,999 - For batch 1100 loss at 500 samples is 1.5493125\n",
            "2024-01-14 10:24:48,833 - For batch 1200 loss at 500 samples is 1.542671875\n",
            "2024-01-14 10:24:49,637 - For batch 1300 loss at 500 samples is 1.5552734375\n",
            "2024-01-14 10:24:50,492 - For batch 1400 loss at 500 samples is 1.5528125\n",
            "2024-01-14 10:24:51,282 - LOSS train 1.5528125 valid 12.03125\n",
            "2024-01-14 10:24:51,285 - Starting training at 110\n",
            "2024-01-14 10:24:52,572 - For batch 100 loss at 500 samples is 1.567640625\n",
            "2024-01-14 10:24:53,688 - For batch 200 loss at 500 samples is 1.57484375\n",
            "2024-01-14 10:24:54,545 - For batch 300 loss at 500 samples is 1.5694765625\n",
            "2024-01-14 10:24:55,351 - For batch 400 loss at 500 samples is 1.5600546875\n",
            "2024-01-14 10:24:56,127 - For batch 500 loss at 500 samples is 1.56415625\n",
            "2024-01-14 10:24:56,918 - For batch 600 loss at 500 samples is 1.5701328125\n",
            "2024-01-14 10:24:57,740 - For batch 700 loss at 500 samples is 1.553109375\n",
            "2024-01-14 10:24:58,578 - For batch 800 loss at 500 samples is 1.5571484375\n",
            "2024-01-14 10:24:59,291 - For batch 900 loss at 500 samples is 1.5628203125\n",
            "2024-01-14 10:25:00,115 - For batch 1000 loss at 500 samples is 1.5637265625\n",
            "2024-01-14 10:25:00,965 - For batch 1100 loss at 500 samples is 1.5513125\n",
            "2024-01-14 10:25:01,690 - For batch 1200 loss at 500 samples is 1.54896875\n",
            "2024-01-14 10:25:02,579 - For batch 1300 loss at 500 samples is 1.5588671875\n",
            "2024-01-14 10:25:03,399 - For batch 1400 loss at 500 samples is 1.549234375\n",
            "2024-01-14 10:25:04,068 - LOSS train 1.549234375 valid 11.9609375\n",
            "2024-01-14 10:25:04,070 - Starting training at 111\n",
            "2024-01-14 10:25:05,164 - For batch 100 loss at 500 samples is 1.567328125\n",
            "2024-01-14 10:25:06,317 - For batch 200 loss at 500 samples is 1.5725078125\n",
            "2024-01-14 10:25:07,276 - For batch 300 loss at 500 samples is 1.5701875\n",
            "2024-01-14 10:25:08,059 - For batch 400 loss at 500 samples is 1.560984375\n",
            "2024-01-14 10:25:08,874 - For batch 500 loss at 500 samples is 1.55984375\n",
            "2024-01-14 10:25:09,677 - For batch 600 loss at 500 samples is 1.573484375\n",
            "2024-01-14 10:25:10,474 - For batch 700 loss at 500 samples is 1.5546484375\n",
            "2024-01-14 10:25:11,363 - For batch 800 loss at 500 samples is 1.5565859375\n",
            "2024-01-14 10:25:12,149 - For batch 900 loss at 500 samples is 1.5666875\n",
            "2024-01-14 10:25:12,952 - For batch 1000 loss at 500 samples is 1.565515625\n",
            "2024-01-14 10:25:13,885 - For batch 1100 loss at 500 samples is 1.5487109375\n",
            "2024-01-14 10:25:14,666 - For batch 1200 loss at 500 samples is 1.541390625\n",
            "2024-01-14 10:25:15,460 - For batch 1300 loss at 500 samples is 1.5592421875\n",
            "2024-01-14 10:25:16,323 - For batch 1400 loss at 500 samples is 1.554734375\n",
            "2024-01-14 10:25:16,825 - LOSS train 1.554734375 valid 11.7890625\n",
            "2024-01-14 10:25:16,827 - Starting training at 112\n",
            "2024-01-14 10:25:18,037 - For batch 100 loss at 500 samples is 1.566828125\n",
            "2024-01-14 10:25:19,248 - For batch 200 loss at 500 samples is 1.574484375\n",
            "2024-01-14 10:25:20,352 - For batch 300 loss at 500 samples is 1.571125\n",
            "2024-01-14 10:25:21,168 - For batch 400 loss at 500 samples is 1.5616796875\n",
            "2024-01-14 10:25:21,965 - For batch 500 loss at 500 samples is 1.563265625\n",
            "2024-01-14 10:25:22,819 - For batch 600 loss at 500 samples is 1.5741484375\n",
            "2024-01-14 10:25:23,724 - For batch 700 loss at 500 samples is 1.5532421875\n",
            "2024-01-14 10:25:24,525 - For batch 800 loss at 500 samples is 1.5558203125\n",
            "2024-01-14 10:25:25,371 - For batch 900 loss at 500 samples is 1.5625859375\n",
            "2024-01-14 10:25:26,140 - For batch 1000 loss at 500 samples is 1.568171875\n",
            "2024-01-14 10:25:26,870 - For batch 1100 loss at 500 samples is 1.5482578125\n",
            "2024-01-14 10:25:27,746 - For batch 1200 loss at 500 samples is 1.5395078125\n",
            "2024-01-14 10:25:28,548 - For batch 1300 loss at 500 samples is 1.559515625\n",
            "2024-01-14 10:25:29,367 - For batch 1400 loss at 500 samples is 1.5581328125\n",
            "2024-01-14 10:25:29,961 - LOSS train 1.5581328125 valid 11.7265625\n",
            "2024-01-14 10:25:29,963 - Starting training at 113\n",
            "2024-01-14 10:25:30,943 - For batch 100 loss at 500 samples is 1.567078125\n",
            "2024-01-14 10:25:32,091 - For batch 200 loss at 500 samples is 1.572609375\n",
            "2024-01-14 10:25:33,239 - For batch 300 loss at 500 samples is 1.571828125\n",
            "2024-01-14 10:25:34,198 - For batch 400 loss at 500 samples is 1.563484375\n",
            "2024-01-14 10:25:35,003 - For batch 500 loss at 500 samples is 1.5653046875\n",
            "2024-01-14 10:25:35,798 - For batch 600 loss at 500 samples is 1.5750703125\n",
            "2024-01-14 10:25:36,591 - For batch 700 loss at 500 samples is 1.5579140625\n",
            "2024-01-14 10:25:37,446 - For batch 800 loss at 500 samples is 1.5531875\n",
            "2024-01-14 10:25:38,286 - For batch 900 loss at 500 samples is 1.563828125\n",
            "2024-01-14 10:25:39,170 - For batch 1000 loss at 500 samples is 1.56665625\n",
            "2024-01-14 10:25:40,006 - For batch 1100 loss at 500 samples is 1.549875\n",
            "2024-01-14 10:25:40,918 - For batch 1200 loss at 500 samples is 1.5444453125\n",
            "2024-01-14 10:25:41,688 - For batch 1300 loss at 500 samples is 1.5590546875\n",
            "2024-01-14 10:25:42,582 - For batch 1400 loss at 500 samples is 1.5511328125\n",
            "2024-01-14 10:25:43,161 - LOSS train 1.5511328125 valid 11.5625\n",
            "2024-01-14 10:25:43,165 - Starting training at 114\n",
            "2024-01-14 10:25:44,246 - For batch 100 loss at 500 samples is 1.5673984375\n",
            "2024-01-14 10:25:45,424 - For batch 200 loss at 500 samples is 1.572125\n",
            "2024-01-14 10:25:46,581 - For batch 300 loss at 500 samples is 1.571484375\n",
            "2024-01-14 10:25:47,476 - For batch 400 loss at 500 samples is 1.565390625\n",
            "2024-01-14 10:25:48,318 - For batch 500 loss at 500 samples is 1.5684140625\n",
            "2024-01-14 10:25:49,072 - For batch 600 loss at 500 samples is 1.5751640625\n",
            "2024-01-14 10:25:49,857 - For batch 700 loss at 500 samples is 1.5554921875\n",
            "2024-01-14 10:25:50,552 - For batch 800 loss at 500 samples is 1.557203125\n",
            "2024-01-14 10:25:51,369 - For batch 900 loss at 500 samples is 1.5655859375\n",
            "2024-01-14 10:25:52,216 - For batch 1000 loss at 500 samples is 1.5671484375\n",
            "2024-01-14 10:25:53,072 - For batch 1100 loss at 500 samples is 1.55059375\n",
            "2024-01-14 10:25:53,857 - For batch 1200 loss at 500 samples is 1.545640625\n",
            "2024-01-14 10:25:54,595 - For batch 1300 loss at 500 samples is 1.5604609375\n",
            "2024-01-14 10:25:55,455 - For batch 1400 loss at 500 samples is 1.5541875\n",
            "2024-01-14 10:25:55,907 - LOSS train 1.5541875 valid 11.4921875\n",
            "2024-01-14 10:25:55,909 - Starting training at 115\n",
            "2024-01-14 10:25:56,783 - For batch 100 loss at 500 samples is 1.5683671875\n",
            "2024-01-14 10:25:57,913 - For batch 200 loss at 500 samples is 1.5736484375\n",
            "2024-01-14 10:25:59,081 - For batch 300 loss at 500 samples is 1.57228125\n",
            "2024-01-14 10:26:00,164 - For batch 400 loss at 500 samples is 1.5637578125\n",
            "2024-01-14 10:26:01,063 - For batch 500 loss at 500 samples is 1.566578125\n",
            "2024-01-14 10:26:01,930 - For batch 600 loss at 500 samples is 1.5734453125\n",
            "2024-01-14 10:26:02,796 - For batch 700 loss at 500 samples is 1.55540625\n",
            "2024-01-14 10:26:03,635 - For batch 800 loss at 500 samples is 1.5595703125\n",
            "2024-01-14 10:26:04,512 - For batch 900 loss at 500 samples is 1.562921875\n",
            "2024-01-14 10:26:05,384 - For batch 1000 loss at 500 samples is 1.5699453125\n",
            "2024-01-14 10:26:06,205 - For batch 1100 loss at 500 samples is 1.5496171875\n",
            "2024-01-14 10:26:06,976 - For batch 1200 loss at 500 samples is 1.550015625\n",
            "2024-01-14 10:26:07,707 - For batch 1300 loss at 500 samples is 1.5607109375\n",
            "2024-01-14 10:26:08,610 - For batch 1400 loss at 500 samples is 1.5535\n",
            "2024-01-14 10:26:09,174 - LOSS train 1.5535 valid 11.3828125\n",
            "2024-01-14 10:26:09,176 - Starting training at 116\n",
            "2024-01-14 10:26:10,031 - For batch 100 loss at 500 samples is 1.5688125\n",
            "2024-01-14 10:26:11,231 - For batch 200 loss at 500 samples is 1.5743671875\n",
            "2024-01-14 10:26:12,428 - For batch 300 loss at 500 samples is 1.57253125\n",
            "2024-01-14 10:26:13,512 - For batch 400 loss at 500 samples is 1.560765625\n",
            "2024-01-14 10:26:14,295 - For batch 500 loss at 500 samples is 1.5695078125\n",
            "2024-01-14 10:26:15,149 - For batch 600 loss at 500 samples is 1.574234375\n",
            "2024-01-14 10:26:15,973 - For batch 700 loss at 500 samples is 1.55884375\n",
            "2024-01-14 10:26:16,802 - For batch 800 loss at 500 samples is 1.5587265625\n",
            "2024-01-14 10:26:17,630 - For batch 900 loss at 500 samples is 1.5637109375\n",
            "2024-01-14 10:26:18,445 - For batch 1000 loss at 500 samples is 1.5676015625\n",
            "2024-01-14 10:26:19,287 - For batch 1100 loss at 500 samples is 1.5503203125\n",
            "2024-01-14 10:26:20,081 - For batch 1200 loss at 500 samples is 1.5401875\n",
            "2024-01-14 10:26:20,935 - For batch 1300 loss at 500 samples is 1.5617109375\n",
            "2024-01-14 10:26:21,805 - For batch 1400 loss at 500 samples is 1.5507578125\n",
            "2024-01-14 10:26:22,369 - LOSS train 1.5507578125 valid 11.3203125\n",
            "2024-01-14 10:26:22,372 - Starting training at 117\n",
            "2024-01-14 10:26:23,101 - For batch 100 loss at 500 samples is 1.5702578125\n",
            "2024-01-14 10:26:24,357 - For batch 200 loss at 500 samples is 1.574703125\n",
            "2024-01-14 10:26:25,685 - For batch 300 loss at 500 samples is 1.5755078125\n",
            "2024-01-14 10:26:26,620 - For batch 400 loss at 500 samples is 1.5609765625\n",
            "2024-01-14 10:26:27,364 - For batch 500 loss at 500 samples is 1.565421875\n",
            "2024-01-14 10:26:28,209 - For batch 600 loss at 500 samples is 1.5735078125\n",
            "2024-01-14 10:26:28,976 - For batch 700 loss at 500 samples is 1.5555625\n",
            "2024-01-14 10:26:29,771 - For batch 800 loss at 500 samples is 1.556578125\n",
            "2024-01-14 10:26:30,586 - For batch 900 loss at 500 samples is 1.571375\n",
            "2024-01-14 10:26:31,471 - For batch 1000 loss at 500 samples is 1.5682421875\n",
            "2024-01-14 10:26:32,137 - For batch 1100 loss at 500 samples is 1.5540625\n",
            "2024-01-14 10:26:32,944 - For batch 1200 loss at 500 samples is 1.5447109375\n",
            "2024-01-14 10:26:33,742 - For batch 1300 loss at 500 samples is 1.5608671875\n",
            "2024-01-14 10:26:34,657 - For batch 1400 loss at 500 samples is 1.5521953125\n",
            "2024-01-14 10:26:35,141 - LOSS train 1.5521953125 valid 11.15625\n",
            "2024-01-14 10:26:35,143 - Starting training at 118\n",
            "2024-01-14 10:26:35,968 - For batch 100 loss at 500 samples is 1.568734375\n",
            "2024-01-14 10:26:37,130 - For batch 200 loss at 500 samples is 1.5749140625\n",
            "2024-01-14 10:26:38,365 - For batch 300 loss at 500 samples is 1.5738828125\n",
            "2024-01-14 10:26:39,516 - For batch 400 loss at 500 samples is 1.562796875\n",
            "2024-01-14 10:26:40,200 - For batch 500 loss at 500 samples is 1.567421875\n",
            "2024-01-14 10:26:41,059 - For batch 600 loss at 500 samples is 1.57365625\n",
            "2024-01-14 10:26:41,792 - For batch 700 loss at 500 samples is 1.555265625\n",
            "2024-01-14 10:26:42,591 - For batch 800 loss at 500 samples is 1.5566328125\n",
            "2024-01-14 10:26:43,394 - For batch 900 loss at 500 samples is 1.563640625\n",
            "2024-01-14 10:26:44,231 - For batch 1000 loss at 500 samples is 1.56821875\n",
            "2024-01-14 10:26:45,070 - For batch 1100 loss at 500 samples is 1.553453125\n",
            "2024-01-14 10:26:45,873 - For batch 1200 loss at 500 samples is 1.5454765625\n",
            "2024-01-14 10:26:46,735 - For batch 1300 loss at 500 samples is 1.5607578125\n",
            "2024-01-14 10:26:47,648 - For batch 1400 loss at 500 samples is 1.5487109375\n",
            "2024-01-14 10:26:48,273 - LOSS train 1.5487109375 valid 11.1015625\n",
            "2024-01-14 10:26:48,275 - Starting training at 119\n",
            "2024-01-14 10:26:49,064 - For batch 100 loss at 500 samples is 1.5697890625\n",
            "2024-01-14 10:26:49,841 - For batch 200 loss at 500 samples is 1.5760390625\n",
            "2024-01-14 10:26:50,938 - For batch 300 loss at 500 samples is 1.5738984375\n",
            "2024-01-14 10:26:52,066 - For batch 400 loss at 500 samples is 1.56775\n",
            "2024-01-14 10:26:53,128 - For batch 500 loss at 500 samples is 1.564828125\n",
            "2024-01-14 10:26:53,956 - For batch 600 loss at 500 samples is 1.573953125\n",
            "2024-01-14 10:26:54,793 - For batch 700 loss at 500 samples is 1.5546640625\n",
            "2024-01-14 10:26:55,561 - For batch 800 loss at 500 samples is 1.5558515625\n",
            "2024-01-14 10:26:56,346 - For batch 900 loss at 500 samples is 1.5664453125\n",
            "2024-01-14 10:26:57,152 - For batch 1000 loss at 500 samples is 1.5659609375\n",
            "2024-01-14 10:26:58,046 - For batch 1100 loss at 500 samples is 1.554140625\n",
            "2024-01-14 10:26:58,895 - For batch 1200 loss at 500 samples is 1.5514140625\n",
            "2024-01-14 10:26:59,696 - For batch 1300 loss at 500 samples is 1.560078125\n",
            "2024-01-14 10:27:00,485 - For batch 1400 loss at 500 samples is 1.5553671875\n",
            "2024-01-14 10:27:01,042 - LOSS train 1.5553671875 valid 10.984375\n",
            "2024-01-14 10:27:01,050 - Starting training at 120\n",
            "2024-01-14 10:27:01,923 - For batch 100 loss at 500 samples is 1.5712890625\n",
            "2024-01-14 10:27:02,741 - For batch 200 loss at 500 samples is 1.574046875\n",
            "2024-01-14 10:27:03,906 - For batch 300 loss at 500 samples is 1.57365625\n",
            "2024-01-14 10:27:05,053 - For batch 400 loss at 500 samples is 1.5627421875\n",
            "2024-01-14 10:27:06,104 - For batch 500 loss at 500 samples is 1.567546875\n",
            "2024-01-14 10:27:06,857 - For batch 600 loss at 500 samples is 1.5754296875\n",
            "2024-01-14 10:27:07,677 - For batch 700 loss at 500 samples is 1.5556015625\n",
            "2024-01-14 10:27:08,421 - For batch 800 loss at 500 samples is 1.556125\n",
            "2024-01-14 10:27:09,153 - For batch 900 loss at 500 samples is 1.56615625\n",
            "2024-01-14 10:27:09,924 - For batch 1000 loss at 500 samples is 1.566609375\n",
            "2024-01-14 10:27:10,655 - For batch 1100 loss at 500 samples is 1.55284375\n",
            "2024-01-14 10:27:11,413 - For batch 1200 loss at 500 samples is 1.5491328125\n",
            "2024-01-14 10:27:12,270 - For batch 1300 loss at 500 samples is 1.561015625\n",
            "2024-01-14 10:27:13,001 - For batch 1400 loss at 500 samples is 1.55484375\n",
            "2024-01-14 10:27:13,580 - LOSS train 1.55484375 valid 10.9765625\n",
            "2024-01-14 10:27:13,589 - Starting training at 121\n",
            "2024-01-14 10:27:14,312 - For batch 100 loss at 500 samples is 1.570953125\n",
            "2024-01-14 10:27:15,096 - For batch 200 loss at 500 samples is 1.5760625\n",
            "2024-01-14 10:27:15,908 - For batch 300 loss at 500 samples is 1.5726875\n",
            "2024-01-14 10:27:17,006 - For batch 400 loss at 500 samples is 1.56421875\n",
            "2024-01-14 10:27:18,220 - For batch 500 loss at 500 samples is 1.568609375\n",
            "2024-01-14 10:27:19,335 - For batch 600 loss at 500 samples is 1.57625\n",
            "2024-01-14 10:27:20,032 - For batch 700 loss at 500 samples is 1.5546796875\n",
            "2024-01-14 10:27:20,875 - For batch 800 loss at 500 samples is 1.5564375\n",
            "2024-01-14 10:27:21,593 - For batch 900 loss at 500 samples is 1.5643125\n",
            "2024-01-14 10:27:22,356 - For batch 1000 loss at 500 samples is 1.5691953125\n",
            "2024-01-14 10:27:23,161 - For batch 1100 loss at 500 samples is 1.5558359375\n",
            "2024-01-14 10:27:23,917 - For batch 1200 loss at 500 samples is 1.5488359375\n",
            "2024-01-14 10:27:24,641 - For batch 1300 loss at 500 samples is 1.561546875\n",
            "2024-01-14 10:27:25,558 - For batch 1400 loss at 500 samples is 1.552796875\n",
            "2024-01-14 10:27:26,109 - LOSS train 1.552796875 valid 10.828125\n",
            "2024-01-14 10:27:26,113 - Starting training at 122\n",
            "2024-01-14 10:27:26,887 - For batch 100 loss at 500 samples is 1.570890625\n",
            "2024-01-14 10:27:27,736 - For batch 200 loss at 500 samples is 1.5746328125\n",
            "2024-01-14 10:27:28,520 - For batch 300 loss at 500 samples is 1.5750859375\n",
            "2024-01-14 10:27:29,387 - For batch 400 loss at 500 samples is 1.566109375\n",
            "2024-01-14 10:27:30,528 - For batch 500 loss at 500 samples is 1.5702890625\n",
            "2024-01-14 10:27:31,604 - For batch 600 loss at 500 samples is 1.5757734375\n",
            "2024-01-14 10:27:32,717 - For batch 700 loss at 500 samples is 1.5563125\n",
            "2024-01-14 10:27:33,564 - For batch 800 loss at 500 samples is 1.55840625\n",
            "2024-01-14 10:27:34,404 - For batch 900 loss at 500 samples is 1.5648984375\n",
            "2024-01-14 10:27:35,082 - For batch 1000 loss at 500 samples is 1.5679296875\n",
            "2024-01-14 10:27:35,868 - For batch 1100 loss at 500 samples is 1.5553671875\n",
            "2024-01-14 10:27:36,721 - For batch 1200 loss at 500 samples is 1.5501875\n",
            "2024-01-14 10:27:37,452 - For batch 1300 loss at 500 samples is 1.56259375\n",
            "2024-01-14 10:27:38,178 - For batch 1400 loss at 500 samples is 1.55525\n",
            "2024-01-14 10:27:38,776 - LOSS train 1.55525 valid 10.71875\n",
            "2024-01-14 10:27:38,778 - Starting training at 123\n",
            "2024-01-14 10:27:39,540 - For batch 100 loss at 500 samples is 1.5710625\n",
            "2024-01-14 10:27:40,344 - For batch 200 loss at 500 samples is 1.575671875\n",
            "2024-01-14 10:27:41,121 - For batch 300 loss at 500 samples is 1.5755234375\n",
            "2024-01-14 10:27:41,974 - For batch 400 loss at 500 samples is 1.5647265625\n",
            "2024-01-14 10:27:42,914 - For batch 500 loss at 500 samples is 1.5673203125\n",
            "2024-01-14 10:27:44,117 - For batch 600 loss at 500 samples is 1.575921875\n",
            "2024-01-14 10:27:45,283 - For batch 700 loss at 500 samples is 1.5620703125\n",
            "2024-01-14 10:27:46,246 - For batch 800 loss at 500 samples is 1.5589609375\n",
            "2024-01-14 10:27:46,979 - For batch 900 loss at 500 samples is 1.56890625\n",
            "2024-01-14 10:27:47,640 - For batch 1000 loss at 500 samples is 1.568859375\n",
            "2024-01-14 10:27:48,543 - For batch 1100 loss at 500 samples is 1.554453125\n",
            "2024-01-14 10:27:49,420 - For batch 1200 loss at 500 samples is 1.5496015625\n",
            "2024-01-14 10:27:50,245 - For batch 1300 loss at 500 samples is 1.56315625\n",
            "2024-01-14 10:27:51,095 - For batch 1400 loss at 500 samples is 1.558421875\n",
            "2024-01-14 10:27:51,644 - LOSS train 1.558421875 valid 10.6484375\n",
            "2024-01-14 10:27:51,646 - Starting training at 124\n",
            "2024-01-14 10:27:52,352 - For batch 100 loss at 500 samples is 1.5704140625\n",
            "2024-01-14 10:27:53,027 - For batch 200 loss at 500 samples is 1.5765546875\n",
            "2024-01-14 10:27:53,764 - For batch 300 loss at 500 samples is 1.57615625\n",
            "2024-01-14 10:27:54,568 - For batch 400 loss at 500 samples is 1.5664453125\n",
            "2024-01-14 10:27:55,369 - For batch 500 loss at 500 samples is 1.5684609375\n",
            "2024-01-14 10:27:56,373 - For batch 600 loss at 500 samples is 1.5761484375\n",
            "2024-01-14 10:27:57,480 - For batch 700 loss at 500 samples is 1.5570703125\n",
            "2024-01-14 10:27:58,423 - For batch 800 loss at 500 samples is 1.562015625\n",
            "2024-01-14 10:27:59,422 - For batch 900 loss at 500 samples is 1.5650703125\n",
            "2024-01-14 10:28:00,195 - For batch 1000 loss at 500 samples is 1.5704609375\n",
            "2024-01-14 10:28:00,955 - For batch 1100 loss at 500 samples is 1.55334375\n",
            "2024-01-14 10:28:01,747 - For batch 1200 loss at 500 samples is 1.550515625\n",
            "2024-01-14 10:28:02,517 - For batch 1300 loss at 500 samples is 1.5635234375\n",
            "2024-01-14 10:28:03,316 - For batch 1400 loss at 500 samples is 1.5596875\n",
            "2024-01-14 10:28:03,740 - LOSS train 1.5596875 valid 10.53125\n",
            "2024-01-14 10:28:03,742 - Starting training at 125\n",
            "2024-01-14 10:28:04,608 - For batch 100 loss at 500 samples is 1.5714140625\n",
            "2024-01-14 10:28:05,341 - For batch 200 loss at 500 samples is 1.5762734375\n",
            "2024-01-14 10:28:06,140 - For batch 300 loss at 500 samples is 1.574328125\n",
            "2024-01-14 10:28:06,902 - For batch 400 loss at 500 samples is 1.5663359375\n",
            "2024-01-14 10:28:07,691 - For batch 500 loss at 500 samples is 1.570625\n",
            "2024-01-14 10:28:08,429 - For batch 600 loss at 500 samples is 1.5765078125\n",
            "2024-01-14 10:28:09,440 - For batch 700 loss at 500 samples is 1.5594765625\n",
            "2024-01-14 10:28:10,561 - For batch 800 loss at 500 samples is 1.5577890625\n",
            "2024-01-14 10:28:11,630 - For batch 900 loss at 500 samples is 1.56603125\n",
            "2024-01-14 10:28:12,594 - For batch 1000 loss at 500 samples is 1.5689375\n",
            "2024-01-14 10:28:13,419 - For batch 1100 loss at 500 samples is 1.56078125\n",
            "2024-01-14 10:28:14,180 - For batch 1200 loss at 500 samples is 1.55459375\n",
            "2024-01-14 10:28:15,021 - For batch 1300 loss at 500 samples is 1.563203125\n",
            "2024-01-14 10:28:15,884 - For batch 1400 loss at 500 samples is 1.5603671875\n",
            "2024-01-14 10:28:16,447 - LOSS train 1.5603671875 valid 10.5234375\n",
            "2024-01-14 10:28:16,449 - Starting training at 126\n",
            "2024-01-14 10:28:17,238 - For batch 100 loss at 500 samples is 1.5706328125\n",
            "2024-01-14 10:28:18,070 - For batch 200 loss at 500 samples is 1.57675\n",
            "2024-01-14 10:28:18,858 - For batch 300 loss at 500 samples is 1.5761171875\n",
            "2024-01-14 10:28:19,663 - For batch 400 loss at 500 samples is 1.5708828125\n",
            "2024-01-14 10:28:20,408 - For batch 500 loss at 500 samples is 1.5709609375\n",
            "2024-01-14 10:28:21,210 - For batch 600 loss at 500 samples is 1.57759375\n",
            "2024-01-14 10:28:21,963 - For batch 700 loss at 500 samples is 1.562609375\n",
            "2024-01-14 10:28:23,098 - For batch 800 loss at 500 samples is 1.559078125\n",
            "2024-01-14 10:28:24,234 - For batch 900 loss at 500 samples is 1.565765625\n",
            "2024-01-14 10:28:25,366 - For batch 1000 loss at 500 samples is 1.56715625\n",
            "2024-01-14 10:28:26,164 - For batch 1100 loss at 500 samples is 1.5521484375\n",
            "2024-01-14 10:28:26,927 - For batch 1200 loss at 500 samples is 1.5520703125\n",
            "2024-01-14 10:28:27,636 - For batch 1300 loss at 500 samples is 1.564390625\n",
            "2024-01-14 10:28:28,454 - For batch 1400 loss at 500 samples is 1.5577109375\n",
            "2024-01-14 10:28:28,960 - LOSS train 1.5577109375 valid 10.390625\n",
            "2024-01-14 10:28:28,963 - Starting training at 127\n",
            "2024-01-14 10:28:29,713 - For batch 100 loss at 500 samples is 1.5699375\n",
            "2024-01-14 10:28:30,546 - For batch 200 loss at 500 samples is 1.57671875\n",
            "2024-01-14 10:28:31,395 - For batch 300 loss at 500 samples is 1.575921875\n",
            "2024-01-14 10:28:32,162 - For batch 400 loss at 500 samples is 1.56478125\n",
            "2024-01-14 10:28:32,957 - For batch 500 loss at 500 samples is 1.567765625\n",
            "2024-01-14 10:28:33,799 - For batch 600 loss at 500 samples is 1.577765625\n",
            "2024-01-14 10:28:34,566 - For batch 700 loss at 500 samples is 1.5600625\n",
            "2024-01-14 10:28:35,536 - For batch 800 loss at 500 samples is 1.558859375\n",
            "2024-01-14 10:28:36,669 - For batch 900 loss at 500 samples is 1.5703046875\n",
            "2024-01-14 10:28:37,788 - For batch 1000 loss at 500 samples is 1.5696015625\n",
            "2024-01-14 10:28:38,690 - For batch 1100 loss at 500 samples is 1.5550703125\n",
            "2024-01-14 10:28:39,473 - For batch 1200 loss at 500 samples is 1.5472265625\n",
            "2024-01-14 10:28:40,311 - For batch 1300 loss at 500 samples is 1.5633828125\n",
            "2024-01-14 10:28:41,211 - For batch 1400 loss at 500 samples is 1.5643828125\n",
            "2024-01-14 10:28:41,750 - LOSS train 1.5643828125 valid 10.2890625\n",
            "2024-01-14 10:28:41,752 - Starting training at 128\n",
            "2024-01-14 10:28:42,452 - For batch 100 loss at 500 samples is 1.5711328125\n",
            "2024-01-14 10:28:43,211 - For batch 200 loss at 500 samples is 1.5782734375\n",
            "2024-01-14 10:28:43,947 - For batch 300 loss at 500 samples is 1.5771328125\n",
            "2024-01-14 10:28:44,770 - For batch 400 loss at 500 samples is 1.568453125\n",
            "2024-01-14 10:28:45,524 - For batch 500 loss at 500 samples is 1.5685546875\n",
            "2024-01-14 10:28:46,371 - For batch 600 loss at 500 samples is 1.577328125\n",
            "2024-01-14 10:28:47,118 - For batch 700 loss at 500 samples is 1.557359375\n",
            "2024-01-14 10:28:47,934 - For batch 800 loss at 500 samples is 1.56328125\n",
            "2024-01-14 10:28:48,811 - For batch 900 loss at 500 samples is 1.5688203125\n",
            "2024-01-14 10:28:49,952 - For batch 1000 loss at 500 samples is 1.5692734375\n",
            "2024-01-14 10:28:50,963 - For batch 1100 loss at 500 samples is 1.557109375\n",
            "2024-01-14 10:28:51,937 - For batch 1200 loss at 500 samples is 1.549890625\n",
            "2024-01-14 10:28:52,803 - For batch 1300 loss at 500 samples is 1.564265625\n",
            "2024-01-14 10:28:53,642 - For batch 1400 loss at 500 samples is 1.5637578125\n",
            "2024-01-14 10:28:54,175 - LOSS train 1.5637578125 valid 10.2109375\n",
            "2024-01-14 10:28:54,178 - Starting training at 129\n",
            "2024-01-14 10:28:54,890 - For batch 100 loss at 500 samples is 1.5712890625\n",
            "2024-01-14 10:28:55,598 - For batch 200 loss at 500 samples is 1.57725\n",
            "2024-01-14 10:28:56,313 - For batch 300 loss at 500 samples is 1.5749453125\n",
            "2024-01-14 10:28:57,055 - For batch 400 loss at 500 samples is 1.56509375\n",
            "2024-01-14 10:28:57,835 - For batch 500 loss at 500 samples is 1.5697890625\n",
            "2024-01-14 10:28:58,529 - For batch 600 loss at 500 samples is 1.5785546875\n",
            "2024-01-14 10:28:59,354 - For batch 700 loss at 500 samples is 1.562125\n",
            "2024-01-14 10:29:00,129 - For batch 800 loss at 500 samples is 1.5610703125\n",
            "2024-01-14 10:29:00,904 - For batch 900 loss at 500 samples is 1.5665546875\n",
            "2024-01-14 10:29:01,744 - For batch 1000 loss at 500 samples is 1.570359375\n",
            "2024-01-14 10:29:02,836 - For batch 1100 loss at 500 samples is 1.5555703125\n",
            "2024-01-14 10:29:04,139 - For batch 1200 loss at 500 samples is 1.5501328125\n",
            "2024-01-14 10:29:05,047 - For batch 1300 loss at 500 samples is 1.56384375\n",
            "2024-01-14 10:29:05,853 - For batch 1400 loss at 500 samples is 1.56134375\n",
            "2024-01-14 10:29:06,416 - LOSS train 1.56134375 valid 10.1328125\n",
            "2024-01-14 10:29:06,419 - Starting training at 130\n",
            "2024-01-14 10:29:07,087 - For batch 100 loss at 500 samples is 1.570859375\n",
            "2024-01-14 10:29:07,923 - For batch 200 loss at 500 samples is 1.5770703125\n",
            "2024-01-14 10:29:08,658 - For batch 300 loss at 500 samples is 1.5762890625\n",
            "2024-01-14 10:29:09,408 - For batch 400 loss at 500 samples is 1.566609375\n",
            "2024-01-14 10:29:10,107 - For batch 500 loss at 500 samples is 1.572453125\n",
            "2024-01-14 10:29:10,829 - For batch 600 loss at 500 samples is 1.5788828125\n",
            "2024-01-14 10:29:11,669 - For batch 700 loss at 500 samples is 1.562703125\n",
            "2024-01-14 10:29:12,349 - For batch 800 loss at 500 samples is 1.563625\n",
            "2024-01-14 10:29:13,142 - For batch 900 loss at 500 samples is 1.566875\n",
            "2024-01-14 10:29:13,887 - For batch 1000 loss at 500 samples is 1.571796875\n",
            "2024-01-14 10:29:14,644 - For batch 1100 loss at 500 samples is 1.5573515625\n",
            "2024-01-14 10:29:15,619 - For batch 1200 loss at 500 samples is 1.5516484375\n",
            "2024-01-14 10:29:16,875 - For batch 1300 loss at 500 samples is 1.5651484375\n",
            "2024-01-14 10:29:17,941 - For batch 1400 loss at 500 samples is 1.5622421875\n",
            "2024-01-14 10:29:18,514 - LOSS train 1.5622421875 valid 10.03125\n",
            "2024-01-14 10:29:18,518 - Starting training at 131\n",
            "2024-01-14 10:29:19,303 - For batch 100 loss at 500 samples is 1.571359375\n",
            "2024-01-14 10:29:20,009 - For batch 200 loss at 500 samples is 1.5767578125\n",
            "2024-01-14 10:29:20,844 - For batch 300 loss at 500 samples is 1.5745390625\n",
            "2024-01-14 10:29:21,618 - For batch 400 loss at 500 samples is 1.5660703125\n",
            "2024-01-14 10:29:22,428 - For batch 500 loss at 500 samples is 1.5677109375\n",
            "2024-01-14 10:29:23,167 - For batch 600 loss at 500 samples is 1.5786875\n",
            "2024-01-14 10:29:23,936 - For batch 700 loss at 500 samples is 1.56340625\n",
            "2024-01-14 10:29:24,811 - For batch 800 loss at 500 samples is 1.563015625\n",
            "2024-01-14 10:29:25,550 - For batch 900 loss at 500 samples is 1.5718203125\n",
            "2024-01-14 10:29:26,315 - For batch 1000 loss at 500 samples is 1.57109375\n",
            "2024-01-14 10:29:27,052 - For batch 1100 loss at 500 samples is 1.5580546875\n",
            "2024-01-14 10:29:27,831 - For batch 1200 loss at 500 samples is 1.5505703125\n",
            "2024-01-14 10:29:28,900 - For batch 1300 loss at 500 samples is 1.5639765625\n",
            "2024-01-14 10:29:30,239 - For batch 1400 loss at 500 samples is 1.566046875\n",
            "2024-01-14 10:29:30,906 - LOSS train 1.566046875 valid 10.03125\n",
            "2024-01-14 10:29:30,911 - Starting training at 132\n",
            "2024-01-14 10:29:31,665 - For batch 100 loss at 500 samples is 1.570859375\n",
            "2024-01-14 10:29:32,352 - For batch 200 loss at 500 samples is 1.579359375\n",
            "2024-01-14 10:29:33,034 - For batch 300 loss at 500 samples is 1.576359375\n",
            "2024-01-14 10:29:33,822 - For batch 400 loss at 500 samples is 1.57025\n",
            "2024-01-14 10:29:34,560 - For batch 500 loss at 500 samples is 1.566984375\n",
            "2024-01-14 10:29:35,337 - For batch 600 loss at 500 samples is 1.578515625\n",
            "2024-01-14 10:29:36,098 - For batch 700 loss at 500 samples is 1.5562421875\n",
            "2024-01-14 10:29:36,784 - For batch 800 loss at 500 samples is 1.56428125\n",
            "2024-01-14 10:29:37,595 - For batch 900 loss at 500 samples is 1.567\n",
            "2024-01-14 10:29:38,248 - For batch 1000 loss at 500 samples is 1.571484375\n",
            "2024-01-14 10:29:39,035 - For batch 1100 loss at 500 samples is 1.5535234375\n",
            "2024-01-14 10:29:39,750 - For batch 1200 loss at 500 samples is 1.55009375\n",
            "2024-01-14 10:29:40,444 - For batch 1300 loss at 500 samples is 1.5651796875\n",
            "2024-01-14 10:29:41,125 - For batch 1400 loss at 500 samples is 1.557421875\n",
            "2024-01-14 10:29:41,920 - LOSS train 1.557421875 valid 9.8984375\n",
            "2024-01-14 10:29:41,922 - Starting training at 133\n",
            "2024-01-14 10:29:43,052 - For batch 100 loss at 500 samples is 1.5716015625\n",
            "2024-01-14 10:29:44,099 - For batch 200 loss at 500 samples is 1.57828125\n",
            "2024-01-14 10:29:44,877 - For batch 300 loss at 500 samples is 1.5758359375\n",
            "2024-01-14 10:29:45,627 - For batch 400 loss at 500 samples is 1.566640625\n",
            "2024-01-14 10:29:46,398 - For batch 500 loss at 500 samples is 1.568296875\n",
            "2024-01-14 10:29:47,279 - For batch 600 loss at 500 samples is 1.5788984375\n",
            "2024-01-14 10:29:48,119 - For batch 700 loss at 500 samples is 1.5598515625\n",
            "2024-01-14 10:29:48,872 - For batch 800 loss at 500 samples is 1.5608046875\n",
            "2024-01-14 10:29:49,604 - For batch 900 loss at 500 samples is 1.5680859375\n",
            "2024-01-14 10:29:50,328 - For batch 1000 loss at 500 samples is 1.570515625\n",
            "2024-01-14 10:29:51,120 - For batch 1100 loss at 500 samples is 1.5562421875\n",
            "2024-01-14 10:29:51,972 - For batch 1200 loss at 500 samples is 1.5527421875\n",
            "2024-01-14 10:29:52,744 - For batch 1300 loss at 500 samples is 1.56509375\n",
            "2024-01-14 10:29:53,501 - For batch 1400 loss at 500 samples is 1.5630078125\n",
            "2024-01-14 10:29:54,050 - LOSS train 1.5630078125 valid 9.8125\n",
            "2024-01-14 10:29:54,052 - Starting training at 134\n",
            "2024-01-14 10:29:55,157 - For batch 100 loss at 500 samples is 1.571796875\n",
            "2024-01-14 10:29:56,322 - For batch 200 loss at 500 samples is 1.579375\n",
            "2024-01-14 10:29:57,533 - For batch 300 loss at 500 samples is 1.5769921875\n",
            "2024-01-14 10:29:58,265 - For batch 400 loss at 500 samples is 1.5666796875\n",
            "2024-01-14 10:29:58,934 - For batch 500 loss at 500 samples is 1.571078125\n",
            "2024-01-14 10:29:59,630 - For batch 600 loss at 500 samples is 1.577515625\n",
            "2024-01-14 10:30:00,363 - For batch 700 loss at 500 samples is 1.559046875\n",
            "2024-01-14 10:30:00,959 - For batch 800 loss at 500 samples is 1.564734375\n",
            "2024-01-14 10:30:01,716 - For batch 900 loss at 500 samples is 1.5725703125\n",
            "2024-01-14 10:30:02,475 - For batch 1000 loss at 500 samples is 1.571265625\n",
            "2024-01-14 10:30:03,169 - For batch 1100 loss at 500 samples is 1.55765625\n",
            "2024-01-14 10:30:03,820 - For batch 1200 loss at 500 samples is 1.5507734375\n",
            "2024-01-14 10:30:04,503 - For batch 1300 loss at 500 samples is 1.5651796875\n",
            "2024-01-14 10:30:05,215 - For batch 1400 loss at 500 samples is 1.5604296875\n",
            "2024-01-14 10:30:05,595 - LOSS train 1.5604296875 valid 9.796875\n",
            "2024-01-14 10:30:05,598 - Starting training at 135\n",
            "2024-01-14 10:30:06,397 - For batch 100 loss at 500 samples is 1.5714921875\n",
            "2024-01-14 10:30:07,092 - For batch 200 loss at 500 samples is 1.5785234375\n",
            "2024-01-14 10:30:08,178 - For batch 300 loss at 500 samples is 1.5764296875\n",
            "2024-01-14 10:30:09,300 - For batch 400 loss at 500 samples is 1.5679921875\n",
            "2024-01-14 10:30:10,481 - For batch 500 loss at 500 samples is 1.5669609375\n",
            "2024-01-14 10:30:11,205 - For batch 600 loss at 500 samples is 1.5776640625\n",
            "2024-01-14 10:30:11,994 - For batch 700 loss at 500 samples is 1.5597109375\n",
            "2024-01-14 10:30:12,784 - For batch 800 loss at 500 samples is 1.5641640625\n",
            "2024-01-14 10:30:13,514 - For batch 900 loss at 500 samples is 1.5729609375\n",
            "2024-01-14 10:30:14,322 - For batch 1000 loss at 500 samples is 1.5705625\n",
            "2024-01-14 10:30:15,084 - For batch 1100 loss at 500 samples is 1.5566015625\n",
            "2024-01-14 10:30:15,819 - For batch 1200 loss at 500 samples is 1.5516953125\n",
            "2024-01-14 10:30:16,542 - For batch 1300 loss at 500 samples is 1.56453125\n",
            "2024-01-14 10:30:17,230 - For batch 1400 loss at 500 samples is 1.56\n",
            "2024-01-14 10:30:17,707 - LOSS train 1.56 valid 9.6640625\n",
            "2024-01-14 10:30:17,709 - Starting training at 136\n",
            "2024-01-14 10:30:18,506 - For batch 100 loss at 500 samples is 1.5704609375\n",
            "2024-01-14 10:30:19,249 - For batch 200 loss at 500 samples is 1.57971875\n",
            "2024-01-14 10:30:20,025 - For batch 300 loss at 500 samples is 1.5804296875\n",
            "2024-01-14 10:30:20,962 - For batch 400 loss at 500 samples is 1.56628125\n",
            "2024-01-14 10:30:22,033 - For batch 500 loss at 500 samples is 1.5658125\n",
            "2024-01-14 10:30:23,319 - For batch 600 loss at 500 samples is 1.5778359375\n",
            "2024-01-14 10:30:24,265 - For batch 700 loss at 500 samples is 1.5593515625\n",
            "2024-01-14 10:30:25,008 - For batch 800 loss at 500 samples is 1.5648125\n",
            "2024-01-14 10:30:25,731 - For batch 900 loss at 500 samples is 1.5738359375\n",
            "2024-01-14 10:30:26,488 - For batch 1000 loss at 500 samples is 1.5722421875\n",
            "2024-01-14 10:30:27,226 - For batch 1100 loss at 500 samples is 1.5543671875\n",
            "2024-01-14 10:30:27,961 - For batch 1200 loss at 500 samples is 1.551171875\n",
            "2024-01-14 10:30:28,716 - For batch 1300 loss at 500 samples is 1.565265625\n",
            "2024-01-14 10:30:29,579 - For batch 1400 loss at 500 samples is 1.55853125\n",
            "2024-01-14 10:30:30,071 - LOSS train 1.55853125 valid 9.6328125\n",
            "2024-01-14 10:30:30,073 - Starting training at 137\n",
            "2024-01-14 10:30:30,686 - For batch 100 loss at 500 samples is 1.57090625\n",
            "2024-01-14 10:30:31,476 - For batch 200 loss at 500 samples is 1.5777578125\n",
            "2024-01-14 10:30:32,238 - For batch 300 loss at 500 samples is 1.577453125\n",
            "2024-01-14 10:30:33,026 - For batch 400 loss at 500 samples is 1.5645625\n",
            "2024-01-14 10:30:33,751 - For batch 500 loss at 500 samples is 1.567921875\n",
            "2024-01-14 10:30:34,914 - For batch 600 loss at 500 samples is 1.578296875\n",
            "2024-01-14 10:30:35,988 - For batch 700 loss at 500 samples is 1.5600234375\n",
            "2024-01-14 10:30:37,081 - For batch 800 loss at 500 samples is 1.5631328125\n",
            "2024-01-14 10:30:37,875 - For batch 900 loss at 500 samples is 1.56815625\n",
            "2024-01-14 10:30:38,621 - For batch 1000 loss at 500 samples is 1.5735234375\n",
            "2024-01-14 10:30:39,365 - For batch 1100 loss at 500 samples is 1.5584609375\n",
            "2024-01-14 10:30:40,026 - For batch 1200 loss at 500 samples is 1.5537421875\n",
            "2024-01-14 10:30:40,888 - For batch 1300 loss at 500 samples is 1.566453125\n",
            "2024-01-14 10:30:41,725 - For batch 1400 loss at 500 samples is 1.5621484375\n",
            "2024-01-14 10:30:42,210 - LOSS train 1.5621484375 valid 9.5390625\n",
            "2024-01-14 10:30:42,212 - Starting training at 138\n",
            "2024-01-14 10:30:42,968 - For batch 100 loss at 500 samples is 1.5711015625\n",
            "2024-01-14 10:30:43,696 - For batch 200 loss at 500 samples is 1.579515625\n",
            "2024-01-14 10:30:44,360 - For batch 300 loss at 500 samples is 1.5793125\n",
            "2024-01-14 10:30:45,089 - For batch 400 loss at 500 samples is 1.567625\n",
            "2024-01-14 10:30:45,794 - For batch 500 loss at 500 samples is 1.568515625\n",
            "2024-01-14 10:30:46,536 - For batch 600 loss at 500 samples is 1.57771875\n",
            "2024-01-14 10:30:47,353 - For batch 700 loss at 500 samples is 1.560953125\n",
            "2024-01-14 10:30:48,590 - For batch 800 loss at 500 samples is 1.5668203125\n",
            "2024-01-14 10:30:49,629 - For batch 900 loss at 500 samples is 1.5688515625\n",
            "2024-01-14 10:30:50,586 - For batch 1000 loss at 500 samples is 1.5735\n",
            "2024-01-14 10:30:51,311 - For batch 1100 loss at 500 samples is 1.5561875\n",
            "2024-01-14 10:30:52,017 - For batch 1200 loss at 500 samples is 1.5542109375\n",
            "2024-01-14 10:30:52,833 - For batch 1300 loss at 500 samples is 1.565375\n",
            "2024-01-14 10:30:53,708 - For batch 1400 loss at 500 samples is 1.5580859375\n",
            "2024-01-14 10:30:54,246 - LOSS train 1.5580859375 valid 9.5234375\n",
            "2024-01-14 10:30:54,249 - Starting training at 139\n",
            "2024-01-14 10:30:54,987 - For batch 100 loss at 500 samples is 1.571203125\n",
            "2024-01-14 10:30:55,696 - For batch 200 loss at 500 samples is 1.579359375\n",
            "2024-01-14 10:30:56,378 - For batch 300 loss at 500 samples is 1.5773671875\n",
            "2024-01-14 10:30:57,126 - For batch 400 loss at 500 samples is 1.565625\n",
            "2024-01-14 10:30:57,855 - For batch 500 loss at 500 samples is 1.570234375\n",
            "2024-01-14 10:30:58,554 - For batch 600 loss at 500 samples is 1.57840625\n",
            "2024-01-14 10:30:59,253 - For batch 700 loss at 500 samples is 1.5600625\n",
            "2024-01-14 10:30:59,996 - For batch 800 loss at 500 samples is 1.56628125\n",
            "2024-01-14 10:31:00,907 - For batch 900 loss at 500 samples is 1.573171875\n",
            "2024-01-14 10:31:02,002 - For batch 1000 loss at 500 samples is 1.57425\n",
            "2024-01-14 10:31:03,138 - For batch 1100 loss at 500 samples is 1.557484375\n",
            "2024-01-14 10:31:04,046 - For batch 1200 loss at 500 samples is 1.5547734375\n",
            "2024-01-14 10:31:04,714 - For batch 1300 loss at 500 samples is 1.566234375\n",
            "2024-01-14 10:31:05,468 - For batch 1400 loss at 500 samples is 1.563265625\n",
            "2024-01-14 10:31:06,025 - LOSS train 1.563265625 valid 9.40625\n",
            "2024-01-14 10:31:06,031 - Starting training at 140\n",
            "2024-01-14 10:31:06,819 - For batch 100 loss at 500 samples is 1.5718515625\n",
            "2024-01-14 10:31:07,592 - For batch 200 loss at 500 samples is 1.5806875\n",
            "2024-01-14 10:31:08,349 - For batch 300 loss at 500 samples is 1.578921875\n",
            "2024-01-14 10:31:09,119 - For batch 400 loss at 500 samples is 1.5660625\n",
            "2024-01-14 10:31:09,871 - For batch 500 loss at 500 samples is 1.5645234375\n",
            "2024-01-14 10:31:10,582 - For batch 600 loss at 500 samples is 1.5783671875\n",
            "2024-01-14 10:31:11,417 - For batch 700 loss at 500 samples is 1.560859375\n",
            "2024-01-14 10:31:12,126 - For batch 800 loss at 500 samples is 1.5667109375\n",
            "2024-01-14 10:31:12,853 - For batch 900 loss at 500 samples is 1.5743203125\n",
            "2024-01-14 10:31:13,550 - For batch 1000 loss at 500 samples is 1.577234375\n",
            "2024-01-14 10:31:14,529 - For batch 1100 loss at 500 samples is 1.562734375\n",
            "2024-01-14 10:31:15,578 - For batch 1200 loss at 500 samples is 1.55171875\n",
            "2024-01-14 10:31:16,652 - For batch 1300 loss at 500 samples is 1.5663359375\n",
            "2024-01-14 10:31:17,381 - For batch 1400 loss at 500 samples is 1.5574453125\n",
            "2024-01-14 10:31:17,917 - LOSS train 1.5574453125 valid 9.34375\n",
            "2024-01-14 10:31:17,923 - Starting training at 141\n",
            "2024-01-14 10:31:18,569 - For batch 100 loss at 500 samples is 1.57171875\n",
            "2024-01-14 10:31:19,230 - For batch 200 loss at 500 samples is 1.5794140625\n",
            "2024-01-14 10:31:20,087 - For batch 300 loss at 500 samples is 1.5771328125\n",
            "2024-01-14 10:31:20,905 - For batch 400 loss at 500 samples is 1.568328125\n",
            "2024-01-14 10:31:21,693 - For batch 500 loss at 500 samples is 1.566640625\n",
            "2024-01-14 10:31:22,398 - For batch 600 loss at 500 samples is 1.579546875\n",
            "2024-01-14 10:31:23,085 - For batch 700 loss at 500 samples is 1.5615859375\n",
            "2024-01-14 10:31:23,870 - For batch 800 loss at 500 samples is 1.56525\n",
            "2024-01-14 10:31:24,615 - For batch 900 loss at 500 samples is 1.56965625\n",
            "2024-01-14 10:31:25,367 - For batch 1000 loss at 500 samples is 1.574\n",
            "2024-01-14 10:31:26,129 - For batch 1100 loss at 500 samples is 1.5600234375\n",
            "2024-01-14 10:31:27,181 - For batch 1200 loss at 500 samples is 1.5548984375\n",
            "2024-01-14 10:31:28,129 - For batch 1300 loss at 500 samples is 1.5669296875\n",
            "2024-01-14 10:31:29,374 - For batch 1400 loss at 500 samples is 1.5618203125\n",
            "2024-01-14 10:31:29,988 - LOSS train 1.5618203125 valid 9.265625\n",
            "2024-01-14 10:31:29,997 - Starting training at 142\n",
            "2024-01-14 10:31:30,848 - For batch 100 loss at 500 samples is 1.57159375\n",
            "2024-01-14 10:31:31,577 - For batch 200 loss at 500 samples is 1.5806328125\n",
            "2024-01-14 10:31:32,268 - For batch 300 loss at 500 samples is 1.5774140625\n",
            "2024-01-14 10:31:32,998 - For batch 400 loss at 500 samples is 1.5666171875\n",
            "2024-01-14 10:31:33,727 - For batch 500 loss at 500 samples is 1.5685234375\n",
            "2024-01-14 10:31:34,432 - For batch 600 loss at 500 samples is 1.5790234375\n",
            "2024-01-14 10:31:35,110 - For batch 700 loss at 500 samples is 1.56184375\n",
            "2024-01-14 10:31:35,897 - For batch 800 loss at 500 samples is 1.567875\n",
            "2024-01-14 10:31:36,673 - For batch 900 loss at 500 samples is 1.5721015625\n",
            "2024-01-14 10:31:37,381 - For batch 1000 loss at 500 samples is 1.574234375\n",
            "2024-01-14 10:31:38,049 - For batch 1100 loss at 500 samples is 1.5584765625\n",
            "2024-01-14 10:31:38,805 - For batch 1200 loss at 500 samples is 1.5567421875\n",
            "2024-01-14 10:31:39,508 - For batch 1300 loss at 500 samples is 1.5666484375\n",
            "2024-01-14 10:31:40,566 - For batch 1400 loss at 500 samples is 1.561046875\n",
            "2024-01-14 10:31:41,312 - LOSS train 1.561046875 valid 9.2578125\n",
            "2024-01-14 10:31:41,314 - Starting training at 143\n",
            "2024-01-14 10:31:42,415 - For batch 100 loss at 500 samples is 1.5719375\n",
            "2024-01-14 10:31:43,406 - For batch 200 loss at 500 samples is 1.5787265625\n",
            "2024-01-14 10:31:44,142 - For batch 300 loss at 500 samples is 1.5775859375\n",
            "2024-01-14 10:31:44,837 - For batch 400 loss at 500 samples is 1.56753125\n",
            "2024-01-14 10:31:45,529 - For batch 500 loss at 500 samples is 1.5723828125\n",
            "2024-01-14 10:31:46,319 - For batch 600 loss at 500 samples is 1.57878125\n",
            "2024-01-14 10:31:47,087 - For batch 700 loss at 500 samples is 1.5607265625\n",
            "2024-01-14 10:31:47,775 - For batch 800 loss at 500 samples is 1.56625\n",
            "2024-01-14 10:31:48,542 - For batch 900 loss at 500 samples is 1.5754375\n",
            "2024-01-14 10:31:49,170 - For batch 1000 loss at 500 samples is 1.573890625\n",
            "2024-01-14 10:31:50,037 - For batch 1100 loss at 500 samples is 1.5625390625\n",
            "2024-01-14 10:31:50,874 - For batch 1200 loss at 500 samples is 1.5561484375\n",
            "2024-01-14 10:31:51,669 - For batch 1300 loss at 500 samples is 1.567296875\n",
            "2024-01-14 10:31:52,440 - For batch 1400 loss at 500 samples is 1.563171875\n",
            "2024-01-14 10:31:52,960 - LOSS train 1.563171875 valid 9.140625\n",
            "2024-01-14 10:31:52,963 - Starting training at 144\n",
            "2024-01-14 10:31:54,163 - For batch 100 loss at 500 samples is 1.5720859375\n",
            "2024-01-14 10:31:55,298 - For batch 200 loss at 500 samples is 1.578875\n",
            "2024-01-14 10:31:56,430 - For batch 300 loss at 500 samples is 1.579046875\n",
            "2024-01-14 10:31:57,174 - For batch 400 loss at 500 samples is 1.568625\n",
            "2024-01-14 10:31:57,858 - For batch 500 loss at 500 samples is 1.569734375\n",
            "2024-01-14 10:31:58,599 - For batch 600 loss at 500 samples is 1.5794765625\n",
            "2024-01-14 10:31:59,401 - For batch 700 loss at 500 samples is 1.5620625\n",
            "2024-01-14 10:32:00,012 - For batch 800 loss at 500 samples is 1.5673671875\n",
            "2024-01-14 10:32:00,773 - For batch 900 loss at 500 samples is 1.5762890625\n",
            "2024-01-14 10:32:01,522 - For batch 1000 loss at 500 samples is 1.575078125\n",
            "2024-01-14 10:32:02,209 - For batch 1100 loss at 500 samples is 1.5607734375\n",
            "2024-01-14 10:32:02,926 - For batch 1200 loss at 500 samples is 1.5564765625\n",
            "2024-01-14 10:32:03,687 - For batch 1300 loss at 500 samples is 1.5662890625\n",
            "2024-01-14 10:32:04,504 - For batch 1400 loss at 500 samples is 1.5602265625\n",
            "2024-01-14 10:32:04,909 - LOSS train 1.5602265625 valid 9.09375\n",
            "2024-01-14 10:32:04,912 - Starting training at 145\n",
            "2024-01-14 10:32:05,633 - For batch 100 loss at 500 samples is 1.5721484375\n",
            "2024-01-14 10:32:06,359 - For batch 200 loss at 500 samples is 1.5790546875\n",
            "2024-01-14 10:32:07,486 - For batch 300 loss at 500 samples is 1.5785390625\n",
            "2024-01-14 10:32:08,563 - For batch 400 loss at 500 samples is 1.5668359375\n",
            "2024-01-14 10:32:09,675 - For batch 500 loss at 500 samples is 1.568828125\n",
            "2024-01-14 10:32:10,348 - For batch 600 loss at 500 samples is 1.5799296875\n",
            "2024-01-14 10:32:11,148 - For batch 700 loss at 500 samples is 1.56278125\n",
            "2024-01-14 10:32:11,897 - For batch 800 loss at 500 samples is 1.5671015625\n",
            "2024-01-14 10:32:12,661 - For batch 900 loss at 500 samples is 1.570421875\n",
            "2024-01-14 10:32:13,484 - For batch 1000 loss at 500 samples is 1.576234375\n",
            "2024-01-14 10:32:14,163 - For batch 1100 loss at 500 samples is 1.5614765625\n",
            "2024-01-14 10:32:14,898 - For batch 1200 loss at 500 samples is 1.55228125\n",
            "2024-01-14 10:32:15,571 - For batch 1300 loss at 500 samples is 1.56628125\n",
            "2024-01-14 10:32:16,257 - For batch 1400 loss at 500 samples is 1.5654921875\n",
            "2024-01-14 10:32:16,743 - LOSS train 1.5654921875 valid 9.015625\n",
            "2024-01-14 10:32:16,745 - Starting training at 146\n",
            "2024-01-14 10:32:17,558 - For batch 100 loss at 500 samples is 1.5711328125\n",
            "2024-01-14 10:32:18,321 - For batch 200 loss at 500 samples is 1.5800625\n",
            "2024-01-14 10:32:19,024 - For batch 300 loss at 500 samples is 1.5806328125\n",
            "2024-01-14 10:32:19,894 - For batch 400 loss at 500 samples is 1.5670625\n",
            "2024-01-14 10:32:20,886 - For batch 500 loss at 500 samples is 1.5714296875\n",
            "2024-01-14 10:32:22,008 - For batch 600 loss at 500 samples is 1.579703125\n",
            "2024-01-14 10:32:23,036 - For batch 700 loss at 500 samples is 1.5629609375\n",
            "2024-01-14 10:32:23,723 - For batch 800 loss at 500 samples is 1.569671875\n",
            "2024-01-14 10:32:24,458 - For batch 900 loss at 500 samples is 1.5712890625\n",
            "2024-01-14 10:32:25,229 - For batch 1000 loss at 500 samples is 1.575015625\n",
            "2024-01-14 10:32:26,002 - For batch 1100 loss at 500 samples is 1.5658515625\n",
            "2024-01-14 10:32:26,760 - For batch 1200 loss at 500 samples is 1.554171875\n",
            "2024-01-14 10:32:27,594 - For batch 1300 loss at 500 samples is 1.5662890625\n",
            "2024-01-14 10:32:28,401 - For batch 1400 loss at 500 samples is 1.5589921875\n",
            "2024-01-14 10:32:28,980 - LOSS train 1.5589921875 valid 8.96875\n",
            "2024-01-14 10:32:28,983 - Starting training at 147\n",
            "2024-01-14 10:32:29,647 - For batch 100 loss at 500 samples is 1.571953125\n",
            "2024-01-14 10:32:30,511 - For batch 200 loss at 500 samples is 1.580078125\n",
            "2024-01-14 10:32:31,278 - For batch 300 loss at 500 samples is 1.5782890625\n",
            "2024-01-14 10:32:31,958 - For batch 400 loss at 500 samples is 1.5688203125\n",
            "2024-01-14 10:32:32,628 - For batch 500 loss at 500 samples is 1.5701171875\n",
            "2024-01-14 10:32:33,734 - For batch 600 loss at 500 samples is 1.58009375\n",
            "2024-01-14 10:32:34,753 - For batch 700 loss at 500 samples is 1.5606953125\n",
            "2024-01-14 10:32:35,893 - For batch 800 loss at 500 samples is 1.5671875\n",
            "2024-01-14 10:32:36,616 - For batch 900 loss at 500 samples is 1.577375\n",
            "2024-01-14 10:32:37,403 - For batch 1000 loss at 500 samples is 1.5748359375\n",
            "2024-01-14 10:32:38,164 - For batch 1100 loss at 500 samples is 1.5615078125\n",
            "2024-01-14 10:32:38,911 - For batch 1200 loss at 500 samples is 1.5522109375\n",
            "2024-01-14 10:32:39,761 - For batch 1300 loss at 500 samples is 1.5675703125\n",
            "2024-01-14 10:32:40,520 - For batch 1400 loss at 500 samples is 1.566265625\n",
            "2024-01-14 10:32:41,015 - LOSS train 1.566265625 valid 8.8828125\n",
            "2024-01-14 10:32:41,017 - Starting training at 148\n",
            "2024-01-14 10:32:41,718 - For batch 100 loss at 500 samples is 1.5716484375\n",
            "2024-01-14 10:32:42,475 - For batch 200 loss at 500 samples is 1.5812890625\n",
            "2024-01-14 10:32:43,220 - For batch 300 loss at 500 samples is 1.5797109375\n",
            "2024-01-14 10:32:43,981 - For batch 400 loss at 500 samples is 1.566578125\n",
            "2024-01-14 10:32:44,731 - For batch 500 loss at 500 samples is 1.5679375\n",
            "2024-01-14 10:32:45,454 - For batch 600 loss at 500 samples is 1.581671875\n",
            "2024-01-14 10:32:46,406 - For batch 700 loss at 500 samples is 1.56375\n",
            "2024-01-14 10:32:47,620 - For batch 800 loss at 500 samples is 1.5691640625\n",
            "2024-01-14 10:32:48,670 - For batch 900 loss at 500 samples is 1.571765625\n",
            "2024-01-14 10:32:49,474 - For batch 1000 loss at 500 samples is 1.57740625\n",
            "2024-01-14 10:32:50,102 - For batch 1100 loss at 500 samples is 1.5619140625\n",
            "2024-01-14 10:32:50,790 - For batch 1200 loss at 500 samples is 1.5563828125\n",
            "2024-01-14 10:32:51,600 - For batch 1300 loss at 500 samples is 1.5678515625\n",
            "2024-01-14 10:32:52,360 - For batch 1400 loss at 500 samples is 1.562375\n",
            "2024-01-14 10:32:52,894 - LOSS train 1.562375 valid 8.859375\n",
            "2024-01-14 10:32:52,896 - Starting training at 149\n",
            "2024-01-14 10:32:53,659 - For batch 100 loss at 500 samples is 1.5719453125\n",
            "2024-01-14 10:32:54,469 - For batch 200 loss at 500 samples is 1.5804921875\n",
            "2024-01-14 10:32:55,285 - For batch 300 loss at 500 samples is 1.5798125\n",
            "2024-01-14 10:32:56,075 - For batch 400 loss at 500 samples is 1.566671875\n",
            "2024-01-14 10:32:56,862 - For batch 500 loss at 500 samples is 1.573890625\n",
            "2024-01-14 10:32:57,695 - For batch 600 loss at 500 samples is 1.58053125\n",
            "2024-01-14 10:32:58,468 - For batch 700 loss at 500 samples is 1.5642265625\n",
            "2024-01-14 10:32:59,435 - For batch 800 loss at 500 samples is 1.5668828125\n",
            "2024-01-14 10:33:00,478 - For batch 900 loss at 500 samples is 1.5705234375\n",
            "2024-01-14 10:33:01,555 - For batch 1000 loss at 500 samples is 1.5750546875\n",
            "2024-01-14 10:33:02,493 - For batch 1100 loss at 500 samples is 1.5626796875\n",
            "2024-01-14 10:33:03,342 - For batch 1200 loss at 500 samples is 1.552\n",
            "2024-01-14 10:33:04,091 - For batch 1300 loss at 500 samples is 1.5681015625\n",
            "2024-01-14 10:33:04,813 - For batch 1400 loss at 500 samples is 1.5691015625\n",
            "2024-01-14 10:33:05,314 - LOSS train 1.5691015625 valid 8.7734375\n",
            "2024-01-14 10:33:05,316 - Starting training at 150\n",
            "2024-01-14 10:33:06,147 - For batch 100 loss at 500 samples is 1.57221875\n",
            "2024-01-14 10:33:06,853 - For batch 200 loss at 500 samples is 1.58165625\n",
            "2024-01-14 10:33:07,547 - For batch 300 loss at 500 samples is 1.5802421875\n",
            "2024-01-14 10:33:08,257 - For batch 400 loss at 500 samples is 1.5688515625\n",
            "2024-01-14 10:33:09,022 - For batch 500 loss at 500 samples is 1.56821875\n",
            "2024-01-14 10:33:09,774 - For batch 600 loss at 500 samples is 1.581859375\n",
            "2024-01-14 10:33:10,534 - For batch 700 loss at 500 samples is 1.56484375\n",
            "2024-01-14 10:33:11,192 - For batch 800 loss at 500 samples is 1.5698984375\n",
            "2024-01-14 10:33:11,908 - For batch 900 loss at 500 samples is 1.573375\n",
            "2024-01-14 10:33:12,823 - For batch 1000 loss at 500 samples is 1.575359375\n",
            "2024-01-14 10:33:13,846 - For batch 1100 loss at 500 samples is 1.5616328125\n",
            "2024-01-14 10:33:14,811 - For batch 1200 loss at 500 samples is 1.5533046875\n",
            "2024-01-14 10:33:15,660 - For batch 1300 loss at 500 samples is 1.5691328125\n",
            "2024-01-14 10:33:16,373 - For batch 1400 loss at 500 samples is 1.56853125\n",
            "2024-01-14 10:33:16,871 - LOSS train 1.56853125 valid 8.7578125\n",
            "2024-01-14 10:33:16,876 - Starting training at 151\n",
            "2024-01-14 10:33:17,520 - For batch 100 loss at 500 samples is 1.5729921875\n",
            "2024-01-14 10:33:18,201 - For batch 200 loss at 500 samples is 1.580078125\n",
            "2024-01-14 10:33:18,940 - For batch 300 loss at 500 samples is 1.579234375\n",
            "2024-01-14 10:33:19,744 - For batch 400 loss at 500 samples is 1.5672578125\n",
            "2024-01-14 10:33:20,490 - For batch 500 loss at 500 samples is 1.56778125\n",
            "2024-01-14 10:33:21,195 - For batch 600 loss at 500 samples is 1.5813671875\n",
            "2024-01-14 10:33:21,878 - For batch 700 loss at 500 samples is 1.563484375\n",
            "2024-01-14 10:33:22,709 - For batch 800 loss at 500 samples is 1.56846875\n",
            "2024-01-14 10:33:23,435 - For batch 900 loss at 500 samples is 1.5728125\n",
            "2024-01-14 10:33:24,142 - For batch 1000 loss at 500 samples is 1.5778671875\n",
            "2024-01-14 10:33:24,924 - For batch 1100 loss at 500 samples is 1.560640625\n",
            "2024-01-14 10:33:26,101 - For batch 1200 loss at 500 samples is 1.554875\n",
            "2024-01-14 10:33:27,100 - For batch 1300 loss at 500 samples is 1.568359375\n",
            "2024-01-14 10:33:28,158 - For batch 1400 loss at 500 samples is 1.5745234375\n",
            "2024-01-14 10:33:28,682 - LOSS train 1.5745234375 valid 8.671875\n",
            "2024-01-14 10:33:28,684 - Starting training at 152\n",
            "2024-01-14 10:33:29,329 - For batch 100 loss at 500 samples is 1.5729375\n",
            "2024-01-14 10:33:30,022 - For batch 200 loss at 500 samples is 1.5807734375\n",
            "2024-01-14 10:33:30,742 - For batch 300 loss at 500 samples is 1.5798046875\n",
            "2024-01-14 10:33:31,450 - For batch 400 loss at 500 samples is 1.5676328125\n",
            "2024-01-14 10:33:32,127 - For batch 500 loss at 500 samples is 1.5723359375\n",
            "2024-01-14 10:33:32,843 - For batch 600 loss at 500 samples is 1.580875\n",
            "2024-01-14 10:33:33,525 - For batch 700 loss at 500 samples is 1.564796875\n",
            "2024-01-14 10:33:34,269 - For batch 800 loss at 500 samples is 1.570625\n",
            "2024-01-14 10:33:35,039 - For batch 900 loss at 500 samples is 1.5726796875\n",
            "2024-01-14 10:33:35,644 - For batch 1000 loss at 500 samples is 1.5766171875\n",
            "2024-01-14 10:33:36,385 - For batch 1100 loss at 500 samples is 1.56209375\n",
            "2024-01-14 10:33:37,142 - For batch 1200 loss at 500 samples is 1.5569921875\n",
            "2024-01-14 10:33:37,821 - For batch 1300 loss at 500 samples is 1.5677734375\n",
            "2024-01-14 10:33:38,546 - For batch 1400 loss at 500 samples is 1.563625\n",
            "2024-01-14 10:33:39,215 - LOSS train 1.563625 valid 8.6015625\n",
            "2024-01-14 10:33:39,234 - Starting training at 153\n",
            "2024-01-14 10:33:40,261 - For batch 100 loss at 500 samples is 1.57396875\n",
            "2024-01-14 10:33:41,414 - For batch 200 loss at 500 samples is 1.5813671875\n",
            "2024-01-14 10:33:42,232 - For batch 300 loss at 500 samples is 1.5801953125\n",
            "2024-01-14 10:33:42,973 - For batch 400 loss at 500 samples is 1.569765625\n",
            "2024-01-14 10:33:43,732 - For batch 500 loss at 500 samples is 1.5709375\n",
            "2024-01-14 10:33:44,544 - For batch 600 loss at 500 samples is 1.5806640625\n",
            "2024-01-14 10:33:45,354 - For batch 700 loss at 500 samples is 1.5640859375\n",
            "2024-01-14 10:33:46,090 - For batch 800 loss at 500 samples is 1.567359375\n",
            "2024-01-14 10:33:46,816 - For batch 900 loss at 500 samples is 1.5755703125\n",
            "2024-01-14 10:33:47,531 - For batch 1000 loss at 500 samples is 1.576125\n",
            "2024-01-14 10:33:48,355 - For batch 1100 loss at 500 samples is 1.562484375\n",
            "2024-01-14 10:33:49,161 - For batch 1200 loss at 500 samples is 1.5541875\n",
            "2024-01-14 10:33:49,873 - For batch 1300 loss at 500 samples is 1.5679140625\n",
            "2024-01-14 10:33:50,572 - For batch 1400 loss at 500 samples is 1.56696875\n",
            "2024-01-14 10:33:51,090 - LOSS train 1.56696875 valid 8.5546875\n",
            "2024-01-14 10:33:51,095 - Starting training at 154\n",
            "2024-01-14 10:33:51,914 - For batch 100 loss at 500 samples is 1.573515625\n",
            "2024-01-14 10:33:52,997 - For batch 200 loss at 500 samples is 1.583171875\n",
            "2024-01-14 10:33:54,145 - For batch 300 loss at 500 samples is 1.58115625\n",
            "2024-01-14 10:33:55,125 - For batch 400 loss at 500 samples is 1.5680390625\n",
            "2024-01-14 10:33:55,849 - For batch 500 loss at 500 samples is 1.570109375\n",
            "2024-01-14 10:33:56,567 - For batch 600 loss at 500 samples is 1.5823984375\n",
            "2024-01-14 10:33:57,367 - For batch 700 loss at 500 samples is 1.56475\n",
            "2024-01-14 10:33:57,993 - For batch 800 loss at 500 samples is 1.5709609375\n",
            "2024-01-14 10:33:58,747 - For batch 900 loss at 500 samples is 1.571234375\n",
            "2024-01-14 10:33:59,443 - For batch 1000 loss at 500 samples is 1.57515625\n",
            "2024-01-14 10:34:00,114 - For batch 1100 loss at 500 samples is 1.563546875\n",
            "2024-01-14 10:34:00,816 - For batch 1200 loss at 500 samples is 1.55425\n",
            "2024-01-14 10:34:01,572 - For batch 1300 loss at 500 samples is 1.5678125\n",
            "2024-01-14 10:34:02,311 - For batch 1400 loss at 500 samples is 1.567109375\n",
            "2024-01-14 10:34:02,734 - LOSS train 1.567109375 valid 8.4921875\n",
            "2024-01-14 10:34:02,736 - Starting training at 155\n",
            "2024-01-14 10:34:03,510 - For batch 100 loss at 500 samples is 1.5737890625\n",
            "2024-01-14 10:34:04,239 - For batch 200 loss at 500 samples is 1.580328125\n",
            "2024-01-14 10:34:05,029 - For batch 300 loss at 500 samples is 1.579734375\n",
            "2024-01-14 10:34:06,174 - For batch 400 loss at 500 samples is 1.5677890625\n",
            "2024-01-14 10:34:07,273 - For batch 500 loss at 500 samples is 1.57203125\n",
            "2024-01-14 10:34:08,301 - For batch 600 loss at 500 samples is 1.582078125\n",
            "2024-01-14 10:34:09,034 - For batch 700 loss at 500 samples is 1.56371875\n",
            "2024-01-14 10:34:09,811 - For batch 800 loss at 500 samples is 1.567640625\n",
            "2024-01-14 10:34:10,515 - For batch 900 loss at 500 samples is 1.57115625\n",
            "2024-01-14 10:34:11,246 - For batch 1000 loss at 500 samples is 1.57571875\n",
            "2024-01-14 10:34:11,969 - For batch 1100 loss at 500 samples is 1.564515625\n",
            "2024-01-14 10:34:12,793 - For batch 1200 loss at 500 samples is 1.5559765625\n",
            "2024-01-14 10:34:13,517 - For batch 1300 loss at 500 samples is 1.567484375\n",
            "2024-01-14 10:34:14,206 - For batch 1400 loss at 500 samples is 1.568953125\n",
            "2024-01-14 10:34:14,712 - LOSS train 1.568953125 valid 8.4453125\n",
            "2024-01-14 10:34:14,714 - Starting training at 156\n",
            "2024-01-14 10:34:15,474 - For batch 100 loss at 500 samples is 1.5736796875\n",
            "2024-01-14 10:34:16,156 - For batch 200 loss at 500 samples is 1.5801640625\n",
            "2024-01-14 10:34:16,791 - For batch 300 loss at 500 samples is 1.5800234375\n",
            "2024-01-14 10:34:17,549 - For batch 400 loss at 500 samples is 1.56840625\n",
            "2024-01-14 10:34:18,398 - For batch 500 loss at 500 samples is 1.5685078125\n",
            "2024-01-14 10:34:19,428 - For batch 600 loss at 500 samples is 1.5811875\n",
            "2024-01-14 10:34:20,593 - For batch 700 loss at 500 samples is 1.565078125\n",
            "2024-01-14 10:34:21,436 - For batch 800 loss at 500 samples is 1.570890625\n",
            "2024-01-14 10:34:22,116 - For batch 900 loss at 500 samples is 1.57490625\n",
            "2024-01-14 10:34:22,966 - For batch 1000 loss at 500 samples is 1.577765625\n",
            "2024-01-14 10:34:23,718 - For batch 1100 loss at 500 samples is 1.5634765625\n",
            "2024-01-14 10:34:24,485 - For batch 1200 loss at 500 samples is 1.5548046875\n",
            "2024-01-14 10:34:25,230 - For batch 1300 loss at 500 samples is 1.568375\n",
            "2024-01-14 10:34:26,038 - For batch 1400 loss at 500 samples is 1.563109375\n",
            "2024-01-14 10:34:26,577 - LOSS train 1.563109375 valid 8.3828125\n",
            "2024-01-14 10:34:26,581 - Starting training at 157\n",
            "2024-01-14 10:34:27,267 - For batch 100 loss at 500 samples is 1.575984375\n",
            "2024-01-14 10:34:28,100 - For batch 200 loss at 500 samples is 1.582\n",
            "2024-01-14 10:34:28,876 - For batch 300 loss at 500 samples is 1.5801015625\n",
            "2024-01-14 10:34:29,554 - For batch 400 loss at 500 samples is 1.567890625\n",
            "2024-01-14 10:34:30,263 - For batch 500 loss at 500 samples is 1.573046875\n",
            "2024-01-14 10:34:31,038 - For batch 600 loss at 500 samples is 1.5806796875\n",
            "2024-01-14 10:34:31,835 - For batch 700 loss at 500 samples is 1.5647265625\n",
            "2024-01-14 10:34:33,108 - For batch 800 loss at 500 samples is 1.5713828125\n",
            "2024-01-14 10:34:34,009 - For batch 900 loss at 500 samples is 1.574265625\n",
            "2024-01-14 10:34:34,830 - For batch 1000 loss at 500 samples is 1.5764609375\n",
            "2024-01-14 10:34:35,586 - For batch 1100 loss at 500 samples is 1.5662265625\n",
            "2024-01-14 10:34:36,380 - For batch 1200 loss at 500 samples is 1.55734375\n",
            "2024-01-14 10:34:37,143 - For batch 1300 loss at 500 samples is 1.5692421875\n",
            "2024-01-14 10:34:37,847 - For batch 1400 loss at 500 samples is 1.5714140625\n",
            "2024-01-14 10:34:38,344 - LOSS train 1.5714140625 valid 8.3828125\n",
            "2024-01-14 10:34:38,346 - Starting training at 158\n",
            "2024-01-14 10:34:38,996 - For batch 100 loss at 500 samples is 1.5746796875\n",
            "2024-01-14 10:34:39,644 - For batch 200 loss at 500 samples is 1.58378125\n",
            "2024-01-14 10:34:40,436 - For batch 300 loss at 500 samples is 1.582015625\n",
            "2024-01-14 10:34:41,148 - For batch 400 loss at 500 samples is 1.5687109375\n",
            "2024-01-14 10:34:41,920 - For batch 500 loss at 500 samples is 1.5780859375\n",
            "2024-01-14 10:34:42,670 - For batch 600 loss at 500 samples is 1.582015625\n",
            "2024-01-14 10:34:43,329 - For batch 700 loss at 500 samples is 1.5658984375\n",
            "2024-01-14 10:34:44,155 - For batch 800 loss at 500 samples is 1.573375\n",
            "2024-01-14 10:34:45,005 - For batch 900 loss at 500 samples is 1.5704453125\n",
            "2024-01-14 10:34:45,913 - For batch 1000 loss at 500 samples is 1.5767890625\n",
            "2024-01-14 10:34:46,823 - For batch 1100 loss at 500 samples is 1.5633671875\n",
            "2024-01-14 10:34:47,701 - For batch 1200 loss at 500 samples is 1.55821875\n",
            "2024-01-14 10:34:48,465 - For batch 1300 loss at 500 samples is 1.5693125\n",
            "2024-01-14 10:34:49,240 - For batch 1400 loss at 500 samples is 1.56375\n",
            "2024-01-14 10:34:49,755 - LOSS train 1.56375 valid 8.2890625\n",
            "2024-01-14 10:34:49,758 - Starting training at 159\n",
            "2024-01-14 10:34:50,491 - For batch 100 loss at 500 samples is 1.5752109375\n",
            "2024-01-14 10:34:51,265 - For batch 200 loss at 500 samples is 1.5817890625\n",
            "2024-01-14 10:34:51,992 - For batch 300 loss at 500 samples is 1.5805703125\n",
            "2024-01-14 10:34:52,712 - For batch 400 loss at 500 samples is 1.56884375\n",
            "2024-01-14 10:34:53,369 - For batch 500 loss at 500 samples is 1.5703671875\n",
            "2024-01-14 10:34:54,131 - For batch 600 loss at 500 samples is 1.58128125\n",
            "2024-01-14 10:34:54,794 - For batch 700 loss at 500 samples is 1.5645625\n",
            "2024-01-14 10:34:55,525 - For batch 800 loss at 500 samples is 1.5712265625\n",
            "2024-01-14 10:34:56,235 - For batch 900 loss at 500 samples is 1.5730625\n",
            "2024-01-14 10:34:57,017 - For batch 1000 loss at 500 samples is 1.5785703125\n",
            "2024-01-14 10:34:57,926 - For batch 1100 loss at 500 samples is 1.5663046875\n",
            "2024-01-14 10:34:59,068 - For batch 1200 loss at 500 samples is 1.557125\n",
            "2024-01-14 10:35:00,162 - For batch 1300 loss at 500 samples is 1.56984375\n",
            "2024-01-14 10:35:01,123 - For batch 1400 loss at 500 samples is 1.568546875\n",
            "2024-01-14 10:35:01,687 - LOSS train 1.568546875 valid 8.2265625\n",
            "2024-01-14 10:35:01,690 - Starting training at 160\n",
            "2024-01-14 10:35:02,471 - For batch 100 loss at 500 samples is 1.574203125\n",
            "2024-01-14 10:35:03,247 - For batch 200 loss at 500 samples is 1.5809609375\n",
            "2024-01-14 10:35:03,891 - For batch 300 loss at 500 samples is 1.58009375\n",
            "2024-01-14 10:35:04,620 - For batch 400 loss at 500 samples is 1.5713203125\n",
            "2024-01-14 10:35:05,373 - For batch 500 loss at 500 samples is 1.5713125\n",
            "2024-01-14 10:35:06,090 - For batch 600 loss at 500 samples is 1.5821875\n",
            "2024-01-14 10:35:06,888 - For batch 700 loss at 500 samples is 1.5651015625\n",
            "2024-01-14 10:35:07,559 - For batch 800 loss at 500 samples is 1.57034375\n",
            "2024-01-14 10:35:08,229 - For batch 900 loss at 500 samples is 1.577140625\n",
            "2024-01-14 10:35:08,925 - For batch 1000 loss at 500 samples is 1.576625\n",
            "2024-01-14 10:35:09,601 - For batch 1100 loss at 500 samples is 1.564515625\n",
            "2024-01-14 10:35:10,304 - For batch 1200 loss at 500 samples is 1.557875\n",
            "2024-01-14 10:35:11,075 - For batch 1300 loss at 500 samples is 1.5692265625\n",
            "2024-01-14 10:35:12,070 - For batch 1400 loss at 500 samples is 1.5678046875\n",
            "2024-01-14 10:35:12,706 - LOSS train 1.5678046875 valid 8.1796875\n",
            "2024-01-14 10:35:12,712 - Starting training at 161\n",
            "2024-01-14 10:35:13,625 - For batch 100 loss at 500 samples is 1.573875\n",
            "2024-01-14 10:35:14,428 - For batch 200 loss at 500 samples is 1.5808984375\n",
            "2024-01-14 10:35:15,103 - For batch 300 loss at 500 samples is 1.580875\n",
            "2024-01-14 10:35:15,877 - For batch 400 loss at 500 samples is 1.5700546875\n",
            "2024-01-14 10:35:16,576 - For batch 500 loss at 500 samples is 1.5788046875\n",
            "2024-01-14 10:35:17,337 - For batch 600 loss at 500 samples is 1.581640625\n",
            "2024-01-14 10:35:18,151 - For batch 700 loss at 500 samples is 1.5659375\n",
            "2024-01-14 10:35:18,907 - For batch 800 loss at 500 samples is 1.5699921875\n",
            "2024-01-14 10:35:19,682 - For batch 900 loss at 500 samples is 1.5710703125\n",
            "2024-01-14 10:35:20,364 - For batch 1000 loss at 500 samples is 1.5760703125\n",
            "2024-01-14 10:35:21,196 - For batch 1100 loss at 500 samples is 1.5706875\n",
            "2024-01-14 10:35:21,960 - For batch 1200 loss at 500 samples is 1.5590234375\n",
            "2024-01-14 10:35:22,589 - For batch 1300 loss at 500 samples is 1.5679140625\n",
            "2024-01-14 10:35:23,314 - For batch 1400 loss at 500 samples is 1.5704765625\n",
            "2024-01-14 10:35:23,770 - LOSS train 1.5704765625 valid 8.109375\n",
            "2024-01-14 10:35:23,772 - Starting training at 162\n",
            "2024-01-14 10:35:24,681 - For batch 100 loss at 500 samples is 1.57321875\n",
            "2024-01-14 10:35:25,712 - For batch 200 loss at 500 samples is 1.58075\n",
            "2024-01-14 10:35:26,651 - For batch 300 loss at 500 samples is 1.5817109375\n",
            "2024-01-14 10:35:27,583 - For batch 400 loss at 500 samples is 1.5694453125\n",
            "2024-01-14 10:35:28,291 - For batch 500 loss at 500 samples is 1.57115625\n",
            "2024-01-14 10:35:29,001 - For batch 600 loss at 500 samples is 1.5818515625\n",
            "2024-01-14 10:35:29,650 - For batch 700 loss at 500 samples is 1.566890625\n",
            "2024-01-14 10:35:30,327 - For batch 800 loss at 500 samples is 1.5730625\n",
            "2024-01-14 10:35:31,014 - For batch 900 loss at 500 samples is 1.5727265625\n",
            "2024-01-14 10:35:31,711 - For batch 1000 loss at 500 samples is 1.5762734375\n",
            "2024-01-14 10:35:32,345 - For batch 1100 loss at 500 samples is 1.5646484375\n",
            "2024-01-14 10:35:33,039 - For batch 1200 loss at 500 samples is 1.5573125\n",
            "2024-01-14 10:35:33,748 - For batch 1300 loss at 500 samples is 1.5687109375\n",
            "2024-01-14 10:35:34,466 - For batch 1400 loss at 500 samples is 1.563671875\n",
            "2024-01-14 10:35:34,907 - LOSS train 1.563671875 valid 8.09375\n",
            "2024-01-14 10:35:34,910 - Starting training at 163\n",
            "2024-01-14 10:35:35,607 - For batch 100 loss at 500 samples is 1.5739765625\n",
            "2024-01-14 10:35:36,317 - For batch 200 loss at 500 samples is 1.58153125\n",
            "2024-01-14 10:35:37,004 - For batch 300 loss at 500 samples is 1.5814453125\n",
            "2024-01-14 10:35:37,881 - For batch 400 loss at 500 samples is 1.5690625\n",
            "2024-01-14 10:35:38,835 - For batch 500 loss at 500 samples is 1.5733359375\n",
            "2024-01-14 10:35:39,852 - For batch 600 loss at 500 samples is 1.58121875\n",
            "2024-01-14 10:35:40,817 - For batch 700 loss at 500 samples is 1.5661875\n",
            "2024-01-14 10:35:41,571 - For batch 800 loss at 500 samples is 1.5698203125\n",
            "2024-01-14 10:35:42,328 - For batch 900 loss at 500 samples is 1.57521875\n",
            "2024-01-14 10:35:42,942 - For batch 1000 loss at 500 samples is 1.5756640625\n",
            "2024-01-14 10:35:43,721 - For batch 1100 loss at 500 samples is 1.5674453125\n",
            "2024-01-14 10:35:44,505 - For batch 1200 loss at 500 samples is 1.55609375\n",
            "2024-01-14 10:35:45,273 - For batch 1300 loss at 500 samples is 1.5694609375\n",
            "2024-01-14 10:35:46,053 - For batch 1400 loss at 500 samples is 1.570359375\n",
            "2024-01-14 10:35:46,594 - LOSS train 1.570359375 valid 8.0234375\n",
            "2024-01-14 10:35:46,597 - Starting training at 164\n",
            "2024-01-14 10:35:47,329 - For batch 100 loss at 500 samples is 1.573734375\n",
            "2024-01-14 10:35:48,091 - For batch 200 loss at 500 samples is 1.5809453125\n",
            "2024-01-14 10:35:48,867 - For batch 300 loss at 500 samples is 1.5818203125\n",
            "2024-01-14 10:35:49,538 - For batch 400 loss at 500 samples is 1.5707578125\n",
            "2024-01-14 10:35:50,200 - For batch 500 loss at 500 samples is 1.5791015625\n",
            "2024-01-14 10:35:51,276 - For batch 600 loss at 500 samples is 1.5815078125\n",
            "2024-01-14 10:35:52,408 - For batch 700 loss at 500 samples is 1.5684140625\n",
            "2024-01-14 10:35:53,327 - For batch 800 loss at 500 samples is 1.572234375\n",
            "2024-01-14 10:35:54,122 - For batch 900 loss at 500 samples is 1.5727109375\n",
            "2024-01-14 10:35:54,790 - For batch 1000 loss at 500 samples is 1.577125\n",
            "2024-01-14 10:35:55,521 - For batch 1100 loss at 500 samples is 1.56715625\n",
            "2024-01-14 10:35:56,252 - For batch 1200 loss at 500 samples is 1.5610625\n",
            "2024-01-14 10:35:57,083 - For batch 1300 loss at 500 samples is 1.5695234375\n",
            "2024-01-14 10:35:57,883 - For batch 1400 loss at 500 samples is 1.5668359375\n",
            "2024-01-14 10:35:58,266 - LOSS train 1.5668359375 valid 8.0390625\n",
            "2024-01-14 10:35:58,268 - Starting training at 165\n",
            "2024-01-14 10:35:59,066 - For batch 100 loss at 500 samples is 1.5740234375\n",
            "2024-01-14 10:35:59,804 - For batch 200 loss at 500 samples is 1.5825546875\n",
            "2024-01-14 10:36:00,575 - For batch 300 loss at 500 samples is 1.5811953125\n",
            "2024-01-14 10:36:01,316 - For batch 400 loss at 500 samples is 1.572109375\n",
            "2024-01-14 10:36:02,001 - For batch 500 loss at 500 samples is 1.573375\n",
            "2024-01-14 10:36:02,708 - For batch 600 loss at 500 samples is 1.5805625\n",
            "2024-01-14 10:36:03,467 - For batch 700 loss at 500 samples is 1.5672578125\n",
            "2024-01-14 10:36:04,414 - For batch 800 loss at 500 samples is 1.57053125\n",
            "2024-01-14 10:36:05,531 - For batch 900 loss at 500 samples is 1.5765703125\n",
            "2024-01-14 10:36:06,634 - For batch 1000 loss at 500 samples is 1.5769921875\n",
            "2024-01-14 10:36:07,341 - For batch 1100 loss at 500 samples is 1.5675390625\n",
            "2024-01-14 10:36:08,020 - For batch 1200 loss at 500 samples is 1.5571171875\n",
            "2024-01-14 10:36:08,703 - For batch 1300 loss at 500 samples is 1.56978125\n",
            "2024-01-14 10:36:09,411 - For batch 1400 loss at 500 samples is 1.5697109375\n",
            "2024-01-14 10:36:09,909 - LOSS train 1.5697109375 valid 7.9375\n",
            "2024-01-14 10:36:09,914 - Starting training at 166\n",
            "2024-01-14 10:36:10,613 - For batch 100 loss at 500 samples is 1.5754375\n",
            "2024-01-14 10:36:11,297 - For batch 200 loss at 500 samples is 1.583546875\n",
            "2024-01-14 10:36:11,954 - For batch 300 loss at 500 samples is 1.5828515625\n",
            "2024-01-14 10:36:12,701 - For batch 400 loss at 500 samples is 1.570140625\n",
            "2024-01-14 10:36:13,404 - For batch 500 loss at 500 samples is 1.5721875\n",
            "2024-01-14 10:36:14,082 - For batch 600 loss at 500 samples is 1.58140625\n",
            "2024-01-14 10:36:14,797 - For batch 700 loss at 500 samples is 1.5687109375\n",
            "2024-01-14 10:36:15,494 - For batch 800 loss at 500 samples is 1.5734765625\n",
            "2024-01-14 10:36:16,222 - For batch 900 loss at 500 samples is 1.57384375\n",
            "2024-01-14 10:36:17,131 - For batch 1000 loss at 500 samples is 1.577953125\n",
            "2024-01-14 10:36:18,264 - For batch 1100 loss at 500 samples is 1.5649375\n",
            "2024-01-14 10:36:19,338 - For batch 1200 loss at 500 samples is 1.5579921875\n",
            "2024-01-14 10:36:20,200 - For batch 1300 loss at 500 samples is 1.569453125\n",
            "2024-01-14 10:36:20,945 - For batch 1400 loss at 500 samples is 1.5692109375\n",
            "2024-01-14 10:36:21,458 - LOSS train 1.5692109375 valid 7.90234375\n",
            "2024-01-14 10:36:21,461 - Starting training at 167\n",
            "2024-01-14 10:36:22,158 - For batch 100 loss at 500 samples is 1.5751328125\n",
            "2024-01-14 10:36:23,005 - For batch 200 loss at 500 samples is 1.586078125\n",
            "2024-01-14 10:36:23,768 - For batch 300 loss at 500 samples is 1.5817890625\n",
            "2024-01-14 10:36:24,524 - For batch 400 loss at 500 samples is 1.5698671875\n",
            "2024-01-14 10:36:25,230 - For batch 500 loss at 500 samples is 1.5781015625\n",
            "2024-01-14 10:36:25,937 - For batch 600 loss at 500 samples is 1.5827421875\n",
            "2024-01-14 10:36:26,617 - For batch 700 loss at 500 samples is 1.566109375\n",
            "2024-01-14 10:36:27,363 - For batch 800 loss at 500 samples is 1.5717734375\n",
            "2024-01-14 10:36:28,047 - For batch 900 loss at 500 samples is 1.57278125\n",
            "2024-01-14 10:36:28,744 - For batch 1000 loss at 500 samples is 1.5780625\n",
            "2024-01-14 10:36:29,422 - For batch 1100 loss at 500 samples is 1.5682109375\n",
            "2024-01-14 10:36:30,334 - For batch 1200 loss at 500 samples is 1.5574140625\n",
            "2024-01-14 10:36:31,519 - For batch 1300 loss at 500 samples is 1.570046875\n",
            "2024-01-14 10:36:32,621 - For batch 1400 loss at 500 samples is 1.5711484375\n",
            "2024-01-14 10:36:33,309 - LOSS train 1.5711484375 valid 7.83203125\n",
            "2024-01-14 10:36:33,311 - Starting training at 168\n",
            "2024-01-14 10:36:33,981 - For batch 100 loss at 500 samples is 1.575703125\n",
            "2024-01-14 10:36:34,674 - For batch 200 loss at 500 samples is 1.58234375\n",
            "2024-01-14 10:36:35,344 - For batch 300 loss at 500 samples is 1.5827265625\n",
            "2024-01-14 10:36:36,037 - For batch 400 loss at 500 samples is 1.57071875\n",
            "2024-01-14 10:36:36,723 - For batch 500 loss at 500 samples is 1.5720703125\n",
            "2024-01-14 10:36:37,489 - For batch 600 loss at 500 samples is 1.58303125\n",
            "2024-01-14 10:36:38,188 - For batch 700 loss at 500 samples is 1.5686796875\n",
            "2024-01-14 10:36:38,968 - For batch 800 loss at 500 samples is 1.572171875\n",
            "2024-01-14 10:36:39,648 - For batch 900 loss at 500 samples is 1.5743671875\n",
            "2024-01-14 10:36:40,382 - For batch 1000 loss at 500 samples is 1.5800234375\n",
            "2024-01-14 10:36:41,060 - For batch 1100 loss at 500 samples is 1.5655625\n",
            "2024-01-14 10:36:41,683 - For batch 1200 loss at 500 samples is 1.558234375\n",
            "2024-01-14 10:36:42,454 - For batch 1300 loss at 500 samples is 1.57140625\n",
            "2024-01-14 10:36:43,176 - For batch 1400 loss at 500 samples is 1.568015625\n",
            "2024-01-14 10:36:43,793 - LOSS train 1.568015625 valid 7.8046875\n",
            "2024-01-14 10:36:43,804 - Starting training at 169\n",
            "2024-01-14 10:36:44,731 - For batch 100 loss at 500 samples is 1.5756640625\n",
            "2024-01-14 10:36:45,680 - For batch 200 loss at 500 samples is 1.5825390625\n",
            "2024-01-14 10:36:46,533 - For batch 300 loss at 500 samples is 1.5821640625\n",
            "2024-01-14 10:36:47,225 - For batch 400 loss at 500 samples is 1.5695\n",
            "2024-01-14 10:36:48,016 - For batch 500 loss at 500 samples is 1.572484375\n",
            "2024-01-14 10:36:48,791 - For batch 600 loss at 500 samples is 1.5828125\n",
            "2024-01-14 10:36:49,533 - For batch 700 loss at 500 samples is 1.5678125\n",
            "2024-01-14 10:36:50,378 - For batch 800 loss at 500 samples is 1.5731171875\n",
            "2024-01-14 10:36:51,090 - For batch 900 loss at 500 samples is 1.5765546875\n",
            "2024-01-14 10:36:51,767 - For batch 1000 loss at 500 samples is 1.578390625\n",
            "2024-01-14 10:36:52,568 - For batch 1100 loss at 500 samples is 1.569921875\n",
            "2024-01-14 10:36:53,311 - For batch 1200 loss at 500 samples is 1.559484375\n",
            "2024-01-14 10:36:54,007 - For batch 1300 loss at 500 samples is 1.5710703125\n",
            "2024-01-14 10:36:54,678 - For batch 1400 loss at 500 samples is 1.576171875\n",
            "2024-01-14 10:36:55,228 - LOSS train 1.576171875 valid 7.73828125\n",
            "2024-01-14 10:36:55,230 - Starting training at 170\n",
            "2024-01-14 10:36:56,004 - For batch 100 loss at 500 samples is 1.5756875\n",
            "2024-01-14 10:36:56,869 - For batch 200 loss at 500 samples is 1.5833203125\n",
            "2024-01-14 10:36:57,786 - For batch 300 loss at 500 samples is 1.58353125\n",
            "2024-01-14 10:36:58,735 - For batch 400 loss at 500 samples is 1.569953125\n",
            "2024-01-14 10:36:59,706 - For batch 500 loss at 500 samples is 1.5791953125\n",
            "2024-01-14 10:37:00,371 - For batch 600 loss at 500 samples is 1.5816953125\n",
            "2024-01-14 10:37:01,201 - For batch 700 loss at 500 samples is 1.5684140625\n",
            "2024-01-14 10:37:01,968 - For batch 800 loss at 500 samples is 1.5752578125\n",
            "2024-01-14 10:37:02,671 - For batch 900 loss at 500 samples is 1.5721640625\n",
            "2024-01-14 10:37:03,425 - For batch 1000 loss at 500 samples is 1.5785390625\n",
            "2024-01-14 10:37:04,160 - For batch 1100 loss at 500 samples is 1.5655859375\n",
            "2024-01-14 10:37:04,883 - For batch 1200 loss at 500 samples is 1.56284375\n",
            "2024-01-14 10:37:05,627 - For batch 1300 loss at 500 samples is 1.5716796875\n",
            "2024-01-14 10:37:06,345 - For batch 1400 loss at 500 samples is 1.5660625\n",
            "2024-01-14 10:37:06,801 - LOSS train 1.5660625 valid 7.7421875\n",
            "2024-01-14 10:37:06,803 - Starting training at 171\n",
            "2024-01-14 10:37:07,477 - For batch 100 loss at 500 samples is 1.576015625\n",
            "2024-01-14 10:37:08,176 - For batch 200 loss at 500 samples is 1.5843125\n",
            "2024-01-14 10:37:08,832 - For batch 300 loss at 500 samples is 1.5829765625\n",
            "2024-01-14 10:37:09,575 - For batch 400 loss at 500 samples is 1.570671875\n",
            "2024-01-14 10:37:10,482 - For batch 500 loss at 500 samples is 1.5733125\n",
            "2024-01-14 10:37:11,509 - For batch 600 loss at 500 samples is 1.582328125\n",
            "2024-01-14 10:37:12,570 - For batch 700 loss at 500 samples is 1.5684609375\n",
            "2024-01-14 10:37:13,392 - For batch 800 loss at 500 samples is 1.57165625\n",
            "2024-01-14 10:37:14,122 - For batch 900 loss at 500 samples is 1.57590625\n",
            "2024-01-14 10:37:14,762 - For batch 1000 loss at 500 samples is 1.5800546875\n",
            "2024-01-14 10:37:15,537 - For batch 1100 loss at 500 samples is 1.5673359375\n",
            "2024-01-14 10:37:16,253 - For batch 1200 loss at 500 samples is 1.56071875\n",
            "2024-01-14 10:37:16,952 - For batch 1300 loss at 500 samples is 1.5727109375\n",
            "2024-01-14 10:37:17,626 - For batch 1400 loss at 500 samples is 1.57203125\n",
            "2024-01-14 10:37:18,059 - LOSS train 1.57203125 valid 7.66015625\n",
            "2024-01-14 10:37:18,062 - Starting training at 172\n",
            "2024-01-14 10:37:18,755 - For batch 100 loss at 500 samples is 1.5755546875\n",
            "2024-01-14 10:37:19,424 - For batch 200 loss at 500 samples is 1.586515625\n",
            "2024-01-14 10:37:20,052 - For batch 300 loss at 500 samples is 1.583203125\n",
            "2024-01-14 10:37:20,682 - For batch 400 loss at 500 samples is 1.57184375\n",
            "2024-01-14 10:37:21,305 - For batch 500 loss at 500 samples is 1.5726328125\n",
            "2024-01-14 10:37:22,068 - For batch 600 loss at 500 samples is 1.583390625\n",
            "2024-01-14 10:37:22,794 - For batch 700 loss at 500 samples is 1.570203125\n",
            "2024-01-14 10:37:23,843 - For batch 800 loss at 500 samples is 1.57184375\n",
            "2024-01-14 10:37:24,833 - For batch 900 loss at 500 samples is 1.576671875\n",
            "2024-01-14 10:37:25,686 - For batch 1000 loss at 500 samples is 1.578984375\n",
            "2024-01-14 10:37:26,499 - For batch 1100 loss at 500 samples is 1.5651796875\n",
            "2024-01-14 10:37:27,152 - For batch 1200 loss at 500 samples is 1.5629765625\n",
            "2024-01-14 10:37:27,854 - For batch 1300 loss at 500 samples is 1.5732578125\n",
            "2024-01-14 10:37:28,646 - For batch 1400 loss at 500 samples is 1.5711171875\n",
            "2024-01-14 10:37:29,144 - LOSS train 1.5711171875 valid 7.609375\n",
            "2024-01-14 10:37:29,146 - Starting training at 173\n",
            "2024-01-14 10:37:29,821 - For batch 100 loss at 500 samples is 1.5759375\n",
            "2024-01-14 10:37:30,515 - For batch 200 loss at 500 samples is 1.582296875\n",
            "2024-01-14 10:37:31,211 - For batch 300 loss at 500 samples is 1.5821953125\n",
            "2024-01-14 10:37:31,904 - For batch 400 loss at 500 samples is 1.5710390625\n",
            "2024-01-14 10:37:32,578 - For batch 500 loss at 500 samples is 1.5783359375\n",
            "2024-01-14 10:37:33,226 - For batch 600 loss at 500 samples is 1.5843984375\n",
            "2024-01-14 10:37:33,940 - For batch 700 loss at 500 samples is 1.56853125\n",
            "2024-01-14 10:37:34,582 - For batch 800 loss at 500 samples is 1.574328125\n",
            "2024-01-14 10:37:35,295 - For batch 900 loss at 500 samples is 1.5721015625\n",
            "2024-01-14 10:37:35,963 - For batch 1000 loss at 500 samples is 1.580140625\n",
            "2024-01-14 10:37:37,021 - For batch 1100 loss at 500 samples is 1.567171875\n",
            "2024-01-14 10:37:38,113 - For batch 1200 loss at 500 samples is 1.561578125\n",
            "2024-01-14 10:37:39,080 - For batch 1300 loss at 500 samples is 1.573\n",
            "2024-01-14 10:37:39,735 - For batch 1400 loss at 500 samples is 1.573390625\n",
            "2024-01-14 10:37:40,165 - LOSS train 1.573390625 valid 7.56640625\n",
            "2024-01-14 10:37:40,167 - Starting training at 174\n",
            "2024-01-14 10:37:40,747 - For batch 100 loss at 500 samples is 1.5760078125\n",
            "2024-01-14 10:37:41,475 - For batch 200 loss at 500 samples is 1.5825234375\n",
            "2024-01-14 10:37:42,163 - For batch 300 loss at 500 samples is 1.5831640625\n",
            "2024-01-14 10:37:42,825 - For batch 400 loss at 500 samples is 1.5712421875\n",
            "2024-01-14 10:37:43,437 - For batch 500 loss at 500 samples is 1.5718359375\n",
            "2024-01-14 10:37:44,099 - For batch 600 loss at 500 samples is 1.5834921875\n",
            "2024-01-14 10:37:44,821 - For batch 700 loss at 500 samples is 1.57071875\n",
            "2024-01-14 10:37:45,405 - For batch 800 loss at 500 samples is 1.57621875\n",
            "2024-01-14 10:37:46,082 - For batch 900 loss at 500 samples is 1.5738203125\n",
            "2024-01-14 10:37:46,702 - For batch 1000 loss at 500 samples is 1.5813671875\n",
            "2024-01-14 10:37:47,274 - For batch 1100 loss at 500 samples is 1.5688125\n",
            "2024-01-14 10:37:47,886 - For batch 1200 loss at 500 samples is 1.5622578125\n",
            "2024-01-14 10:37:48,579 - For batch 1300 loss at 500 samples is 1.5728515625\n",
            "2024-01-14 10:37:49,432 - For batch 1400 loss at 500 samples is 1.5670078125\n",
            "2024-01-14 10:37:49,975 - LOSS train 1.5670078125 valid 7.515625\n",
            "2024-01-14 10:37:49,980 - Starting training at 175\n",
            "2024-01-14 10:37:51,050 - For batch 100 loss at 500 samples is 1.5767109375\n",
            "2024-01-14 10:37:52,078 - For batch 200 loss at 500 samples is 1.585671875\n",
            "2024-01-14 10:37:52,870 - For batch 300 loss at 500 samples is 1.5826171875\n",
            "2024-01-14 10:37:53,607 - For batch 400 loss at 500 samples is 1.57321875\n",
            "2024-01-14 10:37:54,312 - For batch 500 loss at 500 samples is 1.5742109375\n",
            "2024-01-14 10:37:55,026 - For batch 600 loss at 500 samples is 1.584859375\n",
            "2024-01-14 10:37:55,755 - For batch 700 loss at 500 samples is 1.5674609375\n",
            "2024-01-14 10:37:56,421 - For batch 800 loss at 500 samples is 1.5727578125\n",
            "2024-01-14 10:37:57,142 - For batch 900 loss at 500 samples is 1.5755703125\n",
            "2024-01-14 10:37:57,790 - For batch 1000 loss at 500 samples is 1.57959375\n",
            "2024-01-14 10:37:58,454 - For batch 1100 loss at 500 samples is 1.5680078125\n",
            "2024-01-14 10:37:59,143 - For batch 1200 loss at 500 samples is 1.5646953125\n",
            "2024-01-14 10:37:59,764 - For batch 1300 loss at 500 samples is 1.5726640625\n",
            "2024-01-14 10:38:00,480 - For batch 1400 loss at 500 samples is 1.5747265625\n",
            "2024-01-14 10:38:00,959 - LOSS train 1.5747265625 valid 7.5078125\n",
            "2024-01-14 10:38:00,961 - Starting training at 176\n",
            "2024-01-14 10:38:01,615 - For batch 100 loss at 500 samples is 1.57678125\n",
            "2024-01-14 10:38:02,308 - For batch 200 loss at 500 samples is 1.5827421875\n",
            "2024-01-14 10:38:03,121 - For batch 300 loss at 500 samples is 1.58321875\n",
            "2024-01-14 10:38:04,170 - For batch 400 loss at 500 samples is 1.571453125\n",
            "2024-01-14 10:38:05,129 - For batch 500 loss at 500 samples is 1.5776875\n",
            "2024-01-14 10:38:05,942 - For batch 600 loss at 500 samples is 1.5854375\n",
            "2024-01-14 10:38:06,587 - For batch 700 loss at 500 samples is 1.5698203125\n",
            "2024-01-14 10:38:07,241 - For batch 800 loss at 500 samples is 1.57409375\n",
            "2024-01-14 10:38:07,893 - For batch 900 loss at 500 samples is 1.5765859375\n",
            "2024-01-14 10:38:08,652 - For batch 1000 loss at 500 samples is 1.58003125\n",
            "2024-01-14 10:38:09,318 - For batch 1100 loss at 500 samples is 1.5665078125\n",
            "2024-01-14 10:38:10,005 - For batch 1200 loss at 500 samples is 1.5627578125\n",
            "2024-01-14 10:38:10,653 - For batch 1300 loss at 500 samples is 1.5726328125\n",
            "2024-01-14 10:38:11,393 - For batch 1400 loss at 500 samples is 1.571203125\n",
            "2024-01-14 10:38:11,836 - LOSS train 1.571203125 valid 7.4375\n",
            "2024-01-14 10:38:11,839 - Starting training at 177\n",
            "2024-01-14 10:38:12,495 - For batch 100 loss at 500 samples is 1.5766796875\n",
            "2024-01-14 10:38:13,311 - For batch 200 loss at 500 samples is 1.583140625\n",
            "2024-01-14 10:38:14,098 - For batch 300 loss at 500 samples is 1.5831953125\n",
            "2024-01-14 10:38:14,861 - For batch 400 loss at 500 samples is 1.5719609375\n",
            "2024-01-14 10:38:15,617 - For batch 500 loss at 500 samples is 1.572453125\n",
            "2024-01-14 10:38:16,545 - For batch 600 loss at 500 samples is 1.5850546875\n",
            "2024-01-14 10:38:17,523 - For batch 700 loss at 500 samples is 1.56803125\n",
            "2024-01-14 10:38:18,541 - For batch 800 loss at 500 samples is 1.5741171875\n",
            "2024-01-14 10:38:19,247 - For batch 900 loss at 500 samples is 1.5739453125\n",
            "2024-01-14 10:38:19,993 - For batch 1000 loss at 500 samples is 1.5808203125\n",
            "2024-01-14 10:38:20,694 - For batch 1100 loss at 500 samples is 1.5695625\n",
            "2024-01-14 10:38:21,359 - For batch 1200 loss at 500 samples is 1.56225\n",
            "2024-01-14 10:38:22,026 - For batch 1300 loss at 500 samples is 1.572890625\n",
            "2024-01-14 10:38:22,738 - For batch 1400 loss at 500 samples is 1.578\n",
            "2024-01-14 10:38:23,222 - LOSS train 1.578 valid 7.421875\n",
            "2024-01-14 10:38:23,225 - Starting training at 178\n",
            "2024-01-14 10:38:23,874 - For batch 100 loss at 500 samples is 1.5761171875\n",
            "2024-01-14 10:38:24,577 - For batch 200 loss at 500 samples is 1.583828125\n",
            "2024-01-14 10:38:25,267 - For batch 300 loss at 500 samples is 1.5846484375\n",
            "2024-01-14 10:38:25,939 - For batch 400 loss at 500 samples is 1.5736484375\n",
            "2024-01-14 10:38:26,568 - For batch 500 loss at 500 samples is 1.5728359375\n",
            "2024-01-14 10:38:27,265 - For batch 600 loss at 500 samples is 1.5860546875\n",
            "2024-01-14 10:38:27,957 - For batch 700 loss at 500 samples is 1.570828125\n",
            "2024-01-14 10:38:28,703 - For batch 800 loss at 500 samples is 1.5771875\n",
            "2024-01-14 10:38:29,658 - For batch 900 loss at 500 samples is 1.574625\n",
            "2024-01-14 10:38:30,471 - For batch 1000 loss at 500 samples is 1.580078125\n",
            "2024-01-14 10:38:31,371 - For batch 1100 loss at 500 samples is 1.566578125\n",
            "2024-01-14 10:38:32,198 - For batch 1200 loss at 500 samples is 1.56559375\n",
            "2024-01-14 10:38:32,933 - For batch 1300 loss at 500 samples is 1.573796875\n",
            "2024-01-14 10:38:33,717 - For batch 1400 loss at 500 samples is 1.5708984375\n",
            "2024-01-14 10:38:34,196 - LOSS train 1.5708984375 valid 7.34375\n",
            "2024-01-14 10:38:34,202 - Starting training at 179\n",
            "2024-01-14 10:38:34,904 - For batch 100 loss at 500 samples is 1.576953125\n",
            "2024-01-14 10:38:35,579 - For batch 200 loss at 500 samples is 1.585421875\n",
            "2024-01-14 10:38:36,210 - For batch 300 loss at 500 samples is 1.5843828125\n",
            "2024-01-14 10:38:36,911 - For batch 400 loss at 500 samples is 1.572046875\n",
            "2024-01-14 10:38:37,603 - For batch 500 loss at 500 samples is 1.5784765625\n",
            "2024-01-14 10:38:38,271 - For batch 600 loss at 500 samples is 1.586265625\n",
            "2024-01-14 10:38:39,072 - For batch 700 loss at 500 samples is 1.56803125\n",
            "2024-01-14 10:38:39,793 - For batch 800 loss at 500 samples is 1.5729375\n",
            "2024-01-14 10:38:40,483 - For batch 900 loss at 500 samples is 1.576328125\n",
            "2024-01-14 10:38:41,155 - For batch 1000 loss at 500 samples is 1.5798984375\n",
            "2024-01-14 10:38:41,867 - For batch 1100 loss at 500 samples is 1.570625\n",
            "2024-01-14 10:38:42,855 - For batch 1200 loss at 500 samples is 1.56284375\n",
            "2024-01-14 10:38:43,712 - For batch 1300 loss at 500 samples is 1.574\n",
            "2024-01-14 10:38:44,623 - For batch 1400 loss at 500 samples is 1.5705234375\n",
            "2024-01-14 10:38:45,202 - LOSS train 1.5705234375 valid 7.34765625\n",
            "2024-01-14 10:38:45,210 - Starting training at 180\n",
            "2024-01-14 10:38:45,911 - For batch 100 loss at 500 samples is 1.5770625\n",
            "2024-01-14 10:38:46,563 - For batch 200 loss at 500 samples is 1.5893984375\n",
            "2024-01-14 10:38:47,233 - For batch 300 loss at 500 samples is 1.5851171875\n",
            "2024-01-14 10:38:47,863 - For batch 400 loss at 500 samples is 1.57296875\n",
            "2024-01-14 10:38:48,618 - For batch 500 loss at 500 samples is 1.57225\n",
            "2024-01-14 10:38:49,317 - For batch 600 loss at 500 samples is 1.58628125\n",
            "2024-01-14 10:38:50,050 - For batch 700 loss at 500 samples is 1.57028125\n",
            "2024-01-14 10:38:50,730 - For batch 800 loss at 500 samples is 1.5746875\n",
            "2024-01-14 10:38:51,397 - For batch 900 loss at 500 samples is 1.57334375\n",
            "2024-01-14 10:38:52,037 - For batch 1000 loss at 500 samples is 1.582328125\n",
            "2024-01-14 10:38:52,754 - For batch 1100 loss at 500 samples is 1.566484375\n",
            "2024-01-14 10:38:53,430 - For batch 1200 loss at 500 samples is 1.56375\n",
            "2024-01-14 10:38:54,134 - For batch 1300 loss at 500 samples is 1.5735625\n",
            "2024-01-14 10:38:54,811 - For batch 1400 loss at 500 samples is 1.5683984375\n",
            "2024-01-14 10:38:55,342 - LOSS train 1.5683984375 valid 7.265625\n",
            "2024-01-14 10:38:55,345 - Starting training at 181\n",
            "2024-01-14 10:38:56,228 - For batch 100 loss at 500 samples is 1.5771953125\n",
            "2024-01-14 10:38:57,218 - For batch 200 loss at 500 samples is 1.5849296875\n",
            "2024-01-14 10:38:58,167 - For batch 300 loss at 500 samples is 1.5841796875\n",
            "2024-01-14 10:38:58,928 - For batch 400 loss at 500 samples is 1.5733203125\n",
            "2024-01-14 10:38:59,594 - For batch 500 loss at 500 samples is 1.573203125\n",
            "2024-01-14 10:39:00,213 - For batch 600 loss at 500 samples is 1.5857578125\n",
            "2024-01-14 10:39:00,959 - For batch 700 loss at 500 samples is 1.56884375\n",
            "2024-01-14 10:39:01,700 - For batch 800 loss at 500 samples is 1.5759140625\n",
            "2024-01-14 10:39:02,388 - For batch 900 loss at 500 samples is 1.5740625\n",
            "2024-01-14 10:39:03,030 - For batch 1000 loss at 500 samples is 1.58103125\n",
            "2024-01-14 10:39:03,790 - For batch 1100 loss at 500 samples is 1.5685\n",
            "2024-01-14 10:39:04,414 - For batch 1200 loss at 500 samples is 1.565\n",
            "2024-01-14 10:39:05,075 - For batch 1300 loss at 500 samples is 1.5739765625\n",
            "2024-01-14 10:39:05,735 - For batch 1400 loss at 500 samples is 1.570578125\n",
            "2024-01-14 10:39:06,129 - LOSS train 1.570578125 valid 7.25\n",
            "2024-01-14 10:39:06,135 - Starting training at 182\n",
            "2024-01-14 10:39:06,779 - For batch 100 loss at 500 samples is 1.576953125\n",
            "2024-01-14 10:39:07,492 - For batch 200 loss at 500 samples is 1.5866875\n",
            "2024-01-14 10:39:08,137 - For batch 300 loss at 500 samples is 1.585296875\n",
            "2024-01-14 10:39:09,060 - For batch 400 loss at 500 samples is 1.5730625\n",
            "2024-01-14 10:39:09,914 - For batch 500 loss at 500 samples is 1.5788359375\n",
            "2024-01-14 10:39:10,898 - For batch 600 loss at 500 samples is 1.5863125\n",
            "2024-01-14 10:39:11,656 - For batch 700 loss at 500 samples is 1.5713515625\n",
            "2024-01-14 10:39:12,409 - For batch 800 loss at 500 samples is 1.5788359375\n",
            "2024-01-14 10:39:13,069 - For batch 900 loss at 500 samples is 1.57384375\n",
            "2024-01-14 10:39:13,713 - For batch 1000 loss at 500 samples is 1.580859375\n",
            "2024-01-14 10:39:14,495 - For batch 1100 loss at 500 samples is 1.5677578125\n",
            "2024-01-14 10:39:15,093 - For batch 1200 loss at 500 samples is 1.56313671875\n",
            "2024-01-14 10:39:15,756 - For batch 1300 loss at 500 samples is 1.57409375\n",
            "2024-01-14 10:39:16,464 - For batch 1400 loss at 500 samples is 1.570828125\n",
            "2024-01-14 10:39:16,926 - LOSS train 1.570828125 valid 7.19140625\n",
            "2024-01-14 10:39:16,928 - Starting training at 183\n",
            "2024-01-14 10:39:17,675 - For batch 100 loss at 500 samples is 1.57628125\n",
            "2024-01-14 10:39:18,365 - For batch 200 loss at 500 samples is 1.5892578125\n",
            "2024-01-14 10:39:19,021 - For batch 300 loss at 500 samples is 1.584453125\n",
            "2024-01-14 10:39:19,713 - For batch 400 loss at 500 samples is 1.5738828125\n",
            "2024-01-14 10:39:20,357 - For batch 500 loss at 500 samples is 1.57415625\n",
            "2024-01-14 10:39:21,037 - For batch 600 loss at 500 samples is 1.58621875\n",
            "2024-01-14 10:39:21,841 - For batch 700 loss at 500 samples is 1.570390625\n",
            "2024-01-14 10:39:22,797 - For batch 800 loss at 500 samples is 1.5750625\n",
            "2024-01-14 10:39:23,736 - For batch 900 loss at 500 samples is 1.5754375\n",
            "2024-01-14 10:39:24,635 - For batch 1000 loss at 500 samples is 1.5814765625\n",
            "2024-01-14 10:39:25,340 - For batch 1100 loss at 500 samples is 1.5684453125\n",
            "2024-01-14 10:39:26,048 - For batch 1200 loss at 500 samples is 1.56286328125\n",
            "2024-01-14 10:39:26,685 - For batch 1300 loss at 500 samples is 1.5743125\n",
            "2024-01-14 10:39:27,370 - For batch 1400 loss at 500 samples is 1.5724921875\n",
            "2024-01-14 10:39:27,820 - LOSS train 1.5724921875 valid 7.1953125\n",
            "2024-01-14 10:39:27,823 - Starting training at 184\n",
            "2024-01-14 10:39:28,437 - For batch 100 loss at 500 samples is 1.5771171875\n",
            "2024-01-14 10:39:29,097 - For batch 200 loss at 500 samples is 1.585859375\n",
            "2024-01-14 10:39:29,756 - For batch 300 loss at 500 samples is 1.5853984375\n",
            "2024-01-14 10:39:30,473 - For batch 400 loss at 500 samples is 1.57359375\n",
            "2024-01-14 10:39:31,044 - For batch 500 loss at 500 samples is 1.5736640625\n",
            "2024-01-14 10:39:31,642 - For batch 600 loss at 500 samples is 1.5868359375\n",
            "2024-01-14 10:39:32,304 - For batch 700 loss at 500 samples is 1.571515625\n",
            "2024-01-14 10:39:32,918 - For batch 800 loss at 500 samples is 1.574578125\n",
            "2024-01-14 10:39:33,574 - For batch 900 loss at 500 samples is 1.5746796875\n",
            "2024-01-14 10:39:34,175 - For batch 1000 loss at 500 samples is 1.5872265625\n",
            "2024-01-14 10:39:34,917 - For batch 1100 loss at 500 samples is 1.56884375\n",
            "2024-01-14 10:39:35,723 - For batch 1200 loss at 500 samples is 1.565953125\n",
            "2024-01-14 10:39:36,667 - For batch 1300 loss at 500 samples is 1.5750234375\n",
            "2024-01-14 10:39:37,698 - For batch 1400 loss at 500 samples is 1.5662265625\n",
            "2024-01-14 10:39:38,058 - LOSS train 1.5662265625 valid 7.11328125\n",
            "2024-01-14 10:39:38,067 - Starting training at 185\n",
            "2024-01-14 10:39:38,763 - For batch 100 loss at 500 samples is 1.57709375\n",
            "2024-01-14 10:39:39,414 - For batch 200 loss at 500 samples is 1.586484375\n",
            "2024-01-14 10:39:40,094 - For batch 300 loss at 500 samples is 1.5847734375\n",
            "2024-01-14 10:39:40,823 - For batch 400 loss at 500 samples is 1.5736171875\n",
            "2024-01-14 10:39:41,493 - For batch 500 loss at 500 samples is 1.5802109375\n",
            "2024-01-14 10:39:42,195 - For batch 600 loss at 500 samples is 1.5866015625\n",
            "2024-01-14 10:39:42,911 - For batch 700 loss at 500 samples is 1.5704765625\n",
            "2024-01-14 10:39:43,517 - For batch 800 loss at 500 samples is 1.5765234375\n",
            "2024-01-14 10:39:44,141 - For batch 900 loss at 500 samples is 1.5745234375\n",
            "2024-01-14 10:39:44,800 - For batch 1000 loss at 500 samples is 1.58115625\n",
            "2024-01-14 10:39:45,432 - For batch 1100 loss at 500 samples is 1.568640625\n",
            "2024-01-14 10:39:46,180 - For batch 1200 loss at 500 samples is 1.563359375\n",
            "2024-01-14 10:39:46,807 - For batch 1300 loss at 500 samples is 1.575875\n",
            "2024-01-14 10:39:47,782 - For batch 1400 loss at 500 samples is 1.570640625\n",
            "2024-01-14 10:39:48,480 - LOSS train 1.570640625 valid 7.0703125\n",
            "2024-01-14 10:39:48,489 - Starting training at 186\n",
            "2024-01-14 10:39:49,435 - For batch 100 loss at 500 samples is 1.57946875\n",
            "2024-01-14 10:39:50,384 - For batch 200 loss at 500 samples is 1.5898359375\n",
            "2024-01-14 10:39:51,164 - For batch 300 loss at 500 samples is 1.586015625\n",
            "2024-01-14 10:39:51,833 - For batch 400 loss at 500 samples is 1.5742421875\n",
            "2024-01-14 10:39:52,487 - For batch 500 loss at 500 samples is 1.5735\n",
            "2024-01-14 10:39:53,188 - For batch 600 loss at 500 samples is 1.5863671875\n",
            "2024-01-14 10:39:53,866 - For batch 700 loss at 500 samples is 1.57240625\n",
            "2024-01-14 10:39:54,515 - For batch 800 loss at 500 samples is 1.5797890625\n",
            "2024-01-14 10:39:55,179 - For batch 900 loss at 500 samples is 1.5791796875\n",
            "2024-01-14 10:39:55,913 - For batch 1000 loss at 500 samples is 1.5834375\n",
            "2024-01-14 10:39:56,574 - For batch 1100 loss at 500 samples is 1.567484375\n",
            "2024-01-14 10:39:57,290 - For batch 1200 loss at 500 samples is 1.56343359375\n",
            "2024-01-14 10:39:57,958 - For batch 1300 loss at 500 samples is 1.5752578125\n",
            "2024-01-14 10:39:58,666 - For batch 1400 loss at 500 samples is 1.572546875\n",
            "2024-01-14 10:39:59,070 - LOSS train 1.572546875 valid 7.046875\n",
            "2024-01-14 10:39:59,073 - Starting training at 187\n",
            "2024-01-14 10:39:59,726 - For batch 100 loss at 500 samples is 1.577625\n",
            "2024-01-14 10:40:00,446 - For batch 200 loss at 500 samples is 1.5884921875\n",
            "2024-01-14 10:40:01,250 - For batch 300 loss at 500 samples is 1.5848359375\n",
            "2024-01-14 10:40:02,136 - For batch 400 loss at 500 samples is 1.575640625\n",
            "2024-01-14 10:40:03,125 - For batch 500 loss at 500 samples is 1.573421875\n",
            "2024-01-14 10:40:04,003 - For batch 600 loss at 500 samples is 1.5872421875\n",
            "2024-01-14 10:40:04,665 - For batch 700 loss at 500 samples is 1.5695859375\n",
            "2024-01-14 10:40:05,324 - For batch 800 loss at 500 samples is 1.5756171875\n",
            "2024-01-14 10:40:06,006 - For batch 900 loss at 500 samples is 1.5733203125\n",
            "2024-01-14 10:40:06,727 - For batch 1000 loss at 500 samples is 1.5870625\n",
            "2024-01-14 10:40:07,324 - For batch 1100 loss at 500 samples is 1.568828125\n",
            "2024-01-14 10:40:07,950 - For batch 1200 loss at 500 samples is 1.56591796875\n",
            "2024-01-14 10:40:08,645 - For batch 1300 loss at 500 samples is 1.5753046875\n",
            "2024-01-14 10:40:09,356 - For batch 1400 loss at 500 samples is 1.569359375\n",
            "2024-01-14 10:40:09,847 - LOSS train 1.569359375 valid 6.98828125\n",
            "2024-01-14 10:40:09,849 - Starting training at 188\n",
            "2024-01-14 10:40:10,477 - For batch 100 loss at 500 samples is 1.5781171875\n",
            "2024-01-14 10:40:11,111 - For batch 200 loss at 500 samples is 1.588625\n",
            "2024-01-14 10:40:11,718 - For batch 300 loss at 500 samples is 1.586234375\n",
            "2024-01-14 10:40:12,279 - For batch 400 loss at 500 samples is 1.5745390625\n",
            "2024-01-14 10:40:12,947 - For batch 500 loss at 500 samples is 1.5789453125\n",
            "2024-01-14 10:40:13,669 - For batch 600 loss at 500 samples is 1.58778125\n",
            "2024-01-14 10:40:14,463 - For batch 700 loss at 500 samples is 1.5709921875\n",
            "2024-01-14 10:40:15,398 - For batch 800 loss at 500 samples is 1.5769453125\n",
            "2024-01-14 10:40:16,274 - For batch 900 loss at 500 samples is 1.57459375\n",
            "2024-01-14 10:40:17,144 - For batch 1000 loss at 500 samples is 1.5819765625\n",
            "2024-01-14 10:40:17,820 - For batch 1100 loss at 500 samples is 1.5684609375\n",
            "2024-01-14 10:40:18,574 - For batch 1200 loss at 500 samples is 1.5630078125\n",
            "2024-01-14 10:40:19,342 - For batch 1300 loss at 500 samples is 1.5754609375\n",
            "2024-01-14 10:40:20,059 - For batch 1400 loss at 500 samples is 1.5686796875\n",
            "2024-01-14 10:40:20,528 - LOSS train 1.5686796875 valid 6.99609375\n",
            "2024-01-14 10:40:20,534 - Starting training at 189\n",
            "2024-01-14 10:40:21,212 - For batch 100 loss at 500 samples is 1.5781953125\n",
            "2024-01-14 10:40:21,890 - For batch 200 loss at 500 samples is 1.5888359375\n",
            "2024-01-14 10:40:22,527 - For batch 300 loss at 500 samples is 1.5853515625\n",
            "2024-01-14 10:40:23,194 - For batch 400 loss at 500 samples is 1.574265625\n",
            "2024-01-14 10:40:23,914 - For batch 500 loss at 500 samples is 1.5743046875\n",
            "2024-01-14 10:40:24,549 - For batch 600 loss at 500 samples is 1.5878125\n",
            "2024-01-14 10:40:25,284 - For batch 700 loss at 500 samples is 1.571140625\n",
            "2024-01-14 10:40:26,106 - For batch 800 loss at 500 samples is 1.577828125\n",
            "2024-01-14 10:40:26,731 - For batch 900 loss at 500 samples is 1.5775390625\n",
            "2024-01-14 10:40:27,450 - For batch 1000 loss at 500 samples is 1.5835\n",
            "2024-01-14 10:40:28,453 - For batch 1100 loss at 500 samples is 1.5731328125\n",
            "2024-01-14 10:40:29,413 - For batch 1200 loss at 500 samples is 1.564140625\n",
            "2024-01-14 10:40:30,346 - For batch 1300 loss at 500 samples is 1.5761953125\n",
            "2024-01-14 10:40:31,049 - For batch 1400 loss at 500 samples is 1.5788203125\n",
            "2024-01-14 10:40:31,574 - LOSS train 1.5788203125 valid 6.921875\n",
            "2024-01-14 10:40:31,577 - Starting training at 190\n",
            "2024-01-14 10:40:32,255 - For batch 100 loss at 500 samples is 1.5786171875\n",
            "2024-01-14 10:40:32,945 - For batch 200 loss at 500 samples is 1.5902734375\n",
            "2024-01-14 10:40:33,599 - For batch 300 loss at 500 samples is 1.586125\n",
            "2024-01-14 10:40:34,236 - For batch 400 loss at 500 samples is 1.575640625\n",
            "2024-01-14 10:40:35,012 - For batch 500 loss at 500 samples is 1.5734296875\n",
            "2024-01-14 10:40:35,692 - For batch 600 loss at 500 samples is 1.5872734375\n",
            "2024-01-14 10:40:36,450 - For batch 700 loss at 500 samples is 1.5727109375\n",
            "2024-01-14 10:40:37,133 - For batch 800 loss at 500 samples is 1.5803828125\n",
            "2024-01-14 10:40:37,844 - For batch 900 loss at 500 samples is 1.5757421875\n",
            "2024-01-14 10:40:38,522 - For batch 1000 loss at 500 samples is 1.5874140625\n",
            "2024-01-14 10:40:39,294 - For batch 1100 loss at 500 samples is 1.568171875\n",
            "2024-01-14 10:40:40,004 - For batch 1200 loss at 500 samples is 1.56494140625\n",
            "2024-01-14 10:40:40,807 - For batch 1300 loss at 500 samples is 1.576140625\n",
            "2024-01-14 10:40:41,732 - For batch 1400 loss at 500 samples is 1.5669921875\n",
            "2024-01-14 10:40:42,300 - LOSS train 1.5669921875 valid 6.89453125\n",
            "2024-01-14 10:40:42,302 - Starting training at 191\n",
            "2024-01-14 10:40:43,189 - For batch 100 loss at 500 samples is 1.5782265625\n",
            "2024-01-14 10:40:44,010 - For batch 200 loss at 500 samples is 1.5896953125\n",
            "2024-01-14 10:40:44,679 - For batch 300 loss at 500 samples is 1.5855078125\n",
            "2024-01-14 10:40:45,403 - For batch 400 loss at 500 samples is 1.574171875\n",
            "2024-01-14 10:40:45,983 - For batch 500 loss at 500 samples is 1.57959375\n",
            "2024-01-14 10:40:46,634 - For batch 600 loss at 500 samples is 1.58775\n",
            "2024-01-14 10:40:47,294 - For batch 700 loss at 500 samples is 1.570328125\n",
            "2024-01-14 10:40:47,968 - For batch 800 loss at 500 samples is 1.5764921875\n",
            "2024-01-14 10:40:48,626 - For batch 900 loss at 500 samples is 1.576078125\n",
            "2024-01-14 10:40:49,271 - For batch 1000 loss at 500 samples is 1.58309375\n",
            "2024-01-14 10:40:50,015 - For batch 1100 loss at 500 samples is 1.5693671875\n",
            "2024-01-14 10:40:50,668 - For batch 1200 loss at 500 samples is 1.56365234375\n",
            "2024-01-14 10:40:51,375 - For batch 1300 loss at 500 samples is 1.5766171875\n",
            "2024-01-14 10:40:52,040 - For batch 1400 loss at 500 samples is 1.5728671875\n",
            "2024-01-14 10:40:52,389 - LOSS train 1.5728671875 valid 6.859375\n",
            "2024-01-14 10:40:52,391 - Starting training at 192\n",
            "2024-01-14 10:40:53,039 - For batch 100 loss at 500 samples is 1.5786015625\n",
            "2024-01-14 10:40:53,661 - For batch 200 loss at 500 samples is 1.589859375\n",
            "2024-01-14 10:40:54,517 - For batch 300 loss at 500 samples is 1.58609375\n",
            "2024-01-14 10:40:55,468 - For batch 400 loss at 500 samples is 1.574890625\n",
            "2024-01-14 10:40:56,335 - For batch 500 loss at 500 samples is 1.5742578125\n",
            "2024-01-14 10:40:57,136 - For batch 600 loss at 500 samples is 1.58775\n",
            "2024-01-14 10:40:57,771 - For batch 700 loss at 500 samples is 1.57253125\n",
            "2024-01-14 10:40:58,458 - For batch 800 loss at 500 samples is 1.5794453125\n",
            "2024-01-14 10:40:59,132 - For batch 900 loss at 500 samples is 1.5760546875\n",
            "2024-01-14 10:40:59,742 - For batch 1000 loss at 500 samples is 1.5826328125\n",
            "2024-01-14 10:41:00,444 - For batch 1100 loss at 500 samples is 1.5705859375\n",
            "2024-01-14 10:41:01,034 - For batch 1200 loss at 500 samples is 1.5631796875\n",
            "2024-01-14 10:41:01,667 - For batch 1300 loss at 500 samples is 1.5764453125\n",
            "2024-01-14 10:41:02,346 - For batch 1400 loss at 500 samples is 1.572828125\n",
            "2024-01-14 10:41:02,776 - LOSS train 1.572828125 valid 6.828125\n",
            "2024-01-14 10:41:02,779 - Starting training at 193\n",
            "2024-01-14 10:41:03,422 - For batch 100 loss at 500 samples is 1.57834375\n",
            "2024-01-14 10:41:04,076 - For batch 200 loss at 500 samples is 1.5898125\n",
            "2024-01-14 10:41:04,768 - For batch 300 loss at 500 samples is 1.5852890625\n",
            "2024-01-14 10:41:05,535 - For batch 400 loss at 500 samples is 1.5749140625\n",
            "2024-01-14 10:41:06,197 - For batch 500 loss at 500 samples is 1.5742734375\n",
            "2024-01-14 10:41:06,864 - For batch 600 loss at 500 samples is 1.587515625\n",
            "2024-01-14 10:41:07,783 - For batch 700 loss at 500 samples is 1.5713203125\n",
            "2024-01-14 10:41:08,730 - For batch 800 loss at 500 samples is 1.577140625\n",
            "2024-01-14 10:41:09,700 - For batch 900 loss at 500 samples is 1.575828125\n",
            "2024-01-14 10:41:10,394 - For batch 1000 loss at 500 samples is 1.5859765625\n",
            "2024-01-14 10:41:11,163 - For batch 1100 loss at 500 samples is 1.5696640625\n",
            "2024-01-14 10:41:11,843 - For batch 1200 loss at 500 samples is 1.56465625\n",
            "2024-01-14 10:41:12,486 - For batch 1300 loss at 500 samples is 1.576859375\n",
            "2024-01-14 10:41:13,184 - For batch 1400 loss at 500 samples is 1.575125\n",
            "2024-01-14 10:41:13,634 - LOSS train 1.575125 valid 6.796875\n",
            "2024-01-14 10:41:13,637 - Starting training at 194\n",
            "2024-01-14 10:41:14,233 - For batch 100 loss at 500 samples is 1.5789921875\n",
            "2024-01-14 10:41:14,896 - For batch 200 loss at 500 samples is 1.589796875\n",
            "2024-01-14 10:41:15,517 - For batch 300 loss at 500 samples is 1.5862109375\n",
            "2024-01-14 10:41:16,195 - For batch 400 loss at 500 samples is 1.575421875\n",
            "2024-01-14 10:41:16,821 - For batch 500 loss at 500 samples is 1.5793671875\n",
            "2024-01-14 10:41:17,418 - For batch 600 loss at 500 samples is 1.5881484375\n",
            "2024-01-14 10:41:18,077 - For batch 700 loss at 500 samples is 1.5731796875\n",
            "2024-01-14 10:41:18,762 - For batch 800 loss at 500 samples is 1.58071875\n",
            "2024-01-14 10:41:19,412 - For batch 900 loss at 500 samples is 1.5797265625\n",
            "2024-01-14 10:41:20,065 - For batch 1000 loss at 500 samples is 1.5830703125\n",
            "2024-01-14 10:41:20,885 - For batch 1100 loss at 500 samples is 1.569921875\n",
            "2024-01-14 10:41:21,697 - For batch 1200 loss at 500 samples is 1.5634296875\n",
            "2024-01-14 10:41:22,667 - For batch 1300 loss at 500 samples is 1.5768515625\n",
            "2024-01-14 10:41:23,453 - For batch 1400 loss at 500 samples is 1.57409375\n",
            "2024-01-14 10:41:23,818 - LOSS train 1.57409375 valid 6.78515625\n",
            "2024-01-14 10:41:23,820 - Starting training at 195\n",
            "2024-01-14 10:41:24,515 - For batch 100 loss at 500 samples is 1.5787421875\n",
            "2024-01-14 10:41:25,116 - For batch 200 loss at 500 samples is 1.58984375\n",
            "2024-01-14 10:41:25,768 - For batch 300 loss at 500 samples is 1.58565625\n",
            "2024-01-14 10:41:26,486 - For batch 400 loss at 500 samples is 1.576015625\n",
            "2024-01-14 10:41:27,154 - For batch 500 loss at 500 samples is 1.5748984375\n",
            "2024-01-14 10:41:27,775 - For batch 600 loss at 500 samples is 1.5883046875\n",
            "2024-01-14 10:41:28,485 - For batch 700 loss at 500 samples is 1.570890625\n",
            "2024-01-14 10:41:29,118 - For batch 800 loss at 500 samples is 1.5766796875\n",
            "2024-01-14 10:41:29,714 - For batch 900 loss at 500 samples is 1.5762890625\n",
            "2024-01-14 10:41:30,356 - For batch 1000 loss at 500 samples is 1.5819140625\n",
            "2024-01-14 10:41:30,940 - For batch 1100 loss at 500 samples is 1.5706015625\n",
            "2024-01-14 10:41:31,670 - For batch 1200 loss at 500 samples is 1.56597265625\n",
            "2024-01-14 10:41:32,291 - For batch 1300 loss at 500 samples is 1.5765625\n",
            "2024-01-14 10:41:32,961 - For batch 1400 loss at 500 samples is 1.5754609375\n",
            "2024-01-14 10:41:33,535 - LOSS train 1.5754609375 valid 6.7265625\n",
            "2024-01-14 10:41:33,538 - Starting training at 196\n",
            "2024-01-14 10:41:34,465 - For batch 100 loss at 500 samples is 1.58009375\n",
            "2024-01-14 10:41:35,338 - For batch 200 loss at 500 samples is 1.5903828125\n",
            "2024-01-14 10:41:36,215 - For batch 300 loss at 500 samples is 1.5863828125\n",
            "2024-01-14 10:41:36,925 - For batch 400 loss at 500 samples is 1.575609375\n",
            "2024-01-14 10:41:37,579 - For batch 500 loss at 500 samples is 1.57465625\n",
            "2024-01-14 10:41:38,265 - For batch 600 loss at 500 samples is 1.588859375\n",
            "2024-01-14 10:41:38,925 - For batch 700 loss at 500 samples is 1.573515625\n",
            "2024-01-14 10:41:39,554 - For batch 800 loss at 500 samples is 1.577375\n",
            "2024-01-14 10:41:40,148 - For batch 900 loss at 500 samples is 1.576765625\n",
            "2024-01-14 10:41:40,848 - For batch 1000 loss at 500 samples is 1.5863359375\n",
            "2024-01-14 10:41:41,532 - For batch 1100 loss at 500 samples is 1.571703125\n",
            "2024-01-14 10:41:42,193 - For batch 1200 loss at 500 samples is 1.56341015625\n",
            "2024-01-14 10:41:42,886 - For batch 1300 loss at 500 samples is 1.577\n",
            "2024-01-14 10:41:43,671 - For batch 1400 loss at 500 samples is 1.57459375\n",
            "2024-01-14 10:41:44,082 - LOSS train 1.57459375 valid 6.68359375\n",
            "2024-01-14 10:41:44,085 - Starting training at 197\n",
            "2024-01-14 10:41:44,730 - For batch 100 loss at 500 samples is 1.5797578125\n",
            "2024-01-14 10:41:45,381 - For batch 200 loss at 500 samples is 1.5902421875\n",
            "2024-01-14 10:41:46,120 - For batch 300 loss at 500 samples is 1.586015625\n",
            "2024-01-14 10:41:46,932 - For batch 400 loss at 500 samples is 1.5753125\n",
            "2024-01-14 10:41:47,740 - For batch 500 loss at 500 samples is 1.57971875\n",
            "2024-01-14 10:41:48,623 - For batch 600 loss at 500 samples is 1.588390625\n",
            "2024-01-14 10:41:49,513 - For batch 700 loss at 500 samples is 1.5712421875\n",
            "2024-01-14 10:41:50,210 - For batch 800 loss at 500 samples is 1.5782734375\n",
            "2024-01-14 10:41:50,849 - For batch 900 loss at 500 samples is 1.5791953125\n",
            "2024-01-14 10:41:51,513 - For batch 1000 loss at 500 samples is 1.5818984375\n",
            "2024-01-14 10:41:52,183 - For batch 1100 loss at 500 samples is 1.5738359375\n",
            "2024-01-14 10:41:52,821 - For batch 1200 loss at 500 samples is 1.56334375\n",
            "2024-01-14 10:41:53,453 - For batch 1300 loss at 500 samples is 1.5769375\n",
            "2024-01-14 10:41:54,106 - For batch 1400 loss at 500 samples is 1.576171875\n",
            "2024-01-14 10:41:54,565 - LOSS train 1.576171875 valid 6.65625\n",
            "2024-01-14 10:41:54,568 - Starting training at 198\n",
            "2024-01-14 10:41:55,239 - For batch 100 loss at 500 samples is 1.5798359375\n",
            "2024-01-14 10:41:55,903 - For batch 200 loss at 500 samples is 1.5905\n",
            "2024-01-14 10:41:56,511 - For batch 300 loss at 500 samples is 1.5866796875\n",
            "2024-01-14 10:41:57,124 - For batch 400 loss at 500 samples is 1.575671875\n",
            "2024-01-14 10:41:57,738 - For batch 500 loss at 500 samples is 1.575125\n",
            "2024-01-14 10:41:58,384 - For batch 600 loss at 500 samples is 1.5889609375\n",
            "2024-01-14 10:41:59,090 - For batch 700 loss at 500 samples is 1.573296875\n",
            "2024-01-14 10:41:59,798 - For batch 800 loss at 500 samples is 1.58209375\n",
            "2024-01-14 10:42:00,625 - For batch 900 loss at 500 samples is 1.576578125\n",
            "2024-01-14 10:42:01,570 - For batch 1000 loss at 500 samples is 1.5841875\n",
            "2024-01-14 10:42:02,466 - For batch 1100 loss at 500 samples is 1.5734453125\n",
            "2024-01-14 10:42:03,233 - For batch 1200 loss at 500 samples is 1.5642109375\n",
            "2024-01-14 10:42:03,937 - For batch 1300 loss at 500 samples is 1.577390625\n",
            "2024-01-14 10:42:04,658 - For batch 1400 loss at 500 samples is 1.571421875\n",
            "2024-01-14 10:42:05,161 - LOSS train 1.571421875 valid 6.6328125\n",
            "2024-01-14 10:42:05,168 - Starting training at 199\n",
            "2024-01-14 10:42:05,781 - For batch 100 loss at 500 samples is 1.58025\n",
            "2024-01-14 10:42:06,422 - For batch 200 loss at 500 samples is 1.5903125\n",
            "2024-01-14 10:42:07,088 - For batch 300 loss at 500 samples is 1.5864453125\n",
            "2024-01-14 10:42:07,769 - For batch 400 loss at 500 samples is 1.57634375\n",
            "2024-01-14 10:42:08,492 - For batch 500 loss at 500 samples is 1.5754765625\n",
            "2024-01-14 10:42:09,156 - For batch 600 loss at 500 samples is 1.5886953125\n",
            "2024-01-14 10:42:09,821 - For batch 700 loss at 500 samples is 1.5720078125\n",
            "2024-01-14 10:42:10,535 - For batch 800 loss at 500 samples is 1.577515625\n",
            "2024-01-14 10:42:11,163 - For batch 900 loss at 500 samples is 1.579171875\n",
            "2024-01-14 10:42:11,805 - For batch 1000 loss at 500 samples is 1.5875859375\n",
            "2024-01-14 10:42:12,510 - For batch 1100 loss at 500 samples is 1.573640625\n",
            "2024-01-14 10:42:13,384 - For batch 1200 loss at 500 samples is 1.56573828125\n",
            "2024-01-14 10:42:14,269 - For batch 1300 loss at 500 samples is 1.577734375\n",
            "2024-01-14 10:42:15,156 - For batch 1400 loss at 500 samples is 1.578328125\n",
            "2024-01-14 10:42:15,834 - LOSS train 1.578328125 valid 6.58203125\n",
            "2024-01-14 10:42:15,836 - Starting training at 200\n",
            "2024-01-14 10:42:16,452 - For batch 100 loss at 500 samples is 1.5803125\n",
            "2024-01-14 10:42:17,040 - For batch 200 loss at 500 samples is 1.59\n",
            "2024-01-14 10:42:17,664 - For batch 300 loss at 500 samples is 1.5865\n",
            "2024-01-14 10:42:18,359 - For batch 400 loss at 500 samples is 1.5764375\n",
            "2024-01-14 10:42:19,110 - For batch 500 loss at 500 samples is 1.5798359375\n",
            "2024-01-14 10:42:19,820 - For batch 600 loss at 500 samples is 1.58984375\n",
            "2024-01-14 10:42:20,581 - For batch 700 loss at 500 samples is 1.5748125\n",
            "2024-01-14 10:42:21,228 - For batch 800 loss at 500 samples is 1.578484375\n",
            "2024-01-14 10:42:21,887 - For batch 900 loss at 500 samples is 1.5813046875\n",
            "2024-01-14 10:42:22,632 - For batch 1000 loss at 500 samples is 1.5837578125\n",
            "2024-01-14 10:42:23,351 - For batch 1100 loss at 500 samples is 1.5737109375\n",
            "2024-01-14 10:42:24,081 - For batch 1200 loss at 500 samples is 1.56503515625\n",
            "2024-01-14 10:42:24,846 - For batch 1300 loss at 500 samples is 1.5779609375\n",
            "2024-01-14 10:42:25,538 - For batch 1400 loss at 500 samples is 1.569953125\n",
            "2024-01-14 10:42:26,116 - LOSS train 1.569953125 valid 6.57421875\n",
            "2024-01-14 10:42:26,121 - Starting training at 201\n",
            "2024-01-14 10:42:27,033 - For batch 100 loss at 500 samples is 1.580875\n",
            "2024-01-14 10:42:28,005 - For batch 200 loss at 500 samples is 1.591078125\n",
            "2024-01-14 10:42:29,025 - For batch 300 loss at 500 samples is 1.586890625\n",
            "2024-01-14 10:42:29,760 - For batch 400 loss at 500 samples is 1.5762734375\n",
            "2024-01-14 10:42:30,408 - For batch 500 loss at 500 samples is 1.575015625\n",
            "2024-01-14 10:42:31,105 - For batch 600 loss at 500 samples is 1.589109375\n",
            "2024-01-14 10:42:31,795 - For batch 700 loss at 500 samples is 1.57209375\n",
            "2024-01-14 10:42:32,437 - For batch 800 loss at 500 samples is 1.5813984375\n",
            "2024-01-14 10:42:33,142 - For batch 900 loss at 500 samples is 1.5764453125\n",
            "2024-01-14 10:42:33,830 - For batch 1000 loss at 500 samples is 1.584078125\n",
            "2024-01-14 10:42:34,571 - For batch 1100 loss at 500 samples is 1.5743515625\n",
            "2024-01-14 10:42:35,187 - For batch 1200 loss at 500 samples is 1.567078125\n",
            "2024-01-14 10:42:35,872 - For batch 1300 loss at 500 samples is 1.577625\n",
            "2024-01-14 10:42:36,644 - For batch 1400 loss at 500 samples is 1.5765625\n",
            "2024-01-14 10:42:37,141 - LOSS train 1.5765625 valid 6.52734375\n",
            "2024-01-14 10:42:37,144 - Starting training at 202\n",
            "2024-01-14 10:42:37,737 - For batch 100 loss at 500 samples is 1.5808203125\n",
            "2024-01-14 10:42:38,399 - For batch 200 loss at 500 samples is 1.5906953125\n",
            "2024-01-14 10:42:39,104 - For batch 300 loss at 500 samples is 1.5861015625\n",
            "2024-01-14 10:42:39,974 - For batch 400 loss at 500 samples is 1.576\n",
            "2024-01-14 10:42:40,897 - For batch 500 loss at 500 samples is 1.5746640625\n",
            "2024-01-14 10:42:41,865 - For batch 600 loss at 500 samples is 1.5894140625\n",
            "2024-01-14 10:42:42,608 - For batch 700 loss at 500 samples is 1.5727578125\n",
            "2024-01-14 10:42:43,372 - For batch 800 loss at 500 samples is 1.5772890625\n",
            "2024-01-14 10:42:44,059 - For batch 900 loss at 500 samples is 1.58025\n",
            "2024-01-14 10:42:44,685 - For batch 1000 loss at 500 samples is 1.5883515625\n",
            "2024-01-14 10:42:45,366 - For batch 1100 loss at 500 samples is 1.5741015625\n",
            "2024-01-14 10:42:45,942 - For batch 1200 loss at 500 samples is 1.5649296875\n",
            "2024-01-14 10:42:46,573 - For batch 1300 loss at 500 samples is 1.578078125\n",
            "2024-01-14 10:42:47,213 - For batch 1400 loss at 500 samples is 1.5823515625\n",
            "2024-01-14 10:42:47,679 - LOSS train 1.5823515625 valid 6.5\n",
            "2024-01-14 10:42:47,682 - Starting training at 203\n",
            "2024-01-14 10:42:48,328 - For batch 100 loss at 500 samples is 1.5809609375\n",
            "2024-01-14 10:42:48,990 - For batch 200 loss at 500 samples is 1.591109375\n",
            "2024-01-14 10:42:49,655 - For batch 300 loss at 500 samples is 1.5869921875\n",
            "2024-01-14 10:42:50,280 - For batch 400 loss at 500 samples is 1.576703125\n",
            "2024-01-14 10:42:50,910 - For batch 500 loss at 500 samples is 1.5801796875\n",
            "2024-01-14 10:42:51,516 - For batch 600 loss at 500 samples is 1.58975\n",
            "2024-01-14 10:42:52,158 - For batch 700 loss at 500 samples is 1.5734609375\n",
            "2024-01-14 10:42:53,026 - For batch 800 loss at 500 samples is 1.5783984375\n",
            "2024-01-14 10:42:54,001 - For batch 900 loss at 500 samples is 1.5808203125\n",
            "2024-01-14 10:42:54,815 - For batch 1000 loss at 500 samples is 1.5845078125\n",
            "2024-01-14 10:42:55,713 - For batch 1100 loss at 500 samples is 1.574578125\n",
            "2024-01-14 10:42:56,334 - For batch 1200 loss at 500 samples is 1.565296875\n",
            "2024-01-14 10:42:56,956 - For batch 1300 loss at 500 samples is 1.5779140625\n",
            "2024-01-14 10:42:57,586 - For batch 1400 loss at 500 samples is 1.5820078125\n",
            "2024-01-14 10:42:57,999 - LOSS train 1.5820078125 valid 6.46484375\n",
            "2024-01-14 10:42:58,001 - Starting training at 204\n",
            "2024-01-14 10:42:58,612 - For batch 100 loss at 500 samples is 1.5809765625\n",
            "2024-01-14 10:42:59,325 - For batch 200 loss at 500 samples is 1.590765625\n",
            "2024-01-14 10:42:59,983 - For batch 300 loss at 500 samples is 1.5865625\n",
            "2024-01-14 10:43:00,702 - For batch 400 loss at 500 samples is 1.5762890625\n",
            "2024-01-14 10:43:01,295 - For batch 500 loss at 500 samples is 1.574984375\n",
            "2024-01-14 10:43:01,910 - For batch 600 loss at 500 samples is 1.5898828125\n",
            "2024-01-14 10:43:02,593 - For batch 700 loss at 500 samples is 1.57584375\n",
            "2024-01-14 10:43:03,218 - For batch 800 loss at 500 samples is 1.5819453125\n",
            "2024-01-14 10:43:03,891 - For batch 900 loss at 500 samples is 1.5781484375\n",
            "2024-01-14 10:43:04,516 - For batch 1000 loss at 500 samples is 1.582875\n",
            "2024-01-14 10:43:05,168 - For batch 1100 loss at 500 samples is 1.5735546875\n",
            "2024-01-14 10:43:05,965 - For batch 1200 loss at 500 samples is 1.56505078125\n",
            "2024-01-14 10:43:06,980 - For batch 1300 loss at 500 samples is 1.5781640625\n",
            "2024-01-14 10:43:07,907 - For batch 1400 loss at 500 samples is 1.576375\n",
            "2024-01-14 10:43:08,428 - LOSS train 1.576375 valid 6.44921875\n",
            "2024-01-14 10:43:08,430 - Starting training at 205\n",
            "2024-01-14 10:43:09,069 - For batch 100 loss at 500 samples is 1.5812421875\n",
            "2024-01-14 10:43:09,686 - For batch 200 loss at 500 samples is 1.591234375\n",
            "2024-01-14 10:43:10,327 - For batch 300 loss at 500 samples is 1.587546875\n",
            "2024-01-14 10:43:11,019 - For batch 400 loss at 500 samples is 1.5768203125\n",
            "2024-01-14 10:43:11,614 - For batch 500 loss at 500 samples is 1.575796875\n",
            "2024-01-14 10:43:12,230 - For batch 600 loss at 500 samples is 1.590109375\n",
            "2024-01-14 10:43:12,930 - For batch 700 loss at 500 samples is 1.57265625\n",
            "2024-01-14 10:43:13,553 - For batch 800 loss at 500 samples is 1.5781875\n",
            "2024-01-14 10:43:14,136 - For batch 900 loss at 500 samples is 1.5801875\n",
            "2024-01-14 10:43:14,786 - For batch 1000 loss at 500 samples is 1.5852421875\n",
            "2024-01-14 10:43:15,395 - For batch 1100 loss at 500 samples is 1.57496875\n",
            "2024-01-14 10:43:16,033 - For batch 1200 loss at 500 samples is 1.56864453125\n",
            "2024-01-14 10:43:16,717 - For batch 1300 loss at 500 samples is 1.5779140625\n",
            "2024-01-14 10:43:17,299 - For batch 1400 loss at 500 samples is 1.5803828125\n",
            "2024-01-14 10:43:17,735 - LOSS train 1.5803828125 valid 6.39453125\n",
            "2024-01-14 10:43:17,737 - Starting training at 206\n",
            "2024-01-14 10:43:18,330 - For batch 100 loss at 500 samples is 1.581328125\n",
            "2024-01-14 10:43:19,158 - For batch 200 loss at 500 samples is 1.5911796875\n",
            "2024-01-14 10:43:20,012 - For batch 300 loss at 500 samples is 1.587328125\n",
            "2024-01-14 10:43:20,954 - For batch 400 loss at 500 samples is 1.577171875\n",
            "2024-01-14 10:43:21,836 - For batch 500 loss at 500 samples is 1.5801484375\n",
            "2024-01-14 10:43:22,463 - For batch 600 loss at 500 samples is 1.5898515625\n",
            "2024-01-14 10:43:23,145 - For batch 700 loss at 500 samples is 1.5725546875\n",
            "2024-01-14 10:43:23,808 - For batch 800 loss at 500 samples is 1.57896875\n",
            "2024-01-14 10:43:24,405 - For batch 900 loss at 500 samples is 1.5814296875\n",
            "2024-01-14 10:43:25,013 - For batch 1000 loss at 500 samples is 1.58315625\n",
            "2024-01-14 10:43:25,662 - For batch 1100 loss at 500 samples is 1.5743828125\n",
            "2024-01-14 10:43:26,363 - For batch 1200 loss at 500 samples is 1.56501171875\n",
            "2024-01-14 10:43:27,015 - For batch 1300 loss at 500 samples is 1.5777265625\n",
            "2024-01-14 10:43:27,731 - For batch 1400 loss at 500 samples is 1.57240625\n",
            "2024-01-14 10:43:28,176 - LOSS train 1.57240625 valid 6.3671875\n",
            "2024-01-14 10:43:28,178 - Starting training at 207\n",
            "2024-01-14 10:43:28,785 - For batch 100 loss at 500 samples is 1.5811171875\n",
            "2024-01-14 10:43:29,452 - For batch 200 loss at 500 samples is 1.59096875\n",
            "2024-01-14 10:43:30,082 - For batch 300 loss at 500 samples is 1.5877265625\n",
            "2024-01-14 10:43:30,700 - For batch 400 loss at 500 samples is 1.5770078125\n",
            "2024-01-14 10:43:31,285 - For batch 500 loss at 500 samples is 1.5763515625\n",
            "2024-01-14 10:43:32,044 - For batch 600 loss at 500 samples is 1.5905390625\n",
            "2024-01-14 10:43:32,901 - For batch 700 loss at 500 samples is 1.5740078125\n",
            "2024-01-14 10:43:33,870 - For batch 800 loss at 500 samples is 1.5828515625\n",
            "2024-01-14 10:43:34,728 - For batch 900 loss at 500 samples is 1.57809375\n",
            "2024-01-14 10:43:35,427 - For batch 1000 loss at 500 samples is 1.58353125\n",
            "2024-01-14 10:43:36,055 - For batch 1100 loss at 500 samples is 1.5751875\n",
            "2024-01-14 10:43:36,732 - For batch 1200 loss at 500 samples is 1.56549609375\n",
            "2024-01-14 10:43:37,468 - For batch 1300 loss at 500 samples is 1.5792890625\n",
            "2024-01-14 10:43:38,154 - For batch 1400 loss at 500 samples is 1.5852421875\n",
            "2024-01-14 10:43:38,565 - LOSS train 1.5852421875 valid 6.33203125\n",
            "2024-01-14 10:43:38,571 - Starting training at 208\n",
            "2024-01-14 10:43:39,205 - For batch 100 loss at 500 samples is 1.58159375\n",
            "2024-01-14 10:43:39,822 - For batch 200 loss at 500 samples is 1.591390625\n",
            "2024-01-14 10:43:40,442 - For batch 300 loss at 500 samples is 1.58784375\n",
            "2024-01-14 10:43:41,133 - For batch 400 loss at 500 samples is 1.577390625\n",
            "2024-01-14 10:43:41,827 - For batch 500 loss at 500 samples is 1.575421875\n",
            "2024-01-14 10:43:42,555 - For batch 600 loss at 500 samples is 1.5907890625\n",
            "2024-01-14 10:43:43,187 - For batch 700 loss at 500 samples is 1.575359375\n",
            "2024-01-14 10:43:43,843 - For batch 800 loss at 500 samples is 1.5784609375\n",
            "2024-01-14 10:43:44,441 - For batch 900 loss at 500 samples is 1.5805234375\n",
            "2024-01-14 10:43:45,168 - For batch 1000 loss at 500 samples is 1.5834609375\n",
            "2024-01-14 10:43:46,029 - For batch 1100 loss at 500 samples is 1.5747265625\n",
            "2024-01-14 10:43:46,849 - For batch 1200 loss at 500 samples is 1.5656875\n",
            "2024-01-14 10:43:47,832 - For batch 1300 loss at 500 samples is 1.5783984375\n",
            "2024-01-14 10:43:48,523 - For batch 1400 loss at 500 samples is 1.58\n",
            "2024-01-14 10:43:48,972 - LOSS train 1.58 valid 6.3203125\n",
            "2024-01-14 10:43:48,975 - Starting training at 209\n",
            "2024-01-14 10:43:49,599 - For batch 100 loss at 500 samples is 1.581578125\n",
            "2024-01-14 10:43:50,244 - For batch 200 loss at 500 samples is 1.5915703125\n",
            "2024-01-14 10:43:50,939 - For batch 300 loss at 500 samples is 1.587921875\n",
            "2024-01-14 10:43:51,551 - For batch 400 loss at 500 samples is 1.5770625\n",
            "2024-01-14 10:43:52,231 - For batch 500 loss at 500 samples is 1.5800703125\n",
            "2024-01-14 10:43:52,903 - For batch 600 loss at 500 samples is 1.59034375\n",
            "2024-01-14 10:43:53,559 - For batch 700 loss at 500 samples is 1.57384375\n",
            "2024-01-14 10:43:54,236 - For batch 800 loss at 500 samples is 1.57946875\n",
            "2024-01-14 10:43:54,863 - For batch 900 loss at 500 samples is 1.582421875\n",
            "2024-01-14 10:43:55,495 - For batch 1000 loss at 500 samples is 1.5865078125\n",
            "2024-01-14 10:43:56,139 - For batch 1100 loss at 500 samples is 1.5773359375\n",
            "2024-01-14 10:43:56,805 - For batch 1200 loss at 500 samples is 1.56606640625\n",
            "2024-01-14 10:43:57,421 - For batch 1300 loss at 500 samples is 1.5788515625\n",
            "2024-01-14 10:43:58,146 - For batch 1400 loss at 500 samples is 1.5774921875\n",
            "2024-01-14 10:43:58,841 - LOSS train 1.5774921875 valid 6.27734375\n",
            "2024-01-14 10:43:58,843 - Starting training at 210\n",
            "2024-01-14 10:43:59,757 - For batch 100 loss at 500 samples is 1.582\n",
            "2024-01-14 10:44:00,546 - For batch 200 loss at 500 samples is 1.59159375\n",
            "2024-01-14 10:44:01,292 - For batch 300 loss at 500 samples is 1.5885546875\n",
            "2024-01-14 10:44:01,904 - For batch 400 loss at 500 samples is 1.5778984375\n",
            "2024-01-14 10:44:02,568 - For batch 500 loss at 500 samples is 1.5756796875\n",
            "2024-01-14 10:44:03,152 - For batch 600 loss at 500 samples is 1.59075\n",
            "2024-01-14 10:44:03,913 - For batch 700 loss at 500 samples is 1.574671875\n",
            "2024-01-14 10:44:04,510 - For batch 800 loss at 500 samples is 1.5834140625\n",
            "2024-01-14 10:44:05,162 - For batch 900 loss at 500 samples is 1.5781328125\n",
            "2024-01-14 10:44:05,873 - For batch 1000 loss at 500 samples is 1.583625\n",
            "2024-01-14 10:44:06,492 - For batch 1100 loss at 500 samples is 1.575359375\n",
            "2024-01-14 10:44:07,128 - For batch 1200 loss at 500 samples is 1.567171875\n",
            "2024-01-14 10:44:07,725 - For batch 1300 loss at 500 samples is 1.5786640625\n",
            "2024-01-14 10:44:08,335 - For batch 1400 loss at 500 samples is 1.576359375\n",
            "2024-01-14 10:44:08,746 - LOSS train 1.576359375 valid 6.2578125\n",
            "2024-01-14 10:44:08,749 - Starting training at 211\n",
            "2024-01-14 10:44:09,364 - For batch 100 loss at 500 samples is 1.5816796875\n",
            "2024-01-14 10:44:09,999 - For batch 200 loss at 500 samples is 1.5915\n",
            "2024-01-14 10:44:10,620 - For batch 300 loss at 500 samples is 1.588\n",
            "2024-01-14 10:44:11,384 - For batch 400 loss at 500 samples is 1.5773046875\n",
            "2024-01-14 10:44:12,197 - For batch 500 loss at 500 samples is 1.575546875\n",
            "2024-01-14 10:44:13,125 - For batch 600 loss at 500 samples is 1.590671875\n",
            "2024-01-14 10:44:13,995 - For batch 700 loss at 500 samples is 1.5751953125\n",
            "2024-01-14 10:44:14,710 - For batch 800 loss at 500 samples is 1.579046875\n",
            "2024-01-14 10:44:15,317 - For batch 900 loss at 500 samples is 1.580796875\n",
            "2024-01-14 10:44:15,944 - For batch 1000 loss at 500 samples is 1.5843515625\n",
            "2024-01-14 10:44:16,584 - For batch 1100 loss at 500 samples is 1.576609375\n",
            "2024-01-14 10:44:17,200 - For batch 1200 loss at 500 samples is 1.56573828125\n",
            "2024-01-14 10:44:17,841 - For batch 1300 loss at 500 samples is 1.5785078125\n",
            "2024-01-14 10:44:18,561 - For batch 1400 loss at 500 samples is 1.5824921875\n",
            "2024-01-14 10:44:18,975 - LOSS train 1.5824921875 valid 6.21875\n",
            "2024-01-14 10:44:18,977 - Starting training at 212\n",
            "2024-01-14 10:44:19,627 - For batch 100 loss at 500 samples is 1.5820078125\n",
            "2024-01-14 10:44:20,334 - For batch 200 loss at 500 samples is 1.591828125\n",
            "2024-01-14 10:44:20,926 - For batch 300 loss at 500 samples is 1.588640625\n",
            "2024-01-14 10:44:21,584 - For batch 400 loss at 500 samples is 1.578734375\n",
            "2024-01-14 10:44:22,264 - For batch 500 loss at 500 samples is 1.5801015625\n",
            "2024-01-14 10:44:22,932 - For batch 600 loss at 500 samples is 1.590828125\n",
            "2024-01-14 10:44:23,592 - For batch 700 loss at 500 samples is 1.577109375\n",
            "2024-01-14 10:44:24,227 - For batch 800 loss at 500 samples is 1.5806484375\n",
            "2024-01-14 10:44:25,029 - For batch 900 loss at 500 samples is 1.5819140625\n",
            "2024-01-14 10:44:25,831 - For batch 1000 loss at 500 samples is 1.5877578125\n",
            "2024-01-14 10:44:26,852 - For batch 1100 loss at 500 samples is 1.5754921875\n",
            "2024-01-14 10:44:27,665 - For batch 1200 loss at 500 samples is 1.56626171875\n",
            "2024-01-14 10:44:28,329 - For batch 1300 loss at 500 samples is 1.578953125\n",
            "2024-01-14 10:44:29,022 - For batch 1400 loss at 500 samples is 1.577671875\n",
            "2024-01-14 10:44:29,488 - LOSS train 1.577671875 valid 6.203125\n",
            "2024-01-14 10:44:29,490 - Starting training at 213\n",
            "2024-01-14 10:44:30,181 - For batch 100 loss at 500 samples is 1.5820703125\n",
            "2024-01-14 10:44:30,817 - For batch 200 loss at 500 samples is 1.5918671875\n",
            "2024-01-14 10:44:31,498 - For batch 300 loss at 500 samples is 1.5883984375\n",
            "2024-01-14 10:44:32,163 - For batch 400 loss at 500 samples is 1.577640625\n",
            "2024-01-14 10:44:32,818 - For batch 500 loss at 500 samples is 1.5766875\n",
            "2024-01-14 10:44:33,439 - For batch 600 loss at 500 samples is 1.59084375\n",
            "2024-01-14 10:44:34,092 - For batch 700 loss at 500 samples is 1.574234375\n",
            "2024-01-14 10:44:34,731 - For batch 800 loss at 500 samples is 1.584109375\n",
            "2024-01-14 10:44:35,434 - For batch 900 loss at 500 samples is 1.5788359375\n",
            "2024-01-14 10:44:36,127 - For batch 1000 loss at 500 samples is 1.58471875\n",
            "2024-01-14 10:44:36,901 - For batch 1100 loss at 500 samples is 1.5768125\n",
            "2024-01-14 10:44:37,703 - For batch 1200 loss at 500 samples is 1.56634765625\n",
            "2024-01-14 10:44:38,589 - For batch 1300 loss at 500 samples is 1.5786796875\n",
            "2024-01-14 10:44:39,484 - For batch 1400 loss at 500 samples is 1.5860546875\n",
            "2024-01-14 10:44:40,122 - LOSS train 1.5860546875 valid 6.16015625\n",
            "2024-01-14 10:44:40,131 - Starting training at 214\n",
            "2024-01-14 10:44:40,919 - For batch 100 loss at 500 samples is 1.5820078125\n",
            "2024-01-14 10:44:41,591 - For batch 200 loss at 500 samples is 1.591125\n",
            "2024-01-14 10:44:42,263 - For batch 300 loss at 500 samples is 1.588078125\n",
            "2024-01-14 10:44:42,920 - For batch 400 loss at 500 samples is 1.5773203125\n",
            "2024-01-14 10:44:43,506 - For batch 500 loss at 500 samples is 1.5756875\n",
            "2024-01-14 10:44:44,138 - For batch 600 loss at 500 samples is 1.590453125\n",
            "2024-01-14 10:44:44,791 - For batch 700 loss at 500 samples is 1.57453125\n",
            "2024-01-14 10:44:45,386 - For batch 800 loss at 500 samples is 1.5797734375\n",
            "2024-01-14 10:44:46,020 - For batch 900 loss at 500 samples is 1.5809609375\n",
            "2024-01-14 10:44:46,679 - For batch 1000 loss at 500 samples is 1.585234375\n",
            "2024-01-14 10:44:47,291 - For batch 1100 loss at 500 samples is 1.5762265625\n",
            "2024-01-14 10:44:47,982 - For batch 1200 loss at 500 samples is 1.5681484375\n",
            "2024-01-14 10:44:48,704 - For batch 1300 loss at 500 samples is 1.57921875\n",
            "2024-01-14 10:44:49,361 - For batch 1400 loss at 500 samples is 1.5821484375\n",
            "2024-01-14 10:44:49,746 - LOSS train 1.5821484375 valid 6.140625\n",
            "2024-01-14 10:44:49,749 - Starting training at 215\n",
            "2024-01-14 10:44:50,401 - For batch 100 loss at 500 samples is 1.5823515625\n",
            "2024-01-14 10:44:51,144 - For batch 200 loss at 500 samples is 1.591625\n",
            "2024-01-14 10:44:52,032 - For batch 300 loss at 500 samples is 1.588453125\n",
            "2024-01-14 10:44:52,895 - For batch 400 loss at 500 samples is 1.578171875\n",
            "2024-01-14 10:44:53,711 - For batch 500 loss at 500 samples is 1.57990625\n",
            "2024-01-14 10:44:54,474 - For batch 600 loss at 500 samples is 1.5911328125\n",
            "2024-01-14 10:44:55,180 - For batch 700 loss at 500 samples is 1.5754453125\n",
            "2024-01-14 10:44:55,796 - For batch 800 loss at 500 samples is 1.5809453125\n",
            "2024-01-14 10:44:56,427 - For batch 900 loss at 500 samples is 1.58246875\n",
            "2024-01-14 10:44:57,117 - For batch 1000 loss at 500 samples is 1.5884765625\n",
            "2024-01-14 10:44:57,791 - For batch 1100 loss at 500 samples is 1.57696875\n",
            "2024-01-14 10:44:58,546 - For batch 1200 loss at 500 samples is 1.56628125\n",
            "2024-01-14 10:44:59,170 - For batch 1300 loss at 500 samples is 1.5789921875\n",
            "2024-01-14 10:44:59,753 - For batch 1400 loss at 500 samples is 1.5799296875\n",
            "2024-01-14 10:45:00,187 - LOSS train 1.5799296875 valid 6.09765625\n",
            "2024-01-14 10:45:00,190 - Starting training at 216\n",
            "2024-01-14 10:45:00,783 - For batch 100 loss at 500 samples is 1.5824765625\n",
            "2024-01-14 10:45:01,388 - For batch 200 loss at 500 samples is 1.5915546875\n",
            "2024-01-14 10:45:02,031 - For batch 300 loss at 500 samples is 1.588703125\n",
            "2024-01-14 10:45:02,688 - For batch 400 loss at 500 samples is 1.5778671875\n",
            "2024-01-14 10:45:03,371 - For batch 500 loss at 500 samples is 1.5763203125\n",
            "2024-01-14 10:45:04,066 - For batch 600 loss at 500 samples is 1.590859375\n",
            "2024-01-14 10:45:04,942 - For batch 700 loss at 500 samples is 1.577421875\n",
            "2024-01-14 10:45:05,825 - For batch 800 loss at 500 samples is 1.5839453125\n",
            "2024-01-14 10:45:06,747 - For batch 900 loss at 500 samples is 1.578203125\n",
            "2024-01-14 10:45:07,493 - For batch 1000 loss at 500 samples is 1.58534375\n",
            "2024-01-14 10:45:08,122 - For batch 1100 loss at 500 samples is 1.576078125\n",
            "2024-01-14 10:45:08,813 - For batch 1200 loss at 500 samples is 1.5660078125\n",
            "2024-01-14 10:45:09,415 - For batch 1300 loss at 500 samples is 1.578921875\n",
            "2024-01-14 10:45:10,134 - For batch 1400 loss at 500 samples is 1.5813984375\n",
            "2024-01-14 10:45:10,559 - LOSS train 1.5813984375 valid 6.08984375\n",
            "2024-01-14 10:45:10,561 - Starting training at 217\n",
            "2024-01-14 10:45:11,187 - For batch 100 loss at 500 samples is 1.5826875\n",
            "2024-01-14 10:45:11,881 - For batch 200 loss at 500 samples is 1.5926796875\n",
            "2024-01-14 10:45:12,579 - For batch 300 loss at 500 samples is 1.5889453125\n",
            "2024-01-14 10:45:13,207 - For batch 400 loss at 500 samples is 1.57915625\n",
            "2024-01-14 10:45:13,894 - For batch 500 loss at 500 samples is 1.5784296875\n",
            "2024-01-14 10:45:14,536 - For batch 600 loss at 500 samples is 1.591125\n",
            "2024-01-14 10:45:15,169 - For batch 700 loss at 500 samples is 1.5746484375\n",
            "2024-01-14 10:45:15,894 - For batch 800 loss at 500 samples is 1.5799609375\n",
            "2024-01-14 10:45:16,571 - For batch 900 loss at 500 samples is 1.5809765625\n",
            "2024-01-14 10:45:17,376 - For batch 1000 loss at 500 samples is 1.5847265625\n",
            "2024-01-14 10:45:18,352 - For batch 1100 loss at 500 samples is 1.577546875\n",
            "2024-01-14 10:45:19,296 - For batch 1200 loss at 500 samples is 1.56690234375\n",
            "2024-01-14 10:45:20,210 - For batch 1300 loss at 500 samples is 1.5795859375\n",
            "2024-01-14 10:45:20,911 - For batch 1400 loss at 500 samples is 1.5899140625\n",
            "2024-01-14 10:45:21,392 - LOSS train 1.5899140625 valid 6.046875\n",
            "2024-01-14 10:45:21,394 - Starting training at 218\n",
            "2024-01-14 10:45:22,025 - For batch 100 loss at 500 samples is 1.58271875\n",
            "2024-01-14 10:45:22,635 - For batch 200 loss at 500 samples is 1.592046875\n",
            "2024-01-14 10:45:23,276 - For batch 300 loss at 500 samples is 1.5890546875\n",
            "2024-01-14 10:45:23,926 - For batch 400 loss at 500 samples is 1.578921875\n",
            "2024-01-14 10:45:24,553 - For batch 500 loss at 500 samples is 1.5795\n",
            "2024-01-14 10:45:25,230 - For batch 600 loss at 500 samples is 1.5911875\n",
            "2024-01-14 10:45:25,869 - For batch 700 loss at 500 samples is 1.5743828125\n",
            "2024-01-14 10:45:26,603 - For batch 800 loss at 500 samples is 1.5809609375\n",
            "2024-01-14 10:45:27,270 - For batch 900 loss at 500 samples is 1.5824609375\n",
            "2024-01-14 10:45:27,946 - For batch 1000 loss at 500 samples is 1.5850390625\n",
            "2024-01-14 10:45:28,584 - For batch 1100 loss at 500 samples is 1.5763359375\n",
            "2024-01-14 10:45:29,196 - For batch 1200 loss at 500 samples is 1.5661484375\n",
            "2024-01-14 10:45:29,931 - For batch 1300 loss at 500 samples is 1.57925\n",
            "2024-01-14 10:45:30,775 - For batch 1400 loss at 500 samples is 1.5789765625\n",
            "2024-01-14 10:45:31,432 - LOSS train 1.5789765625 valid 6.02734375\n",
            "2024-01-14 10:45:31,436 - Starting training at 219\n",
            "2024-01-14 10:45:32,270 - For batch 100 loss at 500 samples is 1.5826640625\n",
            "2024-01-14 10:45:33,188 - For batch 200 loss at 500 samples is 1.5921640625\n",
            "2024-01-14 10:45:33,894 - For batch 300 loss at 500 samples is 1.5889453125\n",
            "2024-01-14 10:45:34,499 - For batch 400 loss at 500 samples is 1.5786328125\n",
            "2024-01-14 10:45:35,155 - For batch 500 loss at 500 samples is 1.5761875\n",
            "2024-01-14 10:45:35,763 - For batch 600 loss at 500 samples is 1.5909453125\n",
            "2024-01-14 10:45:36,425 - For batch 700 loss at 500 samples is 1.5746796875\n",
            "2024-01-14 10:45:37,121 - For batch 800 loss at 500 samples is 1.5840546875\n",
            "2024-01-14 10:45:37,793 - For batch 900 loss at 500 samples is 1.5799375\n",
            "2024-01-14 10:45:38,435 - For batch 1000 loss at 500 samples is 1.5854140625\n",
            "2024-01-14 10:45:39,114 - For batch 1100 loss at 500 samples is 1.577328125\n",
            "2024-01-14 10:45:39,787 - For batch 1200 loss at 500 samples is 1.56737109375\n",
            "2024-01-14 10:45:40,439 - For batch 1300 loss at 500 samples is 1.5795546875\n",
            "2024-01-14 10:45:41,093 - For batch 1400 loss at 500 samples is 1.5854375\n",
            "2024-01-14 10:45:41,587 - LOSS train 1.5854375 valid 5.984375\n",
            "2024-01-14 10:45:41,590 - Starting training at 220\n",
            "2024-01-14 10:45:42,243 - For batch 100 loss at 500 samples is 1.5826640625\n",
            "2024-01-14 10:45:42,853 - For batch 200 loss at 500 samples is 1.5925078125\n",
            "2024-01-14 10:45:43,503 - For batch 300 loss at 500 samples is 1.5892578125\n",
            "2024-01-14 10:45:44,463 - For batch 400 loss at 500 samples is 1.579921875\n",
            "2024-01-14 10:45:45,439 - For batch 500 loss at 500 samples is 1.578546875\n",
            "2024-01-14 10:45:46,261 - For batch 600 loss at 500 samples is 1.5914609375\n",
            "2024-01-14 10:45:47,048 - For batch 700 loss at 500 samples is 1.5775234375\n",
            "2024-01-14 10:45:47,701 - For batch 800 loss at 500 samples is 1.580515625\n",
            "2024-01-14 10:45:48,370 - For batch 900 loss at 500 samples is 1.581109375\n",
            "2024-01-14 10:45:49,079 - For batch 1000 loss at 500 samples is 1.58778125\n",
            "2024-01-14 10:45:49,724 - For batch 1100 loss at 500 samples is 1.576765625\n",
            "2024-01-14 10:45:50,375 - For batch 1200 loss at 500 samples is 1.56684375\n",
            "2024-01-14 10:45:50,986 - For batch 1300 loss at 500 samples is 1.5796875\n",
            "2024-01-14 10:45:51,658 - For batch 1400 loss at 500 samples is 1.5840234375\n",
            "2024-01-14 10:45:52,073 - LOSS train 1.5840234375 valid 5.98046875\n",
            "2024-01-14 10:45:52,080 - Starting training at 221\n",
            "2024-01-14 10:45:52,653 - For batch 100 loss at 500 samples is 1.582703125\n",
            "2024-01-14 10:45:53,331 - For batch 200 loss at 500 samples is 1.5925625\n",
            "2024-01-14 10:45:53,983 - For batch 300 loss at 500 samples is 1.589078125\n",
            "2024-01-14 10:45:54,623 - For batch 400 loss at 500 samples is 1.5791015625\n",
            "2024-01-14 10:45:55,283 - For batch 500 loss at 500 samples is 1.578109375\n",
            "2024-01-14 10:45:55,967 - For batch 600 loss at 500 samples is 1.591078125\n",
            "2024-01-14 10:45:56,657 - For batch 700 loss at 500 samples is 1.5740078125\n",
            "2024-01-14 10:45:57,601 - For batch 800 loss at 500 samples is 1.5813125\n",
            "2024-01-14 10:45:58,611 - For batch 900 loss at 500 samples is 1.5826171875\n",
            "2024-01-14 10:45:59,420 - For batch 1000 loss at 500 samples is 1.585578125\n",
            "2024-01-14 10:46:00,199 - For batch 1100 loss at 500 samples is 1.5780625\n",
            "2024-01-14 10:46:00,794 - For batch 1200 loss at 500 samples is 1.5689609375\n",
            "2024-01-14 10:46:01,420 - For batch 1300 loss at 500 samples is 1.5796171875\n",
            "2024-01-14 10:46:02,118 - For batch 1400 loss at 500 samples is 1.583078125\n",
            "2024-01-14 10:46:02,506 - LOSS train 1.583078125 valid 5.94140625\n",
            "2024-01-14 10:46:02,508 - Starting training at 222\n",
            "2024-01-14 10:46:03,133 - For batch 100 loss at 500 samples is 1.5824765625\n",
            "2024-01-14 10:46:03,826 - For batch 200 loss at 500 samples is 1.5920390625\n",
            "2024-01-14 10:46:04,449 - For batch 300 loss at 500 samples is 1.5890625\n",
            "2024-01-14 10:46:05,058 - For batch 400 loss at 500 samples is 1.5795703125\n",
            "2024-01-14 10:46:05,724 - For batch 500 loss at 500 samples is 1.576859375\n",
            "2024-01-14 10:46:06,444 - For batch 600 loss at 500 samples is 1.591640625\n",
            "2024-01-14 10:46:07,133 - For batch 700 loss at 500 samples is 1.57490625\n",
            "2024-01-14 10:46:07,851 - For batch 800 loss at 500 samples is 1.585078125\n",
            "2024-01-14 10:46:08,510 - For batch 900 loss at 500 samples is 1.57925\n",
            "2024-01-14 10:46:09,182 - For batch 1000 loss at 500 samples is 1.5857421875\n",
            "2024-01-14 10:46:09,875 - For batch 1100 loss at 500 samples is 1.5770546875\n",
            "2024-01-14 10:46:10,718 - For batch 1200 loss at 500 samples is 1.5666171875\n",
            "2024-01-14 10:46:11,665 - For batch 1300 loss at 500 samples is 1.5801484375\n",
            "2024-01-14 10:46:12,596 - For batch 1400 loss at 500 samples is 1.5798671875\n",
            "2024-01-14 10:46:13,148 - LOSS train 1.5798671875 valid 5.91796875\n",
            "2024-01-14 10:46:13,151 - Starting training at 223\n",
            "2024-01-14 10:46:13,754 - For batch 100 loss at 500 samples is 1.58334375\n",
            "2024-01-14 10:46:14,329 - For batch 200 loss at 500 samples is 1.5926171875\n",
            "2024-01-14 10:46:14,977 - For batch 300 loss at 500 samples is 1.5894921875\n",
            "2024-01-14 10:46:15,558 - For batch 400 loss at 500 samples is 1.5794921875\n",
            "2024-01-14 10:46:16,218 - For batch 500 loss at 500 samples is 1.579546875\n",
            "2024-01-14 10:46:16,814 - For batch 600 loss at 500 samples is 1.5914765625\n",
            "2024-01-14 10:46:17,415 - For batch 700 loss at 500 samples is 1.5753046875\n",
            "2024-01-14 10:46:18,030 - For batch 800 loss at 500 samples is 1.5809140625\n",
            "2024-01-14 10:46:18,681 - For batch 900 loss at 500 samples is 1.58134375\n",
            "2024-01-14 10:46:19,265 - For batch 1000 loss at 500 samples is 1.5883515625\n",
            "2024-01-14 10:46:19,975 - For batch 1100 loss at 500 samples is 1.5772890625\n",
            "2024-01-14 10:46:20,610 - For batch 1200 loss at 500 samples is 1.5668828125\n",
            "2024-01-14 10:46:21,242 - For batch 1300 loss at 500 samples is 1.580125\n",
            "2024-01-14 10:46:21,872 - For batch 1400 loss at 500 samples is 1.585078125\n",
            "2024-01-14 10:46:22,336 - LOSS train 1.585078125 valid 5.8828125\n",
            "2024-01-14 10:46:22,339 - Starting training at 224\n",
            "2024-01-14 10:46:22,953 - For batch 100 loss at 500 samples is 1.582984375\n",
            "2024-01-14 10:46:23,959 - For batch 200 loss at 500 samples is 1.59228125\n",
            "2024-01-14 10:46:24,835 - For batch 300 loss at 500 samples is 1.589546875\n",
            "2024-01-14 10:46:25,697 - For batch 400 loss at 500 samples is 1.579640625\n",
            "2024-01-14 10:46:26,422 - For batch 500 loss at 500 samples is 1.577625\n",
            "2024-01-14 10:46:27,023 - For batch 600 loss at 500 samples is 1.5914296875\n",
            "2024-01-14 10:46:27,660 - For batch 700 loss at 500 samples is 1.5773828125\n",
            "2024-01-14 10:46:28,309 - For batch 800 loss at 500 samples is 1.5814765625\n",
            "2024-01-14 10:46:28,929 - For batch 900 loss at 500 samples is 1.5830234375\n",
            "2024-01-14 10:46:29,538 - For batch 1000 loss at 500 samples is 1.5858046875\n",
            "2024-01-14 10:46:30,180 - For batch 1100 loss at 500 samples is 1.577296875\n",
            "2024-01-14 10:46:30,821 - For batch 1200 loss at 500 samples is 1.56862890625\n",
            "2024-01-14 10:46:31,455 - For batch 1300 loss at 500 samples is 1.580234375\n",
            "2024-01-14 10:46:32,105 - For batch 1400 loss at 500 samples is 1.58190625\n",
            "2024-01-14 10:46:32,486 - LOSS train 1.58190625 valid 5.8828125\n",
            "2024-01-14 10:46:32,488 - Starting training at 225\n",
            "2024-01-14 10:46:33,137 - For batch 100 loss at 500 samples is 1.5836484375\n",
            "2024-01-14 10:46:33,762 - For batch 200 loss at 500 samples is 1.59309375\n",
            "2024-01-14 10:46:34,374 - For batch 300 loss at 500 samples is 1.5900625\n",
            "2024-01-14 10:46:35,041 - For batch 400 loss at 500 samples is 1.5800078125\n",
            "2024-01-14 10:46:35,635 - For batch 500 loss at 500 samples is 1.5768359375\n",
            "2024-01-14 10:46:36,412 - For batch 600 loss at 500 samples is 1.59184375\n",
            "2024-01-14 10:46:37,437 - For batch 700 loss at 500 samples is 1.5758203125\n",
            "2024-01-14 10:46:38,343 - For batch 800 loss at 500 samples is 1.58565625\n",
            "2024-01-14 10:46:39,207 - For batch 900 loss at 500 samples is 1.5794140625\n",
            "2024-01-14 10:46:39,852 - For batch 1000 loss at 500 samples is 1.5860078125\n",
            "2024-01-14 10:46:40,437 - For batch 1100 loss at 500 samples is 1.5786484375\n",
            "2024-01-14 10:46:41,148 - For batch 1200 loss at 500 samples is 1.569\n",
            "2024-01-14 10:46:41,749 - For batch 1300 loss at 500 samples is 1.5804140625\n",
            "2024-01-14 10:46:42,318 - For batch 1400 loss at 500 samples is 1.582921875\n",
            "2024-01-14 10:46:42,761 - LOSS train 1.582921875 valid 5.83984375\n",
            "2024-01-14 10:46:42,763 - Starting training at 226\n",
            "2024-01-14 10:46:43,346 - For batch 100 loss at 500 samples is 1.583390625\n",
            "2024-01-14 10:46:43,955 - For batch 200 loss at 500 samples is 1.592671875\n",
            "2024-01-14 10:46:44,560 - For batch 300 loss at 500 samples is 1.5897734375\n",
            "2024-01-14 10:46:45,171 - For batch 400 loss at 500 samples is 1.580453125\n",
            "2024-01-14 10:46:45,804 - For batch 500 loss at 500 samples is 1.57775\n",
            "2024-01-14 10:46:46,398 - For batch 600 loss at 500 samples is 1.59128125\n",
            "2024-01-14 10:46:47,009 - For batch 700 loss at 500 samples is 1.57721875\n",
            "2024-01-14 10:46:47,597 - For batch 800 loss at 500 samples is 1.582078125\n",
            "2024-01-14 10:46:48,278 - For batch 900 loss at 500 samples is 1.58196875\n",
            "2024-01-14 10:46:48,927 - For batch 1000 loss at 500 samples is 1.5873515625\n",
            "2024-01-14 10:46:49,626 - For batch 1100 loss at 500 samples is 1.57759375\n",
            "2024-01-14 10:46:50,607 - For batch 1200 loss at 500 samples is 1.5673984375\n",
            "2024-01-14 10:46:51,525 - For batch 1300 loss at 500 samples is 1.5803125\n",
            "2024-01-14 10:46:52,532 - For batch 1400 loss at 500 samples is 1.583375\n",
            "2024-01-14 10:46:52,934 - LOSS train 1.583375 valid 5.8125\n",
            "2024-01-14 10:46:52,936 - Starting training at 227\n",
            "2024-01-14 10:46:53,546 - For batch 100 loss at 500 samples is 1.583453125\n",
            "2024-01-14 10:46:54,286 - For batch 200 loss at 500 samples is 1.5929296875\n",
            "2024-01-14 10:46:54,978 - For batch 300 loss at 500 samples is 1.589734375\n",
            "2024-01-14 10:46:55,589 - For batch 400 loss at 500 samples is 1.57975\n",
            "2024-01-14 10:46:56,163 - For batch 500 loss at 500 samples is 1.578875\n",
            "2024-01-14 10:46:56,759 - For batch 600 loss at 500 samples is 1.59115625\n",
            "2024-01-14 10:46:57,409 - For batch 700 loss at 500 samples is 1.575640625\n",
            "2024-01-14 10:46:58,127 - For batch 800 loss at 500 samples is 1.582453125\n",
            "2024-01-14 10:46:58,771 - For batch 900 loss at 500 samples is 1.5835234375\n",
            "2024-01-14 10:46:59,436 - For batch 1000 loss at 500 samples is 1.5861796875\n",
            "2024-01-14 10:47:00,077 - For batch 1100 loss at 500 samples is 1.5783984375\n",
            "2024-01-14 10:47:00,735 - For batch 1200 loss at 500 samples is 1.56868359375\n",
            "2024-01-14 10:47:01,381 - For batch 1300 loss at 500 samples is 1.5806640625\n",
            "2024-01-14 10:47:02,022 - For batch 1400 loss at 500 samples is 1.581984375\n",
            "2024-01-14 10:47:02,512 - LOSS train 1.581984375 valid 5.78125\n",
            "2024-01-14 10:47:02,514 - Starting training at 228\n",
            "2024-01-14 10:47:03,418 - For batch 100 loss at 500 samples is 1.58375\n",
            "2024-01-14 10:47:04,251 - For batch 200 loss at 500 samples is 1.593265625\n",
            "2024-01-14 10:47:05,100 - For batch 300 loss at 500 samples is 1.590078125\n",
            "2024-01-14 10:47:05,874 - For batch 400 loss at 500 samples is 1.5803671875\n",
            "2024-01-14 10:47:06,473 - For batch 500 loss at 500 samples is 1.5765859375\n",
            "2024-01-14 10:47:07,099 - For batch 600 loss at 500 samples is 1.59196875\n",
            "2024-01-14 10:47:07,739 - For batch 700 loss at 500 samples is 1.57784375\n",
            "2024-01-14 10:47:08,472 - For batch 800 loss at 500 samples is 1.5861640625\n",
            "2024-01-14 10:47:09,115 - For batch 900 loss at 500 samples is 1.579984375\n",
            "2024-01-14 10:47:09,744 - For batch 1000 loss at 500 samples is 1.586046875\n",
            "2024-01-14 10:47:10,333 - For batch 1100 loss at 500 samples is 1.5787109375\n",
            "2024-01-14 10:47:10,925 - For batch 1200 loss at 500 samples is 1.56790234375\n",
            "2024-01-14 10:47:11,599 - For batch 1300 loss at 500 samples is 1.5805703125\n",
            "2024-01-14 10:47:12,225 - For batch 1400 loss at 500 samples is 1.58590625\n",
            "2024-01-14 10:47:12,674 - LOSS train 1.58590625 valid 5.78125\n",
            "2024-01-14 10:47:12,675 - Starting training at 229\n",
            "2024-01-14 10:47:13,276 - For batch 100 loss at 500 samples is 1.58375\n",
            "2024-01-14 10:47:13,977 - For batch 200 loss at 500 samples is 1.593171875\n",
            "2024-01-14 10:47:14,547 - For batch 300 loss at 500 samples is 1.5899921875\n",
            "2024-01-14 10:47:15,169 - For batch 400 loss at 500 samples is 1.5803203125\n",
            "2024-01-14 10:47:15,958 - For batch 500 loss at 500 samples is 1.57671875\n",
            "2024-01-14 10:47:16,815 - For batch 600 loss at 500 samples is 1.59184375\n",
            "2024-01-14 10:47:17,723 - For batch 700 loss at 500 samples is 1.5758046875\n",
            "2024-01-14 10:47:18,663 - For batch 800 loss at 500 samples is 1.5818828125\n",
            "2024-01-14 10:47:19,329 - For batch 900 loss at 500 samples is 1.581546875\n",
            "2024-01-14 10:47:19,963 - For batch 1000 loss at 500 samples is 1.5877890625\n",
            "2024-01-14 10:47:20,587 - For batch 1100 loss at 500 samples is 1.57809375\n",
            "2024-01-14 10:47:21,249 - For batch 1200 loss at 500 samples is 1.56880078125\n",
            "2024-01-14 10:47:21,873 - For batch 1300 loss at 500 samples is 1.5806015625\n",
            "2024-01-14 10:47:22,448 - For batch 1400 loss at 500 samples is 1.5847265625\n",
            "2024-01-14 10:47:22,924 - LOSS train 1.5847265625 valid 5.73046875\n",
            "2024-01-14 10:47:22,929 - Starting training at 230\n",
            "2024-01-14 10:47:23,605 - For batch 100 loss at 500 samples is 1.5836953125\n",
            "2024-01-14 10:47:24,237 - For batch 200 loss at 500 samples is 1.5930390625\n",
            "2024-01-14 10:47:24,885 - For batch 300 loss at 500 samples is 1.589859375\n",
            "2024-01-14 10:47:25,504 - For batch 400 loss at 500 samples is 1.580609375\n",
            "2024-01-14 10:47:26,178 - For batch 500 loss at 500 samples is 1.5784296875\n",
            "2024-01-14 10:47:26,789 - For batch 600 loss at 500 samples is 1.592515625\n",
            "2024-01-14 10:47:27,482 - For batch 700 loss at 500 samples is 1.576359375\n",
            "2024-01-14 10:47:28,162 - For batch 800 loss at 500 samples is 1.5830625\n",
            "2024-01-14 10:47:28,909 - For batch 900 loss at 500 samples is 1.5842890625\n",
            "2024-01-14 10:47:29,890 - For batch 1000 loss at 500 samples is 1.5863828125\n",
            "2024-01-14 10:47:30,822 - For batch 1100 loss at 500 samples is 1.5792421875\n",
            "2024-01-14 10:47:31,743 - For batch 1200 loss at 500 samples is 1.56796875\n",
            "2024-01-14 10:47:32,466 - For batch 1300 loss at 500 samples is 1.5811953125\n",
            "2024-01-14 10:47:33,093 - For batch 1400 loss at 500 samples is 1.58365625\n",
            "2024-01-14 10:47:33,521 - LOSS train 1.58365625 valid 5.72265625\n",
            "2024-01-14 10:47:33,523 - Starting training at 231\n",
            "2024-01-14 10:47:34,101 - For batch 100 loss at 500 samples is 1.5843671875\n",
            "2024-01-14 10:47:34,729 - For batch 200 loss at 500 samples is 1.5939296875\n",
            "2024-01-14 10:47:35,348 - For batch 300 loss at 500 samples is 1.590375\n",
            "2024-01-14 10:47:35,974 - For batch 400 loss at 500 samples is 1.580546875\n",
            "2024-01-14 10:47:36,654 - For batch 500 loss at 500 samples is 1.5788515625\n",
            "2024-01-14 10:47:37,338 - For batch 600 loss at 500 samples is 1.5922578125\n",
            "2024-01-14 10:47:37,939 - For batch 700 loss at 500 samples is 1.5766953125\n",
            "2024-01-14 10:47:38,610 - For batch 800 loss at 500 samples is 1.5861015625\n",
            "2024-01-14 10:47:39,257 - For batch 900 loss at 500 samples is 1.57984375\n",
            "2024-01-14 10:47:39,915 - For batch 1000 loss at 500 samples is 1.5863671875\n",
            "2024-01-14 10:47:40,583 - For batch 1100 loss at 500 samples is 1.5784765625\n",
            "2024-01-14 10:47:41,205 - For batch 1200 loss at 500 samples is 1.5686796875\n",
            "2024-01-14 10:47:41,863 - For batch 1300 loss at 500 samples is 1.5812734375\n",
            "2024-01-14 10:47:42,784 - For batch 1400 loss at 500 samples is 1.5807890625\n",
            "2024-01-14 10:47:43,283 - LOSS train 1.5807890625 valid 5.6796875\n",
            "2024-01-14 10:47:43,296 - Starting training at 232\n",
            "2024-01-14 10:47:44,184 - For batch 100 loss at 500 samples is 1.584125\n",
            "2024-01-14 10:47:45,042 - For batch 200 loss at 500 samples is 1.593625\n",
            "2024-01-14 10:47:45,643 - For batch 300 loss at 500 samples is 1.5902109375\n",
            "2024-01-14 10:47:46,265 - For batch 400 loss at 500 samples is 1.580484375\n",
            "2024-01-14 10:47:46,957 - For batch 500 loss at 500 samples is 1.5772421875\n",
            "2024-01-14 10:47:47,664 - For batch 600 loss at 500 samples is 1.59225\n",
            "2024-01-14 10:47:48,281 - For batch 700 loss at 500 samples is 1.57859375\n",
            "2024-01-14 10:47:48,956 - For batch 800 loss at 500 samples is 1.582515625\n",
            "2024-01-14 10:47:49,525 - For batch 900 loss at 500 samples is 1.5813828125\n",
            "2024-01-14 10:47:50,118 - For batch 1000 loss at 500 samples is 1.5875859375\n",
            "2024-01-14 10:47:50,759 - For batch 1100 loss at 500 samples is 1.5784375\n",
            "2024-01-14 10:47:51,407 - For batch 1200 loss at 500 samples is 1.5679296875\n",
            "2024-01-14 10:47:52,076 - For batch 1300 loss at 500 samples is 1.581046875\n",
            "2024-01-14 10:47:52,695 - For batch 1400 loss at 500 samples is 1.585046875\n",
            "2024-01-14 10:47:53,154 - LOSS train 1.585046875 valid 5.6796875\n",
            "2024-01-14 10:47:53,156 - Starting training at 233\n",
            "2024-01-14 10:47:53,822 - For batch 100 loss at 500 samples is 1.58403125\n",
            "2024-01-14 10:47:54,513 - For batch 200 loss at 500 samples is 1.594296875\n",
            "2024-01-14 10:47:55,239 - For batch 300 loss at 500 samples is 1.5910390625\n",
            "2024-01-14 10:47:56,055 - For batch 400 loss at 500 samples is 1.581234375\n",
            "2024-01-14 10:47:56,881 - For batch 500 loss at 500 samples is 1.5779296875\n",
            "2024-01-14 10:47:57,822 - For batch 600 loss at 500 samples is 1.5920625\n",
            "2024-01-14 10:47:58,594 - For batch 700 loss at 500 samples is 1.576609375\n",
            "2024-01-14 10:47:59,204 - For batch 800 loss at 500 samples is 1.5837734375\n",
            "2024-01-14 10:47:59,871 - For batch 900 loss at 500 samples is 1.5833515625\n",
            "2024-01-14 10:48:00,495 - For batch 1000 loss at 500 samples is 1.5868125\n",
            "2024-01-14 10:48:01,164 - For batch 1100 loss at 500 samples is 1.5795546875\n",
            "2024-01-14 10:48:01,837 - For batch 1200 loss at 500 samples is 1.56949609375\n",
            "2024-01-14 10:48:02,441 - For batch 1300 loss at 500 samples is 1.5816328125\n",
            "2024-01-14 10:48:03,104 - For batch 1400 loss at 500 samples is 1.5844375\n",
            "2024-01-14 10:48:03,558 - LOSS train 1.5844375 valid 5.6484375\n",
            "2024-01-14 10:48:03,561 - Starting training at 234\n",
            "2024-01-14 10:48:04,178 - For batch 100 loss at 500 samples is 1.584625\n",
            "2024-01-14 10:48:04,894 - For batch 200 loss at 500 samples is 1.594125\n",
            "2024-01-14 10:48:05,512 - For batch 300 loss at 500 samples is 1.5904453125\n",
            "2024-01-14 10:48:06,131 - For batch 400 loss at 500 samples is 1.5816484375\n",
            "2024-01-14 10:48:06,735 - For batch 500 loss at 500 samples is 1.577875\n",
            "2024-01-14 10:48:07,333 - For batch 600 loss at 500 samples is 1.592234375\n",
            "2024-01-14 10:48:07,972 - For batch 700 loss at 500 samples is 1.5767734375\n",
            "2024-01-14 10:48:08,697 - For batch 800 loss at 500 samples is 1.586984375\n",
            "2024-01-14 10:48:09,600 - For batch 900 loss at 500 samples is 1.5805390625\n",
            "2024-01-14 10:48:10,476 - For batch 1000 loss at 500 samples is 1.586859375\n",
            "2024-01-14 10:48:11,469 - For batch 1100 loss at 500 samples is 1.578828125\n",
            "2024-01-14 10:48:12,092 - For batch 1200 loss at 500 samples is 1.56807421875\n",
            "2024-01-14 10:48:12,721 - For batch 1300 loss at 500 samples is 1.5814609375\n",
            "2024-01-14 10:48:13,384 - For batch 1400 loss at 500 samples is 1.580625\n",
            "2024-01-14 10:48:13,744 - LOSS train 1.580625 valid 5.62109375\n",
            "2024-01-14 10:48:13,746 - Starting training at 235\n",
            "2024-01-14 10:48:14,376 - For batch 100 loss at 500 samples is 1.584171875\n",
            "2024-01-14 10:48:14,989 - For batch 200 loss at 500 samples is 1.5939296875\n",
            "2024-01-14 10:48:15,607 - For batch 300 loss at 500 samples is 1.5904921875\n",
            "2024-01-14 10:48:16,231 - For batch 400 loss at 500 samples is 1.5812265625\n",
            "2024-01-14 10:48:16,808 - For batch 500 loss at 500 samples is 1.5794296875\n",
            "2024-01-14 10:48:17,473 - For batch 600 loss at 500 samples is 1.592328125\n",
            "2024-01-14 10:48:18,172 - For batch 700 loss at 500 samples is 1.57715625\n",
            "2024-01-14 10:48:18,819 - For batch 800 loss at 500 samples is 1.583078125\n",
            "2024-01-14 10:48:19,432 - For batch 900 loss at 500 samples is 1.58121875\n",
            "2024-01-14 10:48:20,022 - For batch 1000 loss at 500 samples is 1.5879375\n",
            "2024-01-14 10:48:20,626 - For batch 1100 loss at 500 samples is 1.578546875\n",
            "2024-01-14 10:48:21,317 - For batch 1200 loss at 500 samples is 1.56875390625\n",
            "2024-01-14 10:48:22,128 - For batch 1300 loss at 500 samples is 1.58140625\n",
            "2024-01-14 10:48:22,933 - For batch 1400 loss at 500 samples is 1.5852109375\n",
            "2024-01-14 10:48:23,556 - LOSS train 1.5852109375 valid 5.5859375\n",
            "2024-01-14 10:48:23,574 - Starting training at 236\n",
            "2024-01-14 10:48:24,418 - For batch 100 loss at 500 samples is 1.584828125\n",
            "2024-01-14 10:48:25,143 - For batch 200 loss at 500 samples is 1.59440625\n",
            "2024-01-14 10:48:25,737 - For batch 300 loss at 500 samples is 1.591015625\n",
            "2024-01-14 10:48:26,411 - For batch 400 loss at 500 samples is 1.5818984375\n",
            "2024-01-14 10:48:27,065 - For batch 500 loss at 500 samples is 1.5791484375\n",
            "2024-01-14 10:48:27,739 - For batch 600 loss at 500 samples is 1.5927421875\n",
            "2024-01-14 10:48:28,388 - For batch 700 loss at 500 samples is 1.580125\n",
            "2024-01-14 10:48:28,993 - For batch 800 loss at 500 samples is 1.5842890625\n",
            "2024-01-14 10:48:29,628 - For batch 900 loss at 500 samples is 1.5836953125\n",
            "2024-01-14 10:48:30,242 - For batch 1000 loss at 500 samples is 1.58703125\n",
            "2024-01-14 10:48:30,872 - For batch 1100 loss at 500 samples is 1.580390625\n",
            "2024-01-14 10:48:31,590 - For batch 1200 loss at 500 samples is 1.56865625\n",
            "2024-01-14 10:48:32,249 - For batch 1300 loss at 500 samples is 1.5819921875\n",
            "2024-01-14 10:48:32,961 - For batch 1400 loss at 500 samples is 1.5867109375\n",
            "2024-01-14 10:48:33,395 - LOSS train 1.5867109375 valid 5.59375\n",
            "2024-01-14 10:48:33,402 - Starting training at 237\n",
            "2024-01-14 10:48:33,992 - For batch 100 loss at 500 samples is 1.584765625\n",
            "2024-01-14 10:48:34,663 - For batch 200 loss at 500 samples is 1.5943515625\n",
            "2024-01-14 10:48:35,554 - For batch 300 loss at 500 samples is 1.5908359375\n",
            "2024-01-14 10:48:36,448 - For batch 400 loss at 500 samples is 1.5815703125\n",
            "2024-01-14 10:48:37,269 - For batch 500 loss at 500 samples is 1.579875\n",
            "2024-01-14 10:48:38,018 - For batch 600 loss at 500 samples is 1.5925859375\n",
            "2024-01-14 10:48:38,635 - For batch 700 loss at 500 samples is 1.576796875\n",
            "2024-01-14 10:48:39,324 - For batch 800 loss at 500 samples is 1.5873359375\n",
            "2024-01-14 10:48:39,987 - For batch 900 loss at 500 samples is 1.580375\n",
            "2024-01-14 10:48:40,655 - For batch 1000 loss at 500 samples is 1.586890625\n",
            "2024-01-14 10:48:41,303 - For batch 1100 loss at 500 samples is 1.579109375\n",
            "2024-01-14 10:48:41,971 - For batch 1200 loss at 500 samples is 1.5688984375\n",
            "2024-01-14 10:48:42,677 - For batch 1300 loss at 500 samples is 1.5818203125\n",
            "2024-01-14 10:48:43,351 - For batch 1400 loss at 500 samples is 1.579875\n",
            "2024-01-14 10:48:43,793 - LOSS train 1.579875 valid 5.54296875\n",
            "2024-01-14 10:48:43,795 - Starting training at 238\n",
            "2024-01-14 10:48:44,409 - For batch 100 loss at 500 samples is 1.58453125\n",
            "2024-01-14 10:48:45,035 - For batch 200 loss at 500 samples is 1.5943359375\n",
            "2024-01-14 10:48:45,624 - For batch 300 loss at 500 samples is 1.590890625\n",
            "2024-01-14 10:48:46,208 - For batch 400 loss at 500 samples is 1.58209375\n",
            "2024-01-14 10:48:46,857 - For batch 500 loss at 500 samples is 1.580734375\n",
            "2024-01-14 10:48:47,500 - For batch 600 loss at 500 samples is 1.5925078125\n",
            "2024-01-14 10:48:48,276 - For batch 700 loss at 500 samples is 1.576671875\n",
            "2024-01-14 10:48:49,234 - For batch 800 loss at 500 samples is 1.5836875\n",
            "2024-01-14 10:48:50,058 - For batch 900 loss at 500 samples is 1.582171875\n",
            "2024-01-14 10:48:50,946 - For batch 1000 loss at 500 samples is 1.587046875\n",
            "2024-01-14 10:48:51,529 - For batch 1100 loss at 500 samples is 1.5791015625\n",
            "2024-01-14 10:48:52,151 - For batch 1200 loss at 500 samples is 1.5685234375\n",
            "2024-01-14 10:48:52,808 - For batch 1300 loss at 500 samples is 1.581875\n",
            "2024-01-14 10:48:53,414 - For batch 1400 loss at 500 samples is 1.5858359375\n",
            "2024-01-14 10:48:53,857 - LOSS train 1.5858359375 valid 5.53125\n",
            "2024-01-14 10:48:53,861 - Starting training at 239\n",
            "2024-01-14 10:48:54,440 - For batch 100 loss at 500 samples is 1.585234375\n",
            "2024-01-14 10:48:55,053 - For batch 200 loss at 500 samples is 1.5950234375\n",
            "2024-01-14 10:48:55,632 - For batch 300 loss at 500 samples is 1.5911640625\n",
            "2024-01-14 10:48:56,235 - For batch 400 loss at 500 samples is 1.5821015625\n",
            "2024-01-14 10:48:56,905 - For batch 500 loss at 500 samples is 1.5833984375\n",
            "2024-01-14 10:48:57,504 - For batch 600 loss at 500 samples is 1.592953125\n",
            "2024-01-14 10:48:58,199 - For batch 700 loss at 500 samples is 1.5789453125\n",
            "2024-01-14 10:48:58,881 - For batch 800 loss at 500 samples is 1.584421875\n",
            "2024-01-14 10:48:59,467 - For batch 900 loss at 500 samples is 1.5835078125\n",
            "2024-01-14 10:49:00,119 - For batch 1000 loss at 500 samples is 1.58753125\n",
            "2024-01-14 10:49:00,768 - For batch 1100 loss at 500 samples is 1.5797734375\n",
            "2024-01-14 10:49:01,623 - For batch 1200 loss at 500 samples is 1.56946484375\n",
            "2024-01-14 10:49:02,435 - For batch 1300 loss at 500 samples is 1.582328125\n",
            "2024-01-14 10:49:03,255 - For batch 1400 loss at 500 samples is 1.58434375\n",
            "2024-01-14 10:49:03,935 - LOSS train 1.58434375 valid 5.49609375\n",
            "2024-01-14 10:49:03,940 - Starting training at 240\n",
            "2024-01-14 10:49:04,648 - For batch 100 loss at 500 samples is 1.58509375\n",
            "2024-01-14 10:49:05,232 - For batch 200 loss at 500 samples is 1.5950546875\n",
            "2024-01-14 10:49:05,873 - For batch 300 loss at 500 samples is 1.5910859375\n",
            "2024-01-14 10:49:06,470 - For batch 400 loss at 500 samples is 1.5819921875\n",
            "2024-01-14 10:49:07,153 - For batch 500 loss at 500 samples is 1.580796875\n",
            "2024-01-14 10:49:07,775 - For batch 600 loss at 500 samples is 1.5926796875\n",
            "2024-01-14 10:49:08,498 - For batch 700 loss at 500 samples is 1.576890625\n",
            "2024-01-14 10:49:09,125 - For batch 800 loss at 500 samples is 1.58759375\n",
            "2024-01-14 10:49:09,795 - For batch 900 loss at 500 samples is 1.58059375\n",
            "2024-01-14 10:49:10,520 - For batch 1000 loss at 500 samples is 1.5885078125\n",
            "2024-01-14 10:49:11,165 - For batch 1100 loss at 500 samples is 1.5789140625\n",
            "2024-01-14 10:49:11,834 - For batch 1200 loss at 500 samples is 1.56876953125\n",
            "2024-01-14 10:49:12,466 - For batch 1300 loss at 500 samples is 1.58203125\n",
            "2024-01-14 10:49:13,137 - For batch 1400 loss at 500 samples is 1.5833984375\n",
            "2024-01-14 10:49:13,559 - LOSS train 1.5833984375 valid 5.4921875\n",
            "2024-01-14 10:49:13,561 - Starting training at 241\n",
            "2024-01-14 10:49:14,163 - For batch 100 loss at 500 samples is 1.5847265625\n",
            "2024-01-14 10:49:15,103 - For batch 200 loss at 500 samples is 1.594640625\n",
            "2024-01-14 10:49:16,012 - For batch 300 loss at 500 samples is 1.5909375\n",
            "2024-01-14 10:49:16,883 - For batch 400 loss at 500 samples is 1.5820859375\n",
            "2024-01-14 10:49:17,618 - For batch 500 loss at 500 samples is 1.5814375\n",
            "2024-01-14 10:49:18,242 - For batch 600 loss at 500 samples is 1.5926953125\n",
            "2024-01-14 10:49:18,832 - For batch 700 loss at 500 samples is 1.578125\n",
            "2024-01-14 10:49:19,465 - For batch 800 loss at 500 samples is 1.5839140625\n",
            "2024-01-14 10:49:20,067 - For batch 900 loss at 500 samples is 1.5810859375\n",
            "2024-01-14 10:49:20,648 - For batch 1000 loss at 500 samples is 1.5871015625\n",
            "2024-01-14 10:49:21,260 - For batch 1100 loss at 500 samples is 1.57965625\n",
            "2024-01-14 10:49:21,863 - For batch 1200 loss at 500 samples is 1.57023828125\n",
            "2024-01-14 10:49:22,475 - For batch 1300 loss at 500 samples is 1.58184375\n",
            "2024-01-14 10:49:23,196 - For batch 1400 loss at 500 samples is 1.5871875\n",
            "2024-01-14 10:49:23,564 - LOSS train 1.5871875 valid 5.45703125\n",
            "2024-01-14 10:49:23,566 - Starting training at 242\n",
            "2024-01-14 10:49:24,201 - For batch 100 loss at 500 samples is 1.58528125\n",
            "2024-01-14 10:49:24,819 - For batch 200 loss at 500 samples is 1.5955078125\n",
            "2024-01-14 10:49:25,403 - For batch 300 loss at 500 samples is 1.5916484375\n",
            "2024-01-14 10:49:26,045 - For batch 400 loss at 500 samples is 1.5828515625\n",
            "2024-01-14 10:49:26,668 - For batch 500 loss at 500 samples is 1.58171875\n",
            "2024-01-14 10:49:27,306 - For batch 600 loss at 500 samples is 1.593015625\n",
            "2024-01-14 10:49:28,215 - For batch 700 loss at 500 samples is 1.577015625\n",
            "2024-01-14 10:49:29,136 - For batch 800 loss at 500 samples is 1.5847421875\n",
            "2024-01-14 10:49:30,011 - For batch 900 loss at 500 samples is 1.5842265625\n",
            "2024-01-14 10:49:30,746 - For batch 1000 loss at 500 samples is 1.587609375\n",
            "2024-01-14 10:49:31,356 - For batch 1100 loss at 500 samples is 1.579875\n",
            "2024-01-14 10:49:31,937 - For batch 1200 loss at 500 samples is 1.56908203125\n",
            "2024-01-14 10:49:32,566 - For batch 1300 loss at 500 samples is 1.5826953125\n",
            "2024-01-14 10:49:33,185 - For batch 1400 loss at 500 samples is 1.5825703125\n",
            "2024-01-14 10:49:33,608 - LOSS train 1.5825703125 valid 5.4375\n",
            "2024-01-14 10:49:33,610 - Starting training at 243\n",
            "2024-01-14 10:49:34,244 - For batch 100 loss at 500 samples is 1.5851015625\n",
            "2024-01-14 10:49:34,831 - For batch 200 loss at 500 samples is 1.595046875\n",
            "2024-01-14 10:49:35,489 - For batch 300 loss at 500 samples is 1.59140625\n",
            "2024-01-14 10:49:36,136 - For batch 400 loss at 500 samples is 1.582234375\n",
            "2024-01-14 10:49:36,738 - For batch 500 loss at 500 samples is 1.58203125\n",
            "2024-01-14 10:49:37,375 - For batch 600 loss at 500 samples is 1.5930078125\n",
            "2024-01-14 10:49:38,006 - For batch 700 loss at 500 samples is 1.5791953125\n",
            "2024-01-14 10:49:38,647 - For batch 800 loss at 500 samples is 1.5881171875\n",
            "2024-01-14 10:49:39,356 - For batch 900 loss at 500 samples is 1.580171875\n",
            "2024-01-14 10:49:39,929 - For batch 1000 loss at 500 samples is 1.587875\n",
            "2024-01-14 10:49:40,620 - For batch 1100 loss at 500 samples is 1.580265625\n",
            "2024-01-14 10:49:41,558 - For batch 1200 loss at 500 samples is 1.56847265625\n",
            "2024-01-14 10:49:42,380 - For batch 1300 loss at 500 samples is 1.58240625\n",
            "2024-01-14 10:49:43,282 - For batch 1400 loss at 500 samples is 1.5866953125\n",
            "2024-01-14 10:49:43,889 - LOSS train 1.5866953125 valid 5.40625\n",
            "2024-01-14 10:49:43,897 - Starting training at 244\n",
            "2024-01-14 10:49:44,477 - For batch 100 loss at 500 samples is 1.5848515625\n",
            "2024-01-14 10:49:45,191 - For batch 200 loss at 500 samples is 1.595234375\n",
            "2024-01-14 10:49:45,911 - For batch 300 loss at 500 samples is 1.5914921875\n",
            "2024-01-14 10:49:46,558 - For batch 400 loss at 500 samples is 1.582125\n",
            "2024-01-14 10:49:47,166 - For batch 500 loss at 500 samples is 1.5824609375\n",
            "2024-01-14 10:49:47,778 - For batch 600 loss at 500 samples is 1.592953125\n",
            "2024-01-14 10:49:48,420 - For batch 700 loss at 500 samples is 1.5774375\n",
            "2024-01-14 10:49:49,046 - For batch 800 loss at 500 samples is 1.5843671875\n",
            "2024-01-14 10:49:49,682 - For batch 900 loss at 500 samples is 1.5806015625\n",
            "2024-01-14 10:49:50,293 - For batch 1000 loss at 500 samples is 1.5875234375\n",
            "2024-01-14 10:49:50,939 - For batch 1100 loss at 500 samples is 1.5795234375\n",
            "2024-01-14 10:49:51,635 - For batch 1200 loss at 500 samples is 1.56863671875\n",
            "2024-01-14 10:49:52,330 - For batch 1300 loss at 500 samples is 1.5821171875\n",
            "2024-01-14 10:49:52,943 - For batch 1400 loss at 500 samples is 1.5846484375\n",
            "2024-01-14 10:49:53,334 - LOSS train 1.5846484375 valid 5.40234375\n",
            "2024-01-14 10:49:53,337 - Starting training at 245\n",
            "2024-01-14 10:49:54,037 - For batch 100 loss at 500 samples is 1.5850546875\n",
            "2024-01-14 10:49:54,973 - For batch 200 loss at 500 samples is 1.59478125\n",
            "2024-01-14 10:49:55,839 - For batch 300 loss at 500 samples is 1.59378125\n",
            "2024-01-14 10:49:56,729 - For batch 400 loss at 500 samples is 1.58284375\n",
            "2024-01-14 10:49:57,469 - For batch 500 loss at 500 samples is 1.582953125\n",
            "2024-01-14 10:49:58,149 - For batch 600 loss at 500 samples is 1.593671875\n",
            "2024-01-14 10:49:58,906 - For batch 700 loss at 500 samples is 1.57803125\n",
            "2024-01-14 10:49:59,565 - For batch 800 loss at 500 samples is 1.5852109375\n",
            "2024-01-14 10:50:00,236 - For batch 900 loss at 500 samples is 1.5843828125\n",
            "2024-01-14 10:50:00,823 - For batch 1000 loss at 500 samples is 1.5888828125\n",
            "2024-01-14 10:50:01,448 - For batch 1100 loss at 500 samples is 1.5803203125\n",
            "2024-01-14 10:50:02,178 - For batch 1200 loss at 500 samples is 1.5707265625\n",
            "2024-01-14 10:50:02,818 - For batch 1300 loss at 500 samples is 1.58246875\n",
            "2024-01-14 10:50:03,420 - For batch 1400 loss at 500 samples is 1.5831171875\n",
            "2024-01-14 10:50:03,873 - LOSS train 1.5831171875 valid 5.36328125\n",
            "2024-01-14 10:50:03,875 - Starting training at 246\n",
            "2024-01-14 10:50:04,485 - For batch 100 loss at 500 samples is 1.5857734375\n",
            "2024-01-14 10:50:05,151 - For batch 200 loss at 500 samples is 1.595953125\n",
            "2024-01-14 10:50:05,757 - For batch 300 loss at 500 samples is 1.5923828125\n",
            "2024-01-14 10:50:06,415 - For batch 400 loss at 500 samples is 1.582875\n",
            "2024-01-14 10:50:07,199 - For batch 500 loss at 500 samples is 1.58225\n",
            "2024-01-14 10:50:08,140 - For batch 600 loss at 500 samples is 1.5935234375\n",
            "2024-01-14 10:50:09,035 - For batch 700 loss at 500 samples is 1.577859375\n",
            "2024-01-14 10:50:09,841 - For batch 800 loss at 500 samples is 1.588296875\n",
            "2024-01-14 10:50:10,566 - For batch 900 loss at 500 samples is 1.5813984375\n",
            "2024-01-14 10:50:11,219 - For batch 1000 loss at 500 samples is 1.5875234375\n",
            "2024-01-14 10:50:11,847 - For batch 1100 loss at 500 samples is 1.5801328125\n",
            "2024-01-14 10:50:12,513 - For batch 1200 loss at 500 samples is 1.5695\n",
            "2024-01-14 10:50:13,171 - For batch 1300 loss at 500 samples is 1.58275\n",
            "2024-01-14 10:50:13,861 - For batch 1400 loss at 500 samples is 1.58334375\n",
            "2024-01-14 10:50:14,324 - LOSS train 1.58334375 valid 5.34765625\n",
            "2024-01-14 10:50:14,326 - Starting training at 247\n",
            "2024-01-14 10:50:14,923 - For batch 100 loss at 500 samples is 1.585109375\n",
            "2024-01-14 10:50:15,608 - For batch 200 loss at 500 samples is 1.595609375\n",
            "2024-01-14 10:50:16,221 - For batch 300 loss at 500 samples is 1.59175\n",
            "2024-01-14 10:50:16,830 - For batch 400 loss at 500 samples is 1.5823671875\n",
            "2024-01-14 10:50:17,423 - For batch 500 loss at 500 samples is 1.582109375\n",
            "2024-01-14 10:50:18,014 - For batch 600 loss at 500 samples is 1.593140625\n",
            "2024-01-14 10:50:18,630 - For batch 700 loss at 500 samples is 1.579625\n",
            "2024-01-14 10:50:19,304 - For batch 800 loss at 500 samples is 1.5847109375\n",
            "2024-01-14 10:50:19,899 - For batch 900 loss at 500 samples is 1.581703125\n",
            "2024-01-14 10:50:20,717 - For batch 1000 loss at 500 samples is 1.5877890625\n",
            "2024-01-14 10:50:21,665 - For batch 1100 loss at 500 samples is 1.58053125\n",
            "2024-01-14 10:50:22,645 - For batch 1200 loss at 500 samples is 1.5695546875\n",
            "2024-01-14 10:50:23,491 - For batch 1300 loss at 500 samples is 1.5827109375\n",
            "2024-01-14 10:50:24,098 - For batch 1400 loss at 500 samples is 1.5881328125\n",
            "2024-01-14 10:50:24,562 - LOSS train 1.5881328125 valid 5.32421875\n",
            "2024-01-14 10:50:24,564 - Starting training at 248\n",
            "2024-01-14 10:50:25,173 - For batch 100 loss at 500 samples is 1.585078125\n",
            "2024-01-14 10:50:25,799 - For batch 200 loss at 500 samples is 1.595828125\n",
            "2024-01-14 10:50:26,408 - For batch 300 loss at 500 samples is 1.591859375\n",
            "2024-01-14 10:50:27,014 - For batch 400 loss at 500 samples is 1.582546875\n",
            "2024-01-14 10:50:27,678 - For batch 500 loss at 500 samples is 1.582890625\n",
            "2024-01-14 10:50:28,314 - For batch 600 loss at 500 samples is 1.59390625\n",
            "2024-01-14 10:50:28,996 - For batch 700 loss at 500 samples is 1.57775\n",
            "2024-01-14 10:50:29,681 - For batch 800 loss at 500 samples is 1.585515625\n",
            "2024-01-14 10:50:30,290 - For batch 900 loss at 500 samples is 1.584515625\n",
            "2024-01-14 10:50:30,982 - For batch 1000 loss at 500 samples is 1.58853125\n",
            "2024-01-14 10:50:31,611 - For batch 1100 loss at 500 samples is 1.5804453125\n",
            "2024-01-14 10:50:32,268 - For batch 1200 loss at 500 samples is 1.57123828125\n",
            "2024-01-14 10:50:32,938 - For batch 1300 loss at 500 samples is 1.582984375\n",
            "2024-01-14 10:50:33,639 - For batch 1400 loss at 500 samples is 1.582671875\n",
            "2024-01-14 10:50:34,268 - LOSS train 1.582671875 valid 5.31640625\n",
            "2024-01-14 10:50:34,275 - Starting training at 249\n",
            "2024-01-14 10:50:35,112 - For batch 100 loss at 500 samples is 1.5862265625\n",
            "2024-01-14 10:50:35,962 - For batch 200 loss at 500 samples is 1.5962578125\n",
            "2024-01-14 10:50:36,675 - For batch 300 loss at 500 samples is 1.5920390625\n",
            "2024-01-14 10:50:37,265 - For batch 400 loss at 500 samples is 1.58321875\n",
            "2024-01-14 10:50:37,991 - For batch 500 loss at 500 samples is 1.5835546875\n",
            "2024-01-14 10:50:38,594 - For batch 600 loss at 500 samples is 1.5940859375\n",
            "2024-01-14 10:50:39,230 - For batch 700 loss at 500 samples is 1.5779921875\n",
            "2024-01-14 10:50:39,876 - For batch 800 loss at 500 samples is 1.588875\n",
            "2024-01-14 10:50:40,480 - For batch 900 loss at 500 samples is 1.5817265625\n",
            "2024-01-14 10:50:41,105 - For batch 1000 loss at 500 samples is 1.5886484375\n",
            "2024-01-14 10:50:41,733 - For batch 1100 loss at 500 samples is 1.580859375\n",
            "2024-01-14 10:50:42,367 - For batch 1200 loss at 500 samples is 1.57115234375\n",
            "2024-01-14 10:50:43,007 - For batch 1300 loss at 500 samples is 1.582890625\n",
            "2024-01-14 10:50:43,635 - For batch 1400 loss at 500 samples is 1.5832265625\n",
            "2024-01-14 10:50:44,106 - LOSS train 1.5832265625 valid 5.28125\n",
            "2024-01-14 10:50:44,108 - Starting training at 250\n",
            "2024-01-14 10:50:44,751 - For batch 100 loss at 500 samples is 1.586109375\n",
            "2024-01-14 10:50:45,341 - For batch 200 loss at 500 samples is 1.596125\n",
            "2024-01-14 10:50:45,962 - For batch 300 loss at 500 samples is 1.5922578125\n",
            "2024-01-14 10:50:46,628 - For batch 400 loss at 500 samples is 1.5832890625\n",
            "2024-01-14 10:50:47,625 - For batch 500 loss at 500 samples is 1.5820859375\n",
            "2024-01-14 10:50:48,460 - For batch 600 loss at 500 samples is 1.5938984375\n",
            "2024-01-14 10:50:49,507 - For batch 700 loss at 500 samples is 1.5776328125\n",
            "2024-01-14 10:50:50,176 - For batch 800 loss at 500 samples is 1.585015625\n",
            "2024-01-14 10:50:50,862 - For batch 900 loss at 500 samples is 1.5815234375\n",
            "2024-01-14 10:50:51,553 - For batch 1000 loss at 500 samples is 1.5883359375\n",
            "2024-01-14 10:50:52,242 - For batch 1100 loss at 500 samples is 1.579421875\n",
            "2024-01-14 10:50:52,886 - For batch 1200 loss at 500 samples is 1.57176953125\n",
            "2024-01-14 10:50:53,582 - For batch 1300 loss at 500 samples is 1.582765625\n",
            "2024-01-14 10:50:54,261 - For batch 1400 loss at 500 samples is 1.5841796875\n",
            "2024-01-14 10:50:54,720 - LOSS train 1.5841796875 valid 5.265625\n",
            "2024-01-14 10:50:54,723 - Starting training at 251\n",
            "2024-01-14 10:50:55,343 - For batch 100 loss at 500 samples is 1.5861640625\n",
            "2024-01-14 10:50:56,007 - For batch 200 loss at 500 samples is 1.5962265625\n",
            "2024-01-14 10:50:56,689 - For batch 300 loss at 500 samples is 1.59265625\n",
            "2024-01-14 10:50:57,314 - For batch 400 loss at 500 samples is 1.582890625\n",
            "2024-01-14 10:50:57,933 - For batch 500 loss at 500 samples is 1.5823046875\n",
            "2024-01-14 10:50:58,543 - For batch 600 loss at 500 samples is 1.594078125\n",
            "2024-01-14 10:50:59,158 - For batch 700 loss at 500 samples is 1.5777578125\n",
            "2024-01-14 10:50:59,921 - For batch 800 loss at 500 samples is 1.5851015625\n",
            "2024-01-14 10:51:00,825 - For batch 900 loss at 500 samples is 1.5841171875\n",
            "2024-01-14 10:51:01,706 - For batch 1000 loss at 500 samples is 1.5888828125\n",
            "2024-01-14 10:51:02,603 - For batch 1100 loss at 500 samples is 1.5804609375\n",
            "2024-01-14 10:51:03,252 - For batch 1200 loss at 500 samples is 1.5697109375\n",
            "2024-01-14 10:51:03,875 - For batch 1300 loss at 500 samples is 1.5836953125\n",
            "2024-01-14 10:51:04,571 - For batch 1400 loss at 500 samples is 1.583546875\n",
            "2024-01-14 10:51:04,936 - LOSS train 1.583546875 valid 5.234375\n",
            "2024-01-14 10:51:04,938 - Starting training at 252\n",
            "2024-01-14 10:51:05,527 - For batch 100 loss at 500 samples is 1.5865859375\n",
            "2024-01-14 10:51:06,135 - For batch 200 loss at 500 samples is 1.5967734375\n",
            "2024-01-14 10:51:06,815 - For batch 300 loss at 500 samples is 1.5929453125\n",
            "2024-01-14 10:51:07,483 - For batch 400 loss at 500 samples is 1.584359375\n",
            "2024-01-14 10:51:08,101 - For batch 500 loss at 500 samples is 1.584140625\n",
            "2024-01-14 10:51:08,760 - For batch 600 loss at 500 samples is 1.5942265625\n",
            "2024-01-14 10:51:09,394 - For batch 700 loss at 500 samples is 1.577765625\n",
            "2024-01-14 10:51:10,023 - For batch 800 loss at 500 samples is 1.5889609375\n",
            "2024-01-14 10:51:10,689 - For batch 900 loss at 500 samples is 1.581375\n",
            "2024-01-14 10:51:11,299 - For batch 1000 loss at 500 samples is 1.5888984375\n",
            "2024-01-14 10:51:11,920 - For batch 1100 loss at 500 samples is 1.5808359375\n",
            "2024-01-14 10:51:12,552 - For batch 1200 loss at 500 samples is 1.569875\n",
            "2024-01-14 10:51:13,455 - For batch 1300 loss at 500 samples is 1.5831796875\n",
            "2024-01-14 10:51:14,341 - For batch 1400 loss at 500 samples is 1.584234375\n",
            "2024-01-14 10:51:14,944 - LOSS train 1.584234375 valid 5.23046875\n",
            "2024-01-14 10:51:14,946 - Starting training at 253\n",
            "2024-01-14 10:51:15,799 - For batch 100 loss at 500 samples is 1.586484375\n",
            "2024-01-14 10:51:16,414 - For batch 200 loss at 500 samples is 1.5968359375\n",
            "2024-01-14 10:51:17,028 - For batch 300 loss at 500 samples is 1.592765625\n",
            "2024-01-14 10:51:17,607 - For batch 400 loss at 500 samples is 1.58403125\n",
            "2024-01-14 10:51:18,238 - For batch 500 loss at 500 samples is 1.5827109375\n",
            "2024-01-14 10:51:18,848 - For batch 600 loss at 500 samples is 1.593890625\n",
            "2024-01-14 10:51:19,503 - For batch 700 loss at 500 samples is 1.5779609375\n",
            "2024-01-14 10:51:20,074 - For batch 800 loss at 500 samples is 1.5856640625\n",
            "2024-01-14 10:51:20,787 - For batch 900 loss at 500 samples is 1.580984375\n",
            "2024-01-14 10:51:21,378 - For batch 1000 loss at 500 samples is 1.5884921875\n",
            "2024-01-14 10:51:22,051 - For batch 1100 loss at 500 samples is 1.5811015625\n",
            "2024-01-14 10:51:22,649 - For batch 1200 loss at 500 samples is 1.5702890625\n",
            "2024-01-14 10:51:23,238 - For batch 1300 loss at 500 samples is 1.5830390625\n",
            "2024-01-14 10:51:23,828 - For batch 1400 loss at 500 samples is 1.58559375\n",
            "2024-01-14 10:51:24,268 - LOSS train 1.58559375 valid 5.19921875\n",
            "2024-01-14 10:51:24,275 - Starting training at 254\n",
            "2024-01-14 10:51:24,860 - For batch 100 loss at 500 samples is 1.5861953125\n",
            "2024-01-14 10:51:25,527 - For batch 200 loss at 500 samples is 1.5965546875\n",
            "2024-01-14 10:51:26,243 - For batch 300 loss at 500 samples is 1.5922890625\n",
            "2024-01-14 10:51:27,077 - For batch 400 loss at 500 samples is 1.5840859375\n",
            "2024-01-14 10:51:27,917 - For batch 500 loss at 500 samples is 1.58378125\n",
            "2024-01-14 10:51:28,819 - For batch 600 loss at 500 samples is 1.5938828125\n",
            "2024-01-14 10:51:29,500 - For batch 700 loss at 500 samples is 1.5778984375\n",
            "2024-01-14 10:51:30,078 - For batch 800 loss at 500 samples is 1.5855546875\n",
            "2024-01-14 10:51:30,744 - For batch 900 loss at 500 samples is 1.5816953125\n",
            "2024-01-14 10:51:31,358 - For batch 1000 loss at 500 samples is 1.58846875\n",
            "2024-01-14 10:51:31,953 - For batch 1100 loss at 500 samples is 1.5805703125\n",
            "2024-01-14 10:51:32,574 - For batch 1200 loss at 500 samples is 1.5693828125\n",
            "2024-01-14 10:51:33,191 - For batch 1300 loss at 500 samples is 1.5831015625\n",
            "2024-01-14 10:51:33,768 - For batch 1400 loss at 500 samples is 1.5814453125\n",
            "2024-01-14 10:51:34,116 - LOSS train 1.5814453125 valid 5.17578125\n",
            "2024-01-14 10:51:34,118 - Starting training at 255\n",
            "2024-01-14 10:51:34,764 - For batch 100 loss at 500 samples is 1.586265625\n",
            "2024-01-14 10:51:35,395 - For batch 200 loss at 500 samples is 1.596546875\n",
            "2024-01-14 10:51:36,041 - For batch 300 loss at 500 samples is 1.592125\n",
            "2024-01-14 10:51:36,700 - For batch 400 loss at 500 samples is 1.583859375\n",
            "2024-01-14 10:51:37,330 - For batch 500 loss at 500 samples is 1.583453125\n",
            "2024-01-14 10:51:37,994 - For batch 600 loss at 500 samples is 1.5945625\n",
            "2024-01-14 10:51:38,694 - For batch 700 loss at 500 samples is 1.579265625\n",
            "2024-01-14 10:51:39,412 - For batch 800 loss at 500 samples is 1.589390625\n",
            "2024-01-14 10:51:40,275 - For batch 900 loss at 500 samples is 1.582265625\n",
            "2024-01-14 10:51:41,088 - For batch 1000 loss at 500 samples is 1.589484375\n",
            "2024-01-14 10:51:41,889 - For batch 1100 loss at 500 samples is 1.5813515625\n",
            "2024-01-14 10:51:42,704 - For batch 1200 loss at 500 samples is 1.57026171875\n",
            "2024-01-14 10:51:43,328 - For batch 1300 loss at 500 samples is 1.5838828125\n",
            "2024-01-14 10:51:44,000 - For batch 1400 loss at 500 samples is 1.588984375\n",
            "2024-01-14 10:51:44,453 - LOSS train 1.588984375 valid 5.15625\n",
            "2024-01-14 10:51:44,458 - Starting training at 256\n",
            "2024-01-14 10:51:45,077 - For batch 100 loss at 500 samples is 1.586765625\n",
            "2024-01-14 10:51:45,763 - For batch 200 loss at 500 samples is 1.5971640625\n",
            "2024-01-14 10:51:46,350 - For batch 300 loss at 500 samples is 1.5932109375\n",
            "2024-01-14 10:51:46,989 - For batch 400 loss at 500 samples is 1.5847734375\n",
            "2024-01-14 10:51:47,625 - For batch 500 loss at 500 samples is 1.58425\n",
            "2024-01-14 10:51:48,334 - For batch 600 loss at 500 samples is 1.5945859375\n",
            "2024-01-14 10:51:48,957 - For batch 700 loss at 500 samples is 1.5788046875\n",
            "2024-01-14 10:51:49,534 - For batch 800 loss at 500 samples is 1.5861796875\n",
            "2024-01-14 10:51:50,164 - For batch 900 loss at 500 samples is 1.5860234375\n",
            "2024-01-14 10:51:50,778 - For batch 1000 loss at 500 samples is 1.588984375\n",
            "2024-01-14 10:51:51,426 - For batch 1100 loss at 500 samples is 1.5808125\n",
            "2024-01-14 10:51:52,159 - For batch 1200 loss at 500 samples is 1.57005078125\n",
            "2024-01-14 10:51:53,026 - For batch 1300 loss at 500 samples is 1.5837109375\n",
            "2024-01-14 10:51:54,054 - For batch 1400 loss at 500 samples is 1.585\n",
            "2024-01-14 10:51:54,620 - LOSS train 1.585 valid 5.14453125\n",
            "2024-01-14 10:51:54,622 - Starting training at 257\n",
            "2024-01-14 10:51:55,499 - For batch 100 loss at 500 samples is 1.5867421875\n",
            "2024-01-14 10:51:56,212 - For batch 200 loss at 500 samples is 1.597140625\n",
            "2024-01-14 10:51:56,831 - For batch 300 loss at 500 samples is 1.593046875\n",
            "2024-01-14 10:51:57,450 - For batch 400 loss at 500 samples is 1.584453125\n",
            "2024-01-14 10:51:58,049 - For batch 500 loss at 500 samples is 1.5832109375\n",
            "2024-01-14 10:51:58,694 - For batch 600 loss at 500 samples is 1.59428125\n",
            "2024-01-14 10:51:59,304 - For batch 700 loss at 500 samples is 1.581515625\n",
            "2024-01-14 10:51:59,937 - For batch 800 loss at 500 samples is 1.5860859375\n",
            "2024-01-14 10:52:00,542 - For batch 900 loss at 500 samples is 1.58128125\n",
            "2024-01-14 10:52:01,195 - For batch 1000 loss at 500 samples is 1.5890234375\n",
            "2024-01-14 10:52:01,887 - For batch 1100 loss at 500 samples is 1.581140625\n",
            "2024-01-14 10:52:02,576 - For batch 1200 loss at 500 samples is 1.57035546875\n",
            "2024-01-14 10:52:03,236 - For batch 1300 loss at 500 samples is 1.583609375\n",
            "2024-01-14 10:52:03,849 - For batch 1400 loss at 500 samples is 1.5839140625\n",
            "2024-01-14 10:52:04,306 - LOSS train 1.5839140625 valid 5.11328125\n",
            "2024-01-14 10:52:04,311 - Starting training at 258\n",
            "2024-01-14 10:52:04,919 - For batch 100 loss at 500 samples is 1.5867890625\n",
            "2024-01-14 10:52:05,533 - For batch 200 loss at 500 samples is 1.5971328125\n",
            "2024-01-14 10:52:06,381 - For batch 300 loss at 500 samples is 1.592921875\n",
            "2024-01-14 10:52:07,189 - For batch 400 loss at 500 samples is 1.5845390625\n",
            "2024-01-14 10:52:08,045 - For batch 500 loss at 500 samples is 1.5835\n",
            "2024-01-14 10:52:08,836 - For batch 600 loss at 500 samples is 1.5943359375\n",
            "2024-01-14 10:52:09,470 - For batch 700 loss at 500 samples is 1.577953125\n",
            "2024-01-14 10:52:10,117 - For batch 800 loss at 500 samples is 1.58915625\n",
            "2024-01-14 10:52:10,747 - For batch 900 loss at 500 samples is 1.58184375\n",
            "2024-01-14 10:52:11,353 - For batch 1000 loss at 500 samples is 1.588625\n",
            "2024-01-14 10:52:11,945 - For batch 1100 loss at 500 samples is 1.5805625\n",
            "2024-01-14 10:52:12,519 - For batch 1200 loss at 500 samples is 1.56976171875\n",
            "2024-01-14 10:52:13,155 - For batch 1300 loss at 500 samples is 1.583640625\n",
            "2024-01-14 10:52:13,734 - For batch 1400 loss at 500 samples is 1.5816953125\n",
            "2024-01-14 10:52:14,212 - LOSS train 1.5816953125 valid 5.109375\n",
            "2024-01-14 10:52:14,214 - Starting training at 259\n",
            "2024-01-14 10:52:14,830 - For batch 100 loss at 500 samples is 1.5871875\n",
            "2024-01-14 10:52:15,462 - For batch 200 loss at 500 samples is 1.597640625\n",
            "2024-01-14 10:52:16,056 - For batch 300 loss at 500 samples is 1.5934609375\n",
            "2024-01-14 10:52:16,648 - For batch 400 loss at 500 samples is 1.5848984375\n",
            "2024-01-14 10:52:17,309 - For batch 500 loss at 500 samples is 1.5855859375\n",
            "2024-01-14 10:52:17,966 - For batch 600 loss at 500 samples is 1.594625\n",
            "2024-01-14 10:52:18,625 - For batch 700 loss at 500 samples is 1.5786640625\n",
            "2024-01-14 10:52:19,538 - For batch 800 loss at 500 samples is 1.5865859375\n",
            "2024-01-14 10:52:20,366 - For batch 900 loss at 500 samples is 1.582703125\n",
            "2024-01-14 10:52:21,226 - For batch 1000 loss at 500 samples is 1.5899765625\n",
            "2024-01-14 10:52:21,971 - For batch 1100 loss at 500 samples is 1.5814765625\n",
            "2024-01-14 10:52:22,657 - For batch 1200 loss at 500 samples is 1.57047265625\n",
            "2024-01-14 10:52:23,246 - For batch 1300 loss at 500 samples is 1.584359375\n",
            "2024-01-14 10:52:23,825 - For batch 1400 loss at 500 samples is 1.586421875\n",
            "2024-01-14 10:52:24,314 - LOSS train 1.586421875 valid 5.078125\n",
            "2024-01-14 10:52:24,319 - Starting training at 260\n",
            "2024-01-14 10:52:25,002 - For batch 100 loss at 500 samples is 1.5870390625\n",
            "2024-01-14 10:52:25,598 - For batch 200 loss at 500 samples is 1.5975078125\n",
            "2024-01-14 10:52:26,205 - For batch 300 loss at 500 samples is 1.593453125\n",
            "2024-01-14 10:52:26,778 - For batch 400 loss at 500 samples is 1.5849765625\n",
            "2024-01-14 10:52:27,443 - For batch 500 loss at 500 samples is 1.584546875\n",
            "2024-01-14 10:52:28,031 - For batch 600 loss at 500 samples is 1.5947265625\n",
            "2024-01-14 10:52:28,769 - For batch 700 loss at 500 samples is 1.57903125\n",
            "2024-01-14 10:52:29,361 - For batch 800 loss at 500 samples is 1.586171875\n",
            "2024-01-14 10:52:30,007 - For batch 900 loss at 500 samples is 1.584109375\n",
            "2024-01-14 10:52:30,669 - For batch 1000 loss at 500 samples is 1.589359375\n",
            "2024-01-14 10:52:31,313 - For batch 1100 loss at 500 samples is 1.581109375\n",
            "2024-01-14 10:52:32,131 - For batch 1200 loss at 500 samples is 1.57021484375\n",
            "2024-01-14 10:52:33,011 - For batch 1300 loss at 500 samples is 1.5841328125\n",
            "2024-01-14 10:52:33,975 - For batch 1400 loss at 500 samples is 1.5822109375\n",
            "2024-01-14 10:52:34,564 - LOSS train 1.5822109375 valid 5.06640625\n",
            "2024-01-14 10:52:34,570 - Starting training at 261\n",
            "2024-01-14 10:52:35,235 - For batch 100 loss at 500 samples is 1.5868203125\n",
            "2024-01-14 10:52:35,929 - For batch 200 loss at 500 samples is 1.59746875\n",
            "2024-01-14 10:52:36,540 - For batch 300 loss at 500 samples is 1.593109375\n",
            "2024-01-14 10:52:37,150 - For batch 400 loss at 500 samples is 1.58475\n",
            "2024-01-14 10:52:37,776 - For batch 500 loss at 500 samples is 1.5832734375\n",
            "2024-01-14 10:52:38,410 - For batch 600 loss at 500 samples is 1.5944609375\n",
            "2024-01-14 10:52:39,059 - For batch 700 loss at 500 samples is 1.5789140625\n",
            "2024-01-14 10:52:39,699 - For batch 800 loss at 500 samples is 1.589\n",
            "2024-01-14 10:52:40,357 - For batch 900 loss at 500 samples is 1.5851875\n",
            "2024-01-14 10:52:41,010 - For batch 1000 loss at 500 samples is 1.5888984375\n",
            "2024-01-14 10:52:41,666 - For batch 1100 loss at 500 samples is 1.581578125\n",
            "2024-01-14 10:52:42,258 - For batch 1200 loss at 500 samples is 1.57043359375\n",
            "2024-01-14 10:52:42,933 - For batch 1300 loss at 500 samples is 1.584078125\n",
            "2024-01-14 10:52:43,650 - For batch 1400 loss at 500 samples is 1.588703125\n",
            "2024-01-14 10:52:44,011 - LOSS train 1.588703125 valid 5.0390625\n",
            "2024-01-14 10:52:44,014 - Starting training at 262\n",
            "2024-01-14 10:52:44,675 - For batch 100 loss at 500 samples is 1.5867578125\n",
            "2024-01-14 10:52:45,526 - For batch 200 loss at 500 samples is 1.59746875\n",
            "2024-01-14 10:52:46,329 - For batch 300 loss at 500 samples is 1.5930859375\n",
            "2024-01-14 10:52:47,190 - For batch 400 loss at 500 samples is 1.584859375\n",
            "2024-01-14 10:52:48,020 - For batch 500 loss at 500 samples is 1.5849375\n",
            "2024-01-14 10:52:48,629 - For batch 600 loss at 500 samples is 1.5944609375\n",
            "2024-01-14 10:52:49,221 - For batch 700 loss at 500 samples is 1.5810234375\n",
            "2024-01-14 10:52:49,868 - For batch 800 loss at 500 samples is 1.585984375\n",
            "2024-01-14 10:52:50,445 - For batch 900 loss at 500 samples is 1.581953125\n",
            "2024-01-14 10:52:51,039 - For batch 1000 loss at 500 samples is 1.5890703125\n",
            "2024-01-14 10:52:51,653 - For batch 1100 loss at 500 samples is 1.5819921875\n",
            "2024-01-14 10:52:52,254 - For batch 1200 loss at 500 samples is 1.570484375\n",
            "2024-01-14 10:52:52,957 - For batch 1300 loss at 500 samples is 1.584484375\n",
            "2024-01-14 10:52:53,578 - For batch 1400 loss at 500 samples is 1.5850625\n",
            "2024-01-14 10:52:54,056 - LOSS train 1.5850625 valid 5.02734375\n",
            "2024-01-14 10:52:54,060 - Starting training at 263\n",
            "2024-01-14 10:52:54,674 - For batch 100 loss at 500 samples is 1.58721875\n",
            "2024-01-14 10:52:55,265 - For batch 200 loss at 500 samples is 1.597921875\n",
            "2024-01-14 10:52:55,944 - For batch 300 loss at 500 samples is 1.59359375\n",
            "2024-01-14 10:52:56,581 - For batch 400 loss at 500 samples is 1.5852734375\n",
            "2024-01-14 10:52:57,167 - For batch 500 loss at 500 samples is 1.5840703125\n",
            "2024-01-14 10:52:57,826 - For batch 600 loss at 500 samples is 1.594921875\n",
            "2024-01-14 10:52:58,813 - For batch 700 loss at 500 samples is 1.5785234375\n",
            "2024-01-14 10:52:59,629 - For batch 800 loss at 500 samples is 1.586484375\n",
            "2024-01-14 10:53:00,656 - For batch 900 loss at 500 samples is 1.582703125\n",
            "2024-01-14 10:53:01,414 - For batch 1000 loss at 500 samples is 1.589765625\n",
            "2024-01-14 10:53:02,075 - For batch 1100 loss at 500 samples is 1.5825390625\n",
            "2024-01-14 10:53:02,662 - For batch 1200 loss at 500 samples is 1.57070703125\n",
            "2024-01-14 10:53:03,334 - For batch 1300 loss at 500 samples is 1.5843984375\n",
            "2024-01-14 10:53:03,984 - For batch 1400 loss at 500 samples is 1.58546875\n",
            "2024-01-14 10:53:04,430 - LOSS train 1.58546875 valid 5.00390625\n",
            "2024-01-14 10:53:04,432 - Starting training at 264\n",
            "2024-01-14 10:53:05,031 - For batch 100 loss at 500 samples is 1.58715625\n",
            "2024-01-14 10:53:05,684 - For batch 200 loss at 500 samples is 1.597984375\n",
            "2024-01-14 10:53:06,313 - For batch 300 loss at 500 samples is 1.593625\n",
            "2024-01-14 10:53:06,904 - For batch 400 loss at 500 samples is 1.5853359375\n",
            "2024-01-14 10:53:07,481 - For batch 500 loss at 500 samples is 1.5848515625\n",
            "2024-01-14 10:53:08,095 - For batch 600 loss at 500 samples is 1.5946484375\n",
            "2024-01-14 10:53:08,680 - For batch 700 loss at 500 samples is 1.5786953125\n",
            "2024-01-14 10:53:09,281 - For batch 800 loss at 500 samples is 1.5893515625\n",
            "2024-01-14 10:53:09,955 - For batch 900 loss at 500 samples is 1.5856953125\n",
            "2024-01-14 10:53:10,583 - For batch 1000 loss at 500 samples is 1.5896640625\n",
            "2024-01-14 10:53:11,181 - For batch 1100 loss at 500 samples is 1.5817578125\n",
            "2024-01-14 10:53:12,081 - For batch 1200 loss at 500 samples is 1.570671875\n",
            "2024-01-14 10:53:12,896 - For batch 1300 loss at 500 samples is 1.5842890625\n",
            "2024-01-14 10:53:13,721 - For batch 1400 loss at 500 samples is 1.5821484375\n",
            "2024-01-14 10:53:14,228 - LOSS train 1.5821484375 valid 4.98828125\n",
            "2024-01-14 10:53:14,233 - Starting training at 265\n",
            "2024-01-14 10:53:14,917 - For batch 100 loss at 500 samples is 1.5869375\n",
            "2024-01-14 10:53:15,540 - For batch 200 loss at 500 samples is 1.597828125\n",
            "2024-01-14 10:53:16,148 - For batch 300 loss at 500 samples is 1.5932734375\n",
            "2024-01-14 10:53:16,793 - For batch 400 loss at 500 samples is 1.58503125\n",
            "2024-01-14 10:53:17,448 - For batch 500 loss at 500 samples is 1.58446875\n",
            "2024-01-14 10:53:18,100 - For batch 600 loss at 500 samples is 1.594625\n",
            "2024-01-14 10:53:18,853 - For batch 700 loss at 500 samples is 1.578671875\n",
            "2024-01-14 10:53:19,472 - For batch 800 loss at 500 samples is 1.5863515625\n",
            "2024-01-14 10:53:20,117 - For batch 900 loss at 500 samples is 1.5830859375\n",
            "2024-01-14 10:53:20,709 - For batch 1000 loss at 500 samples is 1.5897578125\n",
            "2024-01-14 10:53:21,310 - For batch 1100 loss at 500 samples is 1.5816171875\n",
            "2024-01-14 10:53:21,968 - For batch 1200 loss at 500 samples is 1.57159765625\n",
            "2024-01-14 10:53:22,573 - For batch 1300 loss at 500 samples is 1.5841875\n",
            "2024-01-14 10:53:23,178 - For batch 1400 loss at 500 samples is 1.58459375\n",
            "2024-01-14 10:53:23,599 - LOSS train 1.58459375 valid 4.95703125\n",
            "2024-01-14 10:53:23,601 - Starting training at 266\n",
            "2024-01-14 10:53:24,220 - For batch 100 loss at 500 samples is 1.5869609375\n",
            "2024-01-14 10:53:25,011 - For batch 200 loss at 500 samples is 1.5978828125\n",
            "2024-01-14 10:53:25,817 - For batch 300 loss at 500 samples is 1.5933359375\n",
            "2024-01-14 10:53:26,733 - For batch 400 loss at 500 samples is 1.5851796875\n",
            "2024-01-14 10:53:27,661 - For batch 500 loss at 500 samples is 1.5839140625\n",
            "2024-01-14 10:53:28,346 - For batch 600 loss at 500 samples is 1.594703125\n",
            "2024-01-14 10:53:29,049 - For batch 700 loss at 500 samples is 1.578609375\n",
            "2024-01-14 10:53:29,664 - For batch 800 loss at 500 samples is 1.58625\n",
            "2024-01-14 10:53:30,343 - For batch 900 loss at 500 samples is 1.582640625\n",
            "2024-01-14 10:53:30,951 - For batch 1000 loss at 500 samples is 1.590109375\n",
            "2024-01-14 10:53:31,560 - For batch 1100 loss at 500 samples is 1.5824609375\n",
            "2024-01-14 10:53:32,285 - For batch 1200 loss at 500 samples is 1.5715078125\n",
            "2024-01-14 10:53:32,925 - For batch 1300 loss at 500 samples is 1.5846875\n",
            "2024-01-14 10:53:33,614 - For batch 1400 loss at 500 samples is 1.5908671875\n",
            "2024-01-14 10:53:34,017 - LOSS train 1.5908671875 valid 4.953125\n",
            "2024-01-14 10:53:34,020 - Starting training at 267\n",
            "2024-01-14 10:53:34,640 - For batch 100 loss at 500 samples is 1.587390625\n",
            "2024-01-14 10:53:35,333 - For batch 200 loss at 500 samples is 1.5984609375\n",
            "2024-01-14 10:53:35,948 - For batch 300 loss at 500 samples is 1.5937109375\n",
            "2024-01-14 10:53:36,533 - For batch 400 loss at 500 samples is 1.5854140625\n",
            "2024-01-14 10:53:37,130 - For batch 500 loss at 500 samples is 1.5843828125\n",
            "2024-01-14 10:53:37,829 - For batch 600 loss at 500 samples is 1.5956171875\n",
            "2024-01-14 10:53:38,686 - For batch 700 loss at 500 samples is 1.5792578125\n",
            "2024-01-14 10:53:39,617 - For batch 800 loss at 500 samples is 1.589859375\n",
            "2024-01-14 10:53:40,467 - For batch 900 loss at 500 samples is 1.5860390625\n",
            "2024-01-14 10:53:41,189 - For batch 1000 loss at 500 samples is 1.5904609375\n",
            "2024-01-14 10:53:41,855 - For batch 1100 loss at 500 samples is 1.582171875\n",
            "2024-01-14 10:53:42,485 - For batch 1200 loss at 500 samples is 1.571859375\n",
            "2024-01-14 10:53:43,120 - For batch 1300 loss at 500 samples is 1.5846953125\n",
            "2024-01-14 10:53:43,711 - For batch 1400 loss at 500 samples is 1.58425\n",
            "2024-01-14 10:53:44,132 - LOSS train 1.58425 valid 4.92578125\n",
            "2024-01-14 10:53:44,135 - Starting training at 268\n",
            "2024-01-14 10:53:44,752 - For batch 100 loss at 500 samples is 1.5873203125\n",
            "2024-01-14 10:53:45,346 - For batch 200 loss at 500 samples is 1.598375\n",
            "2024-01-14 10:53:46,008 - For batch 300 loss at 500 samples is 1.593765625\n",
            "2024-01-14 10:53:46,660 - For batch 400 loss at 500 samples is 1.5857265625\n",
            "2024-01-14 10:53:47,271 - For batch 500 loss at 500 samples is 1.5851015625\n",
            "2024-01-14 10:53:47,914 - For batch 600 loss at 500 samples is 1.5957109375\n",
            "2024-01-14 10:53:48,578 - For batch 700 loss at 500 samples is 1.5795859375\n",
            "2024-01-14 10:53:49,311 - For batch 800 loss at 500 samples is 1.5867421875\n",
            "2024-01-14 10:53:50,009 - For batch 900 loss at 500 samples is 1.5827578125\n",
            "2024-01-14 10:53:50,653 - For batch 1000 loss at 500 samples is 1.589875\n",
            "2024-01-14 10:53:51,445 - For batch 1100 loss at 500 samples is 1.581984375\n",
            "2024-01-14 10:53:52,255 - For batch 1200 loss at 500 samples is 1.571296875\n",
            "2024-01-14 10:53:53,167 - For batch 1300 loss at 500 samples is 1.584453125\n",
            "2024-01-14 10:53:53,961 - For batch 1400 loss at 500 samples is 1.58384375\n",
            "2024-01-14 10:53:54,398 - LOSS train 1.58384375 valid 4.9140625\n",
            "2024-01-14 10:53:54,401 - Starting training at 269\n",
            "2024-01-14 10:53:54,984 - For batch 100 loss at 500 samples is 1.5873046875\n",
            "2024-01-14 10:53:55,581 - For batch 200 loss at 500 samples is 1.5981171875\n",
            "2024-01-14 10:53:56,170 - For batch 300 loss at 500 samples is 1.5935859375\n",
            "2024-01-14 10:53:56,733 - For batch 400 loss at 500 samples is 1.585234375\n",
            "2024-01-14 10:53:57,367 - For batch 500 loss at 500 samples is 1.5841640625\n",
            "2024-01-14 10:53:57,957 - For batch 600 loss at 500 samples is 1.5958359375\n",
            "2024-01-14 10:53:58,654 - For batch 700 loss at 500 samples is 1.5793125\n",
            "2024-01-14 10:53:59,401 - For batch 800 loss at 500 samples is 1.5866171875\n",
            "2024-01-14 10:54:00,009 - For batch 900 loss at 500 samples is 1.5827109375\n",
            "2024-01-14 10:54:00,642 - For batch 1000 loss at 500 samples is 1.5897890625\n",
            "2024-01-14 10:54:01,313 - For batch 1100 loss at 500 samples is 1.5821953125\n",
            "2024-01-14 10:54:01,978 - For batch 1200 loss at 500 samples is 1.5715390625\n",
            "2024-01-14 10:54:02,600 - For batch 1300 loss at 500 samples is 1.5844296875\n",
            "2024-01-14 10:54:03,234 - For batch 1400 loss at 500 samples is 1.5852421875\n",
            "2024-01-14 10:54:03,776 - LOSS train 1.5852421875 valid 4.890625\n",
            "2024-01-14 10:54:03,778 - Starting training at 270\n",
            "2024-01-14 10:54:04,699 - For batch 100 loss at 500 samples is 1.587296875\n",
            "2024-01-14 10:54:05,573 - For batch 200 loss at 500 samples is 1.598140625\n",
            "2024-01-14 10:54:06,459 - For batch 300 loss at 500 samples is 1.5933984375\n",
            "2024-01-14 10:54:07,209 - For batch 400 loss at 500 samples is 1.58553125\n",
            "2024-01-14 10:54:07,853 - For batch 500 loss at 500 samples is 1.584453125\n",
            "2024-01-14 10:54:08,476 - For batch 600 loss at 500 samples is 1.59584375\n",
            "2024-01-14 10:54:09,152 - For batch 700 loss at 500 samples is 1.5811796875\n",
            "2024-01-14 10:54:09,760 - For batch 800 loss at 500 samples is 1.5893828125\n",
            "2024-01-14 10:54:10,417 - For batch 900 loss at 500 samples is 1.586171875\n",
            "2024-01-14 10:54:11,079 - For batch 1000 loss at 500 samples is 1.5896796875\n",
            "2024-01-14 10:54:11,741 - For batch 1100 loss at 500 samples is 1.5818515625\n",
            "2024-01-14 10:54:12,393 - For batch 1200 loss at 500 samples is 1.5718359375\n",
            "2024-01-14 10:54:13,011 - For batch 1300 loss at 500 samples is 1.5851953125\n",
            "2024-01-14 10:54:13,661 - For batch 1400 loss at 500 samples is 1.5869453125\n",
            "2024-01-14 10:54:14,059 - LOSS train 1.5869453125 valid 4.88671875\n",
            "2024-01-14 10:54:14,061 - Starting training at 271\n",
            "2024-01-14 10:54:14,658 - For batch 100 loss at 500 samples is 1.587953125\n",
            "2024-01-14 10:54:15,259 - For batch 200 loss at 500 samples is 1.5986875\n",
            "2024-01-14 10:54:15,920 - For batch 300 loss at 500 samples is 1.5940703125\n",
            "2024-01-14 10:54:16,592 - For batch 400 loss at 500 samples is 1.585609375\n",
            "2024-01-14 10:54:17,277 - For batch 500 loss at 500 samples is 1.5855390625\n",
            "2024-01-14 10:54:18,121 - For batch 600 loss at 500 samples is 1.5970625\n",
            "2024-01-14 10:54:19,108 - For batch 700 loss at 500 samples is 1.5792109375\n",
            "2024-01-14 10:54:20,034 - For batch 800 loss at 500 samples is 1.586984375\n",
            "2024-01-14 10:54:20,780 - For batch 900 loss at 500 samples is 1.5829453125\n",
            "2024-01-14 10:54:21,475 - For batch 1000 loss at 500 samples is 1.5905\n",
            "2024-01-14 10:54:22,098 - For batch 1100 loss at 500 samples is 1.5821953125\n",
            "2024-01-14 10:54:22,766 - For batch 1200 loss at 500 samples is 1.5718203125\n",
            "2024-01-14 10:54:23,383 - For batch 1300 loss at 500 samples is 1.5848828125\n",
            "2024-01-14 10:54:24,040 - For batch 1400 loss at 500 samples is 1.5864375\n",
            "2024-01-14 10:54:24,402 - LOSS train 1.5864375 valid 4.85546875\n",
            "2024-01-14 10:54:24,405 - Starting training at 272\n",
            "2024-01-14 10:54:25,076 - For batch 100 loss at 500 samples is 1.5879296875\n",
            "2024-01-14 10:54:25,695 - For batch 200 loss at 500 samples is 1.5986328125\n",
            "2024-01-14 10:54:26,308 - For batch 300 loss at 500 samples is 1.59409375\n",
            "2024-01-14 10:54:26,996 - For batch 400 loss at 500 samples is 1.585953125\n",
            "2024-01-14 10:54:27,632 - For batch 500 loss at 500 samples is 1.585359375\n",
            "2024-01-14 10:54:28,241 - For batch 600 loss at 500 samples is 1.5965546875\n",
            "2024-01-14 10:54:28,870 - For batch 700 loss at 500 samples is 1.579203125\n",
            "2024-01-14 10:54:29,498 - For batch 800 loss at 500 samples is 1.5869453125\n",
            "2024-01-14 10:54:30,134 - For batch 900 loss at 500 samples is 1.584421875\n",
            "2024-01-14 10:54:30,940 - For batch 1000 loss at 500 samples is 1.59015625\n",
            "2024-01-14 10:54:31,797 - For batch 1100 loss at 500 samples is 1.5824296875\n",
            "2024-01-14 10:54:32,625 - For batch 1200 loss at 500 samples is 1.57125\n",
            "2024-01-14 10:54:33,515 - For batch 1300 loss at 500 samples is 1.5846796875\n",
            "2024-01-14 10:54:34,141 - For batch 1400 loss at 500 samples is 1.58496875\n",
            "2024-01-14 10:54:34,593 - LOSS train 1.58496875 valid 4.85546875\n",
            "2024-01-14 10:54:34,596 - Starting training at 273\n",
            "2024-01-14 10:54:35,233 - For batch 100 loss at 500 samples is 1.587734375\n",
            "2024-01-14 10:54:35,830 - For batch 200 loss at 500 samples is 1.598546875\n",
            "2024-01-14 10:54:36,490 - For batch 300 loss at 500 samples is 1.5938828125\n",
            "2024-01-14 10:54:37,080 - For batch 400 loss at 500 samples is 1.58571875\n",
            "2024-01-14 10:54:37,728 - For batch 500 loss at 500 samples is 1.584453125\n",
            "2024-01-14 10:54:38,406 - For batch 600 loss at 500 samples is 1.5964453125\n",
            "2024-01-14 10:54:39,176 - For batch 700 loss at 500 samples is 1.5792109375\n",
            "2024-01-14 10:54:39,805 - For batch 800 loss at 500 samples is 1.5895703125\n",
            "2024-01-14 10:54:40,476 - For batch 900 loss at 500 samples is 1.5862734375\n",
            "2024-01-14 10:54:41,078 - For batch 1000 loss at 500 samples is 1.590125\n",
            "2024-01-14 10:54:41,769 - For batch 1100 loss at 500 samples is 1.5819375\n",
            "2024-01-14 10:54:42,392 - For batch 1200 loss at 500 samples is 1.571390625\n",
            "2024-01-14 10:54:43,027 - For batch 1300 loss at 500 samples is 1.5845546875\n",
            "2024-01-14 10:54:43,783 - For batch 1400 loss at 500 samples is 1.5829453125\n",
            "2024-01-14 10:54:44,439 - LOSS train 1.5829453125 valid 4.8203125\n",
            "2024-01-14 10:54:44,441 - Starting training at 274\n",
            "2024-01-14 10:54:45,275 - For batch 100 loss at 500 samples is 1.5876796875\n",
            "2024-01-14 10:54:46,273 - For batch 200 loss at 500 samples is 1.598359375\n",
            "2024-01-14 10:54:47,017 - For batch 300 loss at 500 samples is 1.59390625\n",
            "2024-01-14 10:54:47,609 - For batch 400 loss at 500 samples is 1.5861015625\n",
            "2024-01-14 10:54:48,256 - For batch 500 loss at 500 samples is 1.584984375\n",
            "2024-01-14 10:54:48,959 - For batch 600 loss at 500 samples is 1.5964453125\n",
            "2024-01-14 10:54:49,600 - For batch 700 loss at 500 samples is 1.578953125\n",
            "2024-01-14 10:54:50,205 - For batch 800 loss at 500 samples is 1.5867578125\n",
            "2024-01-14 10:54:50,891 - For batch 900 loss at 500 samples is 1.58353125\n",
            "2024-01-14 10:54:51,545 - For batch 1000 loss at 500 samples is 1.5899921875\n",
            "2024-01-14 10:54:52,255 - For batch 1100 loss at 500 samples is 1.5818046875\n",
            "2024-01-14 10:54:52,907 - For batch 1200 loss at 500 samples is 1.5712890625\n",
            "2024-01-14 10:54:53,482 - For batch 1300 loss at 500 samples is 1.5843671875\n",
            "2024-01-14 10:54:54,131 - For batch 1400 loss at 500 samples is 1.5838671875\n",
            "2024-01-14 10:54:54,503 - LOSS train 1.5838671875 valid 4.8046875\n",
            "2024-01-14 10:54:54,505 - Starting training at 275\n",
            "2024-01-14 10:54:55,408 - For batch 100 loss at 500 samples is 1.5881640625\n",
            "2024-01-14 10:54:56,128 - For batch 200 loss at 500 samples is 1.599125\n",
            "2024-01-14 10:54:56,984 - For batch 300 loss at 500 samples is 1.5943515625\n",
            "2024-01-14 10:54:57,952 - For batch 400 loss at 500 samples is 1.5864375\n",
            "2024-01-14 10:54:58,860 - For batch 500 loss at 500 samples is 1.585171875\n",
            "2024-01-14 10:54:59,722 - For batch 600 loss at 500 samples is 1.596859375\n",
            "2024-01-14 10:55:00,477 - For batch 700 loss at 500 samples is 1.579703125\n",
            "2024-01-14 10:55:01,145 - For batch 800 loss at 500 samples is 1.587328125\n",
            "2024-01-14 10:55:01,715 - For batch 900 loss at 500 samples is 1.5834296875\n",
            "2024-01-14 10:55:02,317 - For batch 1000 loss at 500 samples is 1.5908203125\n",
            "2024-01-14 10:55:02,962 - For batch 1100 loss at 500 samples is 1.58271875\n",
            "2024-01-14 10:55:03,668 - For batch 1200 loss at 500 samples is 1.571828125\n",
            "2024-01-14 10:55:04,302 - For batch 1300 loss at 500 samples is 1.584859375\n",
            "2024-01-14 10:55:04,900 - For batch 1400 loss at 500 samples is 1.5892578125\n",
            "2024-01-14 10:55:05,339 - LOSS train 1.5892578125 valid 4.78515625\n",
            "2024-01-14 10:55:05,343 - Starting training at 276\n",
            "2024-01-14 10:55:05,929 - For batch 100 loss at 500 samples is 1.5881484375\n",
            "2024-01-14 10:55:06,530 - For batch 200 loss at 500 samples is 1.599125\n",
            "2024-01-14 10:55:07,114 - For batch 300 loss at 500 samples is 1.5944453125\n",
            "2024-01-14 10:55:07,734 - For batch 400 loss at 500 samples is 1.5865625\n",
            "2024-01-14 10:55:08,369 - For batch 500 loss at 500 samples is 1.586265625\n",
            "2024-01-14 10:55:09,009 - For batch 600 loss at 500 samples is 1.596859375\n",
            "2024-01-14 10:55:09,608 - For batch 700 loss at 500 samples is 1.5795859375\n",
            "2024-01-14 10:55:10,351 - For batch 800 loss at 500 samples is 1.59\n",
            "2024-01-14 10:55:11,243 - For batch 900 loss at 500 samples is 1.5876953125\n",
            "2024-01-14 10:55:12,099 - For batch 1000 loss at 500 samples is 1.5902421875\n",
            "2024-01-14 10:55:12,963 - For batch 1100 loss at 500 samples is 1.582296875\n",
            "2024-01-14 10:55:13,638 - For batch 1200 loss at 500 samples is 1.5716796875\n",
            "2024-01-14 10:55:14,317 - For batch 1300 loss at 500 samples is 1.5848359375\n",
            "2024-01-14 10:55:15,027 - For batch 1400 loss at 500 samples is 1.584265625\n",
            "2024-01-14 10:55:15,452 - LOSS train 1.584265625 valid 4.78125\n",
            "2024-01-14 10:55:15,454 - Starting training at 277\n",
            "2024-01-14 10:55:16,033 - For batch 100 loss at 500 samples is 1.588046875\n",
            "2024-01-14 10:55:16,702 - For batch 200 loss at 500 samples is 1.5989375\n",
            "2024-01-14 10:55:17,293 - For batch 300 loss at 500 samples is 1.5943046875\n",
            "2024-01-14 10:55:17,894 - For batch 400 loss at 500 samples is 1.586265625\n",
            "2024-01-14 10:55:18,514 - For batch 500 loss at 500 samples is 1.5848828125\n",
            "2024-01-14 10:55:19,159 - For batch 600 loss at 500 samples is 1.596703125\n",
            "2024-01-14 10:55:19,782 - For batch 700 loss at 500 samples is 1.579625\n",
            "2024-01-14 10:55:20,424 - For batch 800 loss at 500 samples is 1.5871953125\n",
            "2024-01-14 10:55:21,029 - For batch 900 loss at 500 samples is 1.5827890625\n",
            "2024-01-14 10:55:21,680 - For batch 1000 loss at 500 samples is 1.59034375\n",
            "2024-01-14 10:55:22,353 - For batch 1100 loss at 500 samples is 1.58221875\n",
            "2024-01-14 10:55:23,053 - For batch 1200 loss at 500 samples is 1.5719140625\n",
            "2024-01-14 10:55:23,993 - For batch 1300 loss at 500 samples is 1.5846640625\n",
            "2024-01-14 10:55:24,848 - For batch 1400 loss at 500 samples is 1.5831484375\n",
            "2024-01-14 10:55:25,560 - LOSS train 1.5831484375 valid 4.75\n",
            "2024-01-14 10:55:25,567 - Starting training at 278\n",
            "2024-01-14 10:55:26,346 - For batch 100 loss at 500 samples is 1.5877578125\n",
            "2024-01-14 10:55:26,926 - For batch 200 loss at 500 samples is 1.5988359375\n",
            "2024-01-14 10:55:27,580 - For batch 300 loss at 500 samples is 1.5944140625\n",
            "2024-01-14 10:55:28,184 - For batch 400 loss at 500 samples is 1.5862578125\n",
            "2024-01-14 10:55:28,837 - For batch 500 loss at 500 samples is 1.5851640625\n",
            "2024-01-14 10:55:29,457 - For batch 600 loss at 500 samples is 1.5965546875\n",
            "2024-01-14 10:55:30,077 - For batch 700 loss at 500 samples is 1.579515625\n",
            "2024-01-14 10:55:30,727 - For batch 800 loss at 500 samples is 1.5871328125\n",
            "2024-01-14 10:55:31,374 - For batch 900 loss at 500 samples is 1.5835859375\n",
            "2024-01-14 10:55:32,009 - For batch 1000 loss at 500 samples is 1.5902265625\n",
            "2024-01-14 10:55:32,652 - For batch 1100 loss at 500 samples is 1.5823359375\n",
            "2024-01-14 10:55:33,267 - For batch 1200 loss at 500 samples is 1.5713359375\n",
            "2024-01-14 10:55:33,972 - For batch 1300 loss at 500 samples is 1.5847109375\n",
            "2024-01-14 10:55:34,579 - For batch 1400 loss at 500 samples is 1.589046875\n",
            "2024-01-14 10:55:35,015 - LOSS train 1.589046875 valid 4.7421875\n",
            "2024-01-14 10:55:35,020 - Starting training at 279\n",
            "2024-01-14 10:55:35,641 - For batch 100 loss at 500 samples is 1.587625\n",
            "2024-01-14 10:55:36,369 - For batch 200 loss at 500 samples is 1.598953125\n",
            "2024-01-14 10:55:37,181 - For batch 300 loss at 500 samples is 1.5948515625\n",
            "2024-01-14 10:55:38,066 - For batch 400 loss at 500 samples is 1.5869296875\n",
            "2024-01-14 10:55:38,992 - For batch 500 loss at 500 samples is 1.5856875\n",
            "2024-01-14 10:55:39,682 - For batch 600 loss at 500 samples is 1.5973359375\n",
            "2024-01-14 10:55:40,348 - For batch 700 loss at 500 samples is 1.5800234375\n",
            "2024-01-14 10:55:40,983 - For batch 800 loss at 500 samples is 1.5901875\n",
            "2024-01-14 10:55:41,642 - For batch 900 loss at 500 samples is 1.587\n",
            "2024-01-14 10:55:42,299 - For batch 1000 loss at 500 samples is 1.5908828125\n",
            "2024-01-14 10:55:42,934 - For batch 1100 loss at 500 samples is 1.582671875\n",
            "2024-01-14 10:55:43,631 - For batch 1200 loss at 500 samples is 1.5720859375\n",
            "2024-01-14 10:55:44,256 - For batch 1300 loss at 500 samples is 1.5851328125\n",
            "2024-01-14 10:55:44,910 - For batch 1400 loss at 500 samples is 1.583953125\n",
            "2024-01-14 10:55:45,415 - LOSS train 1.583953125 valid 4.71484375\n",
            "2024-01-14 10:55:45,418 - Starting training at 280\n",
            "2024-01-14 10:55:46,064 - For batch 100 loss at 500 samples is 1.58803125\n",
            "2024-01-14 10:55:46,720 - For batch 200 loss at 500 samples is 1.599203125\n",
            "2024-01-14 10:55:47,408 - For batch 300 loss at 500 samples is 1.594859375\n",
            "2024-01-14 10:55:48,068 - For batch 400 loss at 500 samples is 1.5870546875\n",
            "2024-01-14 10:55:48,749 - For batch 500 loss at 500 samples is 1.5869921875\n",
            "2024-01-14 10:55:49,376 - For batch 600 loss at 500 samples is 1.59728125\n",
            "2024-01-14 10:55:50,369 - For batch 700 loss at 500 samples is 1.5796640625\n",
            "2024-01-14 10:55:51,224 - For batch 800 loss at 500 samples is 1.5876171875\n",
            "2024-01-14 10:55:52,125 - For batch 900 loss at 500 samples is 1.58471875\n",
            "2024-01-14 10:55:53,005 - For batch 1000 loss at 500 samples is 1.590578125\n",
            "2024-01-14 10:55:53,637 - For batch 1100 loss at 500 samples is 1.5825703125\n",
            "2024-01-14 10:55:54,223 - For batch 1200 loss at 500 samples is 1.57190625\n",
            "2024-01-14 10:55:54,925 - For batch 1300 loss at 500 samples is 1.58521875\n",
            "2024-01-14 10:55:55,598 - For batch 1400 loss at 500 samples is 1.5833671875\n",
            "2024-01-14 10:55:56,034 - LOSS train 1.5833671875 valid 4.7109375\n",
            "2024-01-14 10:55:56,036 - Starting training at 281\n",
            "2024-01-14 10:55:56,668 - For batch 100 loss at 500 samples is 1.58803125\n",
            "2024-01-14 10:55:57,270 - For batch 200 loss at 500 samples is 1.599109375\n",
            "2024-01-14 10:55:57,878 - For batch 300 loss at 500 samples is 1.5945625\n",
            "2024-01-14 10:55:58,499 - For batch 400 loss at 500 samples is 1.5867734375\n",
            "2024-01-14 10:55:59,133 - For batch 500 loss at 500 samples is 1.5852890625\n",
            "2024-01-14 10:55:59,710 - For batch 600 loss at 500 samples is 1.5970703125\n",
            "2024-01-14 10:56:00,343 - For batch 700 loss at 500 samples is 1.579875\n",
            "2024-01-14 10:56:00,977 - For batch 800 loss at 500 samples is 1.5873671875\n",
            "2024-01-14 10:56:01,576 - For batch 900 loss at 500 samples is 1.583359375\n",
            "2024-01-14 10:56:02,199 - For batch 1000 loss at 500 samples is 1.5904453125\n",
            "2024-01-14 10:56:02,950 - For batch 1100 loss at 500 samples is 1.582703125\n",
            "2024-01-14 10:56:03,749 - For batch 1200 loss at 500 samples is 1.5723046875\n",
            "2024-01-14 10:56:04,551 - For batch 1300 loss at 500 samples is 1.5850390625\n",
            "2024-01-14 10:56:05,556 - For batch 1400 loss at 500 samples is 1.58965625\n",
            "2024-01-14 10:56:05,990 - LOSS train 1.58965625 valid 4.6875\n",
            "2024-01-14 10:56:05,993 - Starting training at 282\n",
            "2024-01-14 10:56:06,635 - For batch 100 loss at 500 samples is 1.5879296875\n",
            "2024-01-14 10:56:07,300 - For batch 200 loss at 500 samples is 1.59909375\n",
            "2024-01-14 10:56:07,923 - For batch 300 loss at 500 samples is 1.5945390625\n",
            "2024-01-14 10:56:08,556 - For batch 400 loss at 500 samples is 1.5866953125\n",
            "2024-01-14 10:56:09,146 - For batch 500 loss at 500 samples is 1.585375\n",
            "2024-01-14 10:56:09,789 - For batch 600 loss at 500 samples is 1.5970390625\n",
            "2024-01-14 10:56:10,429 - For batch 700 loss at 500 samples is 1.579765625\n",
            "2024-01-14 10:56:11,090 - For batch 800 loss at 500 samples is 1.5897421875\n",
            "2024-01-14 10:56:11,706 - For batch 900 loss at 500 samples is 1.5865546875\n",
            "2024-01-14 10:56:12,307 - For batch 1000 loss at 500 samples is 1.590375\n",
            "2024-01-14 10:56:12,920 - For batch 1100 loss at 500 samples is 1.58240625\n",
            "2024-01-14 10:56:13,557 - For batch 1200 loss at 500 samples is 1.571734375\n",
            "2024-01-14 10:56:14,197 - For batch 1300 loss at 500 samples is 1.585015625\n",
            "2024-01-14 10:56:14,817 - For batch 1400 loss at 500 samples is 1.58221875\n",
            "2024-01-14 10:56:15,302 - LOSS train 1.58221875 valid 4.671875\n",
            "2024-01-14 10:56:15,304 - Starting training at 283\n",
            "2024-01-14 10:56:15,974 - For batch 100 loss at 500 samples is 1.587859375\n",
            "2024-01-14 10:56:16,856 - For batch 200 loss at 500 samples is 1.5990078125\n",
            "2024-01-14 10:56:17,800 - For batch 300 loss at 500 samples is 1.594421875\n",
            "2024-01-14 10:56:18,639 - For batch 400 loss at 500 samples is 1.5865546875\n",
            "2024-01-14 10:56:19,350 - For batch 500 loss at 500 samples is 1.585\n",
            "2024-01-14 10:56:19,967 - For batch 600 loss at 500 samples is 1.5967109375\n",
            "2024-01-14 10:56:20,691 - For batch 700 loss at 500 samples is 1.5809453125\n",
            "2024-01-14 10:56:21,336 - For batch 800 loss at 500 samples is 1.5879765625\n",
            "2024-01-14 10:56:21,994 - For batch 900 loss at 500 samples is 1.5839140625\n",
            "2024-01-14 10:56:22,592 - For batch 1000 loss at 500 samples is 1.591203125\n",
            "2024-01-14 10:56:23,293 - For batch 1100 loss at 500 samples is 1.583109375\n",
            "2024-01-14 10:56:23,940 - For batch 1200 loss at 500 samples is 1.5725390625\n",
            "2024-01-14 10:56:24,557 - For batch 1300 loss at 500 samples is 1.5854765625\n",
            "2024-01-14 10:56:25,155 - For batch 1400 loss at 500 samples is 1.5834921875\n",
            "2024-01-14 10:56:25,600 - LOSS train 1.5834921875 valid 4.6484375\n",
            "2024-01-14 10:56:25,602 - Starting training at 284\n",
            "2024-01-14 10:56:26,202 - For batch 100 loss at 500 samples is 1.588515625\n",
            "2024-01-14 10:56:26,840 - For batch 200 loss at 500 samples is 1.5996875\n",
            "2024-01-14 10:56:27,512 - For batch 300 loss at 500 samples is 1.59525\n",
            "2024-01-14 10:56:28,095 - For batch 400 loss at 500 samples is 1.58734375\n",
            "2024-01-14 10:56:28,701 - For batch 500 loss at 500 samples is 1.5863515625\n",
            "2024-01-14 10:56:29,457 - For batch 600 loss at 500 samples is 1.597453125\n",
            "2024-01-14 10:56:30,258 - For batch 700 loss at 500 samples is 1.580234375\n",
            "2024-01-14 10:56:31,074 - For batch 800 loss at 500 samples is 1.5878359375\n",
            "2024-01-14 10:56:31,988 - For batch 900 loss at 500 samples is 1.584765625\n",
            "2024-01-14 10:56:32,659 - For batch 1000 loss at 500 samples is 1.590796875\n",
            "2024-01-14 10:56:33,265 - For batch 1100 loss at 500 samples is 1.5831328125\n",
            "2024-01-14 10:56:33,867 - For batch 1200 loss at 500 samples is 1.5723125\n",
            "2024-01-14 10:56:34,479 - For batch 1300 loss at 500 samples is 1.5855234375\n",
            "2024-01-14 10:56:35,113 - For batch 1400 loss at 500 samples is 1.5874765625\n",
            "2024-01-14 10:56:35,496 - LOSS train 1.5874765625 valid 4.6484375\n",
            "2024-01-14 10:56:35,499 - Starting training at 285\n",
            "2024-01-14 10:56:36,120 - For batch 100 loss at 500 samples is 1.5884296875\n",
            "2024-01-14 10:56:36,733 - For batch 200 loss at 500 samples is 1.5996484375\n",
            "2024-01-14 10:56:37,344 - For batch 300 loss at 500 samples is 1.595046875\n",
            "2024-01-14 10:56:37,983 - For batch 400 loss at 500 samples is 1.587234375\n",
            "2024-01-14 10:56:38,559 - For batch 500 loss at 500 samples is 1.586046875\n",
            "2024-01-14 10:56:39,199 - For batch 600 loss at 500 samples is 1.5974296875\n",
            "2024-01-14 10:56:39,919 - For batch 700 loss at 500 samples is 1.582125\n",
            "2024-01-14 10:56:40,505 - For batch 800 loss at 500 samples is 1.59009375\n",
            "2024-01-14 10:56:41,111 - For batch 900 loss at 500 samples is 1.586625\n",
            "2024-01-14 10:56:41,716 - For batch 1000 loss at 500 samples is 1.5909453125\n",
            "2024-01-14 10:56:42,452 - For batch 1100 loss at 500 samples is 1.5828203125\n",
            "2024-01-14 10:56:43,457 - For batch 1200 loss at 500 samples is 1.572515625\n",
            "2024-01-14 10:56:44,258 - For batch 1300 loss at 500 samples is 1.585203125\n",
            "2024-01-14 10:56:45,129 - For batch 1400 loss at 500 samples is 1.582984375\n",
            "2024-01-14 10:56:45,597 - LOSS train 1.582984375 valid 4.6171875\n",
            "2024-01-14 10:56:45,600 - Starting training at 286\n",
            "2024-01-14 10:56:46,265 - For batch 100 loss at 500 samples is 1.5883359375\n",
            "2024-01-14 10:56:46,993 - For batch 200 loss at 500 samples is 1.5993671875\n",
            "2024-01-14 10:56:47,611 - For batch 300 loss at 500 samples is 1.5950390625\n",
            "2024-01-14 10:56:48,308 - For batch 400 loss at 500 samples is 1.5870546875\n",
            "2024-01-14 10:56:48,953 - For batch 500 loss at 500 samples is 1.5859921875\n",
            "2024-01-14 10:56:49,633 - For batch 600 loss at 500 samples is 1.5972578125\n",
            "2024-01-14 10:56:50,297 - For batch 700 loss at 500 samples is 1.5797734375\n",
            "2024-01-14 10:56:50,930 - For batch 800 loss at 500 samples is 1.587421875\n",
            "2024-01-14 10:56:51,559 - For batch 900 loss at 500 samples is 1.5841328125\n",
            "2024-01-14 10:56:52,189 - For batch 1000 loss at 500 samples is 1.5905\n",
            "2024-01-14 10:56:52,856 - For batch 1100 loss at 500 samples is 1.5827265625\n",
            "2024-01-14 10:56:53,541 - For batch 1200 loss at 500 samples is 1.5719765625\n",
            "2024-01-14 10:56:54,247 - For batch 1300 loss at 500 samples is 1.58525\n",
            "2024-01-14 10:56:55,020 - For batch 1400 loss at 500 samples is 1.5881015625\n",
            "2024-01-14 10:56:55,555 - LOSS train 1.5881015625 valid 4.60546875\n",
            "2024-01-14 10:56:55,560 - Starting training at 287\n",
            "2024-01-14 10:56:56,392 - For batch 100 loss at 500 samples is 1.58821875\n",
            "2024-01-14 10:56:57,365 - For batch 200 loss at 500 samples is 1.5993828125\n",
            "2024-01-14 10:56:58,298 - For batch 300 loss at 500 samples is 1.5950859375\n",
            "2024-01-14 10:56:58,954 - For batch 400 loss at 500 samples is 1.5871328125\n",
            "2024-01-14 10:56:59,596 - For batch 500 loss at 500 samples is 1.5856953125\n",
            "2024-01-14 10:57:00,273 - For batch 600 loss at 500 samples is 1.597203125\n",
            "2024-01-14 10:57:00,932 - For batch 700 loss at 500 samples is 1.5798828125\n",
            "2024-01-14 10:57:01,566 - For batch 800 loss at 500 samples is 1.5875234375\n",
            "2024-01-14 10:57:02,165 - For batch 900 loss at 500 samples is 1.5839375\n",
            "2024-01-14 10:57:02,821 - For batch 1000 loss at 500 samples is 1.590765625\n",
            "2024-01-14 10:57:03,481 - For batch 1100 loss at 500 samples is 1.5826484375\n",
            "2024-01-14 10:57:04,092 - For batch 1200 loss at 500 samples is 1.57196875\n",
            "2024-01-14 10:57:04,795 - For batch 1300 loss at 500 samples is 1.584875\n",
            "2024-01-14 10:57:05,401 - For batch 1400 loss at 500 samples is 1.5842734375\n",
            "2024-01-14 10:57:05,878 - LOSS train 1.5842734375 valid 4.5859375\n",
            "2024-01-14 10:57:05,881 - Starting training at 288\n",
            "2024-01-14 10:57:06,469 - For batch 100 loss at 500 samples is 1.5888359375\n",
            "2024-01-14 10:57:07,065 - For batch 200 loss at 500 samples is 1.6000703125\n",
            "2024-01-14 10:57:07,702 - For batch 300 loss at 500 samples is 1.59596875\n",
            "2024-01-14 10:57:08,310 - For batch 400 loss at 500 samples is 1.58790625\n",
            "2024-01-14 10:57:09,097 - For batch 500 loss at 500 samples is 1.5868125\n",
            "2024-01-14 10:57:09,946 - For batch 600 loss at 500 samples is 1.5976953125\n",
            "2024-01-14 10:57:10,814 - For batch 700 loss at 500 samples is 1.5805546875\n",
            "2024-01-14 10:57:11,703 - For batch 800 loss at 500 samples is 1.59040625\n",
            "2024-01-14 10:57:12,316 - For batch 900 loss at 500 samples is 1.5880234375\n",
            "2024-01-14 10:57:13,051 - For batch 1000 loss at 500 samples is 1.5909140625\n",
            "2024-01-14 10:57:13,637 - For batch 1100 loss at 500 samples is 1.583140625\n",
            "2024-01-14 10:57:14,230 - For batch 1200 loss at 500 samples is 1.572328125\n",
            "2024-01-14 10:57:14,883 - For batch 1300 loss at 500 samples is 1.58553125\n",
            "2024-01-14 10:57:15,506 - For batch 1400 loss at 500 samples is 1.58496875\n",
            "2024-01-14 10:57:15,932 - LOSS train 1.58496875 valid 4.58984375\n",
            "2024-01-14 10:57:15,935 - Starting training at 289\n",
            "2024-01-14 10:57:16,540 - For batch 100 loss at 500 samples is 1.588734375\n",
            "2024-01-14 10:57:17,187 - For batch 200 loss at 500 samples is 1.599578125\n",
            "2024-01-14 10:57:17,798 - For batch 300 loss at 500 samples is 1.595375\n",
            "2024-01-14 10:57:18,429 - For batch 400 loss at 500 samples is 1.587796875\n",
            "2024-01-14 10:57:19,119 - For batch 500 loss at 500 samples is 1.58709375\n",
            "2024-01-14 10:57:19,700 - For batch 600 loss at 500 samples is 1.59784375\n",
            "2024-01-14 10:57:20,435 - For batch 700 loss at 500 samples is 1.5806875\n",
            "2024-01-14 10:57:21,152 - For batch 800 loss at 500 samples is 1.5880078125\n",
            "2024-01-14 10:57:21,837 - For batch 900 loss at 500 samples is 1.5839765625\n",
            "2024-01-14 10:57:22,732 - For batch 1000 loss at 500 samples is 1.5912265625\n",
            "2024-01-14 10:57:23,602 - For batch 1100 loss at 500 samples is 1.583359375\n",
            "2024-01-14 10:57:24,587 - For batch 1200 loss at 500 samples is 1.5728828125\n",
            "2024-01-14 10:57:25,213 - For batch 1300 loss at 500 samples is 1.58525\n",
            "2024-01-14 10:57:25,789 - For batch 1400 loss at 500 samples is 1.5909375\n",
            "2024-01-14 10:57:26,245 - LOSS train 1.5909375 valid 4.5546875\n",
            "2024-01-14 10:57:26,247 - Starting training at 290\n",
            "2024-01-14 10:57:26,889 - For batch 100 loss at 500 samples is 1.5884375\n",
            "2024-01-14 10:57:27,507 - For batch 200 loss at 500 samples is 1.599359375\n",
            "2024-01-14 10:57:28,126 - For batch 300 loss at 500 samples is 1.595734375\n",
            "2024-01-14 10:57:28,731 - For batch 400 loss at 500 samples is 1.5877265625\n",
            "2024-01-14 10:57:29,474 - For batch 500 loss at 500 samples is 1.5866953125\n",
            "2024-01-14 10:57:30,088 - For batch 600 loss at 500 samples is 1.5975703125\n",
            "2024-01-14 10:57:30,758 - For batch 700 loss at 500 samples is 1.5807734375\n",
            "2024-01-14 10:57:31,437 - For batch 800 loss at 500 samples is 1.58765625\n",
            "2024-01-14 10:57:32,093 - For batch 900 loss at 500 samples is 1.584125\n",
            "2024-01-14 10:57:32,885 - For batch 1000 loss at 500 samples is 1.5909609375\n",
            "2024-01-14 10:57:33,555 - For batch 1100 loss at 500 samples is 1.58315625\n",
            "2024-01-14 10:57:34,160 - For batch 1200 loss at 500 samples is 1.572140625\n",
            "2024-01-14 10:57:34,884 - For batch 1300 loss at 500 samples is 1.58521875\n",
            "2024-01-14 10:57:35,858 - For batch 1400 loss at 500 samples is 1.586765625\n",
            "2024-01-14 10:57:36,422 - LOSS train 1.586765625 valid 4.54296875\n",
            "2024-01-14 10:57:36,434 - Starting training at 291\n",
            "2024-01-14 10:57:37,297 - For batch 100 loss at 500 samples is 1.58825\n",
            "2024-01-14 10:57:38,111 - For batch 200 loss at 500 samples is 1.59953125\n",
            "2024-01-14 10:57:38,749 - For batch 300 loss at 500 samples is 1.5955078125\n",
            "2024-01-14 10:57:39,372 - For batch 400 loss at 500 samples is 1.58746875\n",
            "2024-01-14 10:57:39,974 - For batch 500 loss at 500 samples is 1.5859609375\n",
            "2024-01-14 10:57:40,612 - For batch 600 loss at 500 samples is 1.597203125\n",
            "2024-01-14 10:57:41,276 - For batch 700 loss at 500 samples is 1.5806953125\n",
            "2024-01-14 10:57:41,915 - For batch 800 loss at 500 samples is 1.589625\n",
            "2024-01-14 10:57:42,504 - For batch 900 loss at 500 samples is 1.5876953125\n",
            "2024-01-14 10:57:43,183 - For batch 1000 loss at 500 samples is 1.591078125\n",
            "2024-01-14 10:57:43,843 - For batch 1100 loss at 500 samples is 1.583125\n",
            "2024-01-14 10:57:44,412 - For batch 1200 loss at 500 samples is 1.572640625\n",
            "2024-01-14 10:57:45,023 - For batch 1300 loss at 500 samples is 1.5850078125\n",
            "2024-01-14 10:57:45,685 - For batch 1400 loss at 500 samples is 1.5837734375\n",
            "2024-01-14 10:57:46,057 - LOSS train 1.5837734375 valid 4.5234375\n",
            "2024-01-14 10:57:46,059 - Starting training at 292\n",
            "2024-01-14 10:57:46,718 - For batch 100 loss at 500 samples is 1.5884375\n",
            "2024-01-14 10:57:47,340 - For batch 200 loss at 500 samples is 1.59940625\n",
            "2024-01-14 10:57:47,976 - For batch 300 loss at 500 samples is 1.595703125\n",
            "2024-01-14 10:57:48,902 - For batch 400 loss at 500 samples is 1.5876171875\n",
            "2024-01-14 10:57:49,715 - For batch 500 loss at 500 samples is 1.5865625\n",
            "2024-01-14 10:57:50,582 - For batch 600 loss at 500 samples is 1.5971015625\n",
            "2024-01-14 10:57:51,326 - For batch 700 loss at 500 samples is 1.5806484375\n",
            "2024-01-14 10:57:51,933 - For batch 800 loss at 500 samples is 1.587546875\n",
            "2024-01-14 10:57:52,589 - For batch 900 loss at 500 samples is 1.5851328125\n",
            "2024-01-14 10:57:53,184 - For batch 1000 loss at 500 samples is 1.591328125\n",
            "2024-01-14 10:57:53,839 - For batch 1100 loss at 500 samples is 1.5839453125\n",
            "2024-01-14 10:57:54,444 - For batch 1200 loss at 500 samples is 1.572578125\n",
            "2024-01-14 10:57:55,095 - For batch 1300 loss at 500 samples is 1.5858828125\n",
            "2024-01-14 10:57:55,740 - For batch 1400 loss at 500 samples is 1.5873359375\n",
            "2024-01-14 10:57:56,190 - LOSS train 1.5873359375 valid 4.5234375\n",
            "2024-01-14 10:57:56,192 - Starting training at 293\n",
            "2024-01-14 10:57:56,890 - For batch 100 loss at 500 samples is 1.589609375\n",
            "2024-01-14 10:57:57,531 - For batch 200 loss at 500 samples is 1.600421875\n",
            "2024-01-14 10:57:58,163 - For batch 300 loss at 500 samples is 1.59625\n",
            "2024-01-14 10:57:58,861 - For batch 400 loss at 500 samples is 1.588125\n",
            "2024-01-14 10:57:59,542 - For batch 500 loss at 500 samples is 1.58690625\n",
            "2024-01-14 10:58:00,157 - For batch 600 loss at 500 samples is 1.597578125\n",
            "2024-01-14 10:58:00,835 - For batch 700 loss at 500 samples is 1.5814140625\n",
            "2024-01-14 10:58:01,559 - For batch 800 loss at 500 samples is 1.58828125\n",
            "2024-01-14 10:58:02,498 - For batch 900 loss at 500 samples is 1.584375\n",
            "2024-01-14 10:58:03,388 - For batch 1000 loss at 500 samples is 1.5915859375\n",
            "2024-01-14 10:58:04,404 - For batch 1100 loss at 500 samples is 1.5834921875\n",
            "2024-01-14 10:58:05,033 - For batch 1200 loss at 500 samples is 1.573015625\n",
            "2024-01-14 10:58:05,665 - For batch 1300 loss at 500 samples is 1.5857890625\n",
            "2024-01-14 10:58:06,267 - For batch 1400 loss at 500 samples is 1.5900078125\n",
            "2024-01-14 10:58:06,711 - LOSS train 1.5900078125 valid 4.49609375\n",
            "2024-01-14 10:58:06,713 - Starting training at 294\n",
            "2024-01-14 10:58:07,352 - For batch 100 loss at 500 samples is 1.589515625\n",
            "2024-01-14 10:58:08,013 - For batch 200 loss at 500 samples is 1.6004375\n",
            "2024-01-14 10:58:08,649 - For batch 300 loss at 500 samples is 1.5963515625\n",
            "2024-01-14 10:58:09,282 - For batch 400 loss at 500 samples is 1.58828125\n",
            "2024-01-14 10:58:09,963 - For batch 500 loss at 500 samples is 1.5866015625\n",
            "2024-01-14 10:58:10,582 - For batch 600 loss at 500 samples is 1.597609375\n",
            "2024-01-14 10:58:11,207 - For batch 700 loss at 500 samples is 1.581234375\n",
            "2024-01-14 10:58:11,804 - For batch 800 loss at 500 samples is 1.5900234375\n",
            "2024-01-14 10:58:12,445 - For batch 900 loss at 500 samples is 1.5873125\n",
            "2024-01-14 10:58:13,108 - For batch 1000 loss at 500 samples is 1.591515625\n",
            "2024-01-14 10:58:13,688 - For batch 1100 loss at 500 samples is 1.5837421875\n",
            "2024-01-14 10:58:14,296 - For batch 1200 loss at 500 samples is 1.572484375\n",
            "2024-01-14 10:58:15,138 - For batch 1300 loss at 500 samples is 1.5860390625\n",
            "2024-01-14 10:58:16,106 - For batch 1400 loss at 500 samples is 1.587890625\n",
            "2024-01-14 10:58:16,668 - LOSS train 1.587890625 valid 4.48828125\n",
            "2024-01-14 10:58:16,675 - Starting training at 295\n",
            "2024-01-14 10:58:17,571 - For batch 100 loss at 500 samples is 1.5889296875\n",
            "2024-01-14 10:58:18,214 - For batch 200 loss at 500 samples is 1.600359375\n",
            "2024-01-14 10:58:18,798 - For batch 300 loss at 500 samples is 1.59609375\n",
            "2024-01-14 10:58:19,436 - For batch 400 loss at 500 samples is 1.588171875\n",
            "2024-01-14 10:58:20,065 - For batch 500 loss at 500 samples is 1.5858203125\n",
            "2024-01-14 10:58:20,710 - For batch 600 loss at 500 samples is 1.5974921875\n",
            "2024-01-14 10:58:21,439 - For batch 700 loss at 500 samples is 1.58115625\n",
            "2024-01-14 10:58:22,038 - For batch 800 loss at 500 samples is 1.5879375\n",
            "2024-01-14 10:58:22,632 - For batch 900 loss at 500 samples is 1.584171875\n",
            "2024-01-14 10:58:23,264 - For batch 1000 loss at 500 samples is 1.5915\n",
            "2024-01-14 10:58:23,853 - For batch 1100 loss at 500 samples is 1.5831484375\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-adc77d5dd48b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting training at {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#making sure gradient tracking is on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrunning_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-f11acb3bd526>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(input_batch, label_batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#apply the updates to all parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCH = 10000\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    logging.info(f\"Starting training at {epoch+1}\")\n",
        "    model.train(True) #making sure gradient tracking is on\n",
        "    avg_loss = train_one_epoch(batch_inputs_train,batch_labels_train)\n",
        "\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    model.eval() #set the model to eval model(handels dropout calc etc)\n",
        "    with torch.no_grad():\n",
        "        for i,(v_input,v_label) in enumerate(zip(batch_inputs_test,batch_labels_test)):\n",
        "            v_input = torch.tensor(v_input,dtype=torch.long,device=device)\n",
        "            v_label = torch.tensor(v_label,dtype=torch.long,device=device)\n",
        "            v_output = model(v_input)\n",
        "            B,T,C = v_output.shape\n",
        "            v_output = v_output.reshape(B,C,T)\n",
        "            v_loss = loss_fn(v_output,v_label)\n",
        "            running_val_loss += v_loss\n",
        "\n",
        "    avg_vloss = running_val_loss / (epoch + 1)\n",
        "    wandb.log({\"train_loss\":avg_loss})\n",
        "    wandb.log({\"val_loss\":avg_vloss})\n",
        "    wandb.log({\"epoch\":epoch})\n",
        "    logging.info('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing"
      ],
      "metadata": {
        "id": "vquPIRPGXtst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "3bjzPkDqXCdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07fa0d55-2e49-4858-d257-fce64d6b93fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "'.'in vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(vocab).index('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDlog97dYYzh",
        "outputId": "8c3dd7c3-3c6b-4cfa-c6af-fad18c884008"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1612"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_tokens = model.generate(torch.tensor([[1458]],dtype=torch.long,device=device),max_new_tokens=200)"
      ],
      "metadata": {
        "id": "zZU6caPTYJEk"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_after_training = ' '.join(decode(list(generated_tokens[0])))"
      ],
      "metadata": {
        "id": "uqbv7nRAYk06"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text Before Training -->\\n {text_bef_training}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU4GRV5jYwj2",
        "outputId": "959dcea1-1637-4a82-e328-86db2bcf0f28"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Before Training -->\n",
            " gonz inte gentlewoman inte inte inte flattering inte flattering inte gentlewoman flattering flattering inte inte gentlewoman gentlewoman inte gentlewoman gentlewoman inte gentlewoman flattering inte inte gentlewoman gentlewoman gentlewoman inte gentlewoman flattering flattering gentlewoman gentlewoman inte gentlewoman gentlewoman flattering flattering inte flattering inte inte inte inte gentlewoman inte flattering gentlewoman gentlewoman gentlewoman inte flattering flattering inte inte gentlewoman inte flattering flattering flattering gentlewoman inte inte flattering inte flattering gentlewoman inte flattering gentlewoman inte inte gentlewoman gentlewoman inte flattering flattering flattering gentlewoman flattering inte gentlewoman gentlewoman flattering flattering inte flattering gentlewoman flattering inte inte gentlewoman flattering gentlewoman flattering inte inte flattering inte inte gentlewoman gentlewoman gentlewoman inte inte gentlewoman gentlewoman inte flattering gentlewoman flattering inte inte inte gentlewoman gentlewoman inte gentlewoman flattering gentlewoman flattering gentlewoman gentlewoman flattering inte inte inte flattering inte gentlewoman flattering inte inte flattering flattering gentlewoman flattering flattering gentlewoman inte inte inte gentlewoman inte inte gentlewoman gentlewoman flattering inte inte flattering inte inte flattering inte gentlewoman gentlewoman flattering inte gentlewoman inte inte flattering flattering gentlewoman flattering flattering gentlewoman gentlewoman inte gentlewoman gentlewoman flattering gentlewoman flattering inte flattering gentlewoman flattering flattering gentlewoman flattering gentlewoman gentlewoman inte flattering flattering gentlewoman flattering inte inte gentlewoman inte flattering inte gentlewoman gentlewoman gentlewoman gentlewoman inte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text After Traning --> \\n {text_after_training}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuGjQ28mZrGT",
        "outputId": "ed36cdeb-ac7e-4ca3-b8a8-0636b130d47a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text After Traning --> \n",
            " gonz gentlewoman gentlewoman flattering gentlewoman flattering inte gentlewoman gentlewoman gentlewoman gentlewoman flattering flattering gentlewoman gentlewoman inte inte inte flattering gentlewoman flattering gentlewoman flattering inte inte inte flattering flattering flattering gentlewoman inte gentlewoman gentlewoman flattering flattering flattering flattering flattering inte flattering inte gentlewoman flattering inte inte inte flattering gentlewoman flattering flattering gentlewoman flattering inte flattering flattering inte gentlewoman inte inte flattering gentlewoman inte gentlewoman inte inte inte flattering gentlewoman flattering gentlewoman flattering flattering flattering flattering gentlewoman gentlewoman inte inte flattering inte flattering gentlewoman gentlewoman gentlewoman inte gentlewoman inte gentlewoman gentlewoman flattering flattering gentlewoman flattering inte gentlewoman flattering gentlewoman gentlewoman inte inte flattering flattering flattering gentlewoman inte inte inte gentlewoman gentlewoman flattering gentlewoman flattering flattering flattering gentlewoman flattering inte gentlewoman inte flattering inte inte inte gentlewoman flattering gentlewoman inte gentlewoman flattering inte inte gentlewoman inte gentlewoman gentlewoman inte inte flattering inte inte inte inte flattering flattering inte inte inte inte flattering flattering gentlewoman inte flattering gentlewoman gentlewoman flattering gentlewoman inte flattering inte inte flattering inte gentlewoman inte inte inte gentlewoman inte inte inte inte flattering flattering gentlewoman gentlewoman gentlewoman inte gentlewoman flattering inte gentlewoman inte inte flattering flattering gentlewoman inte inte gentlewoman gentlewoman gentlewoman gentlewoman flattering gentlewoman inte flattering gentlewoman flattering flattering gentlewoman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fsBletwZ1gZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3847392,
          "sourceId": 6667539,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4053823,
          "sourceId": 7045013,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30587,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}